FAULTS RECORDS
=================
1. ES 报错 NoNodeAvailableException
	ES集群的IP、端口或集群名称是否正确(es-savoir,not es-savior)
	
2. 本机调试代码报错：org.apache.hadoop.security.AccessControlException: Permission denied: user=yww08, access=WRITE, inode="/anxinyun/structure_data/structure_2/raw/2018/7/液位变送器1.csv":testerone:supergroup:-rw-r--r--
	设置环境变量
	System.setProperty("HADOOP_USER_NAME",hdfsUser)
	
3. 已拥有为“NETStandard.Library”定义的依赖项。
	工具栏---工具----扩展和更新 重装nuget升级
	
4.  Spark: URI is not hierarchical
	引用资源文件错误，修改 `val source=Source.fromFile(getClass.getResource("/jiangsu.json").toURI)`  >> `val source=Source.fromInputStream(getClass.getResourceAsStream("/jiangsu.json"))`
	
5. [Savoir]  修改在Rdd.foreachPartition中存储后，出现Hdfs存储错误：
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /savoir/structure_data/structure_4/theme/2018/7/温湿度.csv for DFSClient_NONMAPREDUCE_-602640997_1250 on 10.8.30.117 because DFSClient_NONMAPREDUCE_-602640997_1250 is already the current lease holder.
	排查可能存在多线程读写文件：
	
6：No configuration setting found for key 'akka'
	<https://stackoverflow.com/questions/31011243/no-configuration-setting-found-for-key-akka-version>
	使用插件maven-shade-plugin,配置见具体项目；
	其中： 
	```xml
		<filters>
			<filter>
				<!--错误:Invalid signature file digest for Manifest main attributes-->
				<artifact>*:*</artifact>
				<excludes>
					<exclude>META-INF/*.SF</exclude>
					<exclude>META-INF/*.DSA</exclude>
					<exclude>META-INF/*.RSA</exclude>
				</excludes>
			</filter>
		</filters>
	```
	
7. java.util.concurrent.TimeoutException: Futures timed out after [100000 milliseconds]
	配置profile未修改为production发布，即还是使用new SparkConf().setMaster("local[*]").setAppName(appname)初始化SparkConf
	
8. Offset commit failed.
	org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	简单翻译一下，“位移提交失败，原因是消费者组开启了rebalance且已然分配对应分区给其他消费者。这表明poll调用间隔超过了max.poll.interval.ms的值，这通常表示poll循环中的消息处理花费了太长的时间。解决方案有两个：1. 增加session.timeout.ms值；2. 减少max.poll.records值”
	
	1. group.max.session.timeout.ms in the server.properties > session.timeout.ms in the consumer.properties.
	2. group.min.session.timeout.ms in the server.properties < session.timeout.ms in the consumer.properties.
	3. request.timeout.ms > session.timeout.ms and fetch.max.wait.ms
	4. (session.timeout.ms)/3 > heartbeat.interval.ms
	5. session.timeout.ms > Worst case processing time of Consumer Records per consumer poll(ms).
	
9. IntelliJ IDEA中提交svn时卡在 performing vcs refresh
	File > Invalidate Caches / Restart...   选择 Invalidate and Restart

10. kubeadm join token过期
	$ kubeadm token create
	abcdef.1234567890abcdef

	# get root ca cert fingerprint
	$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
	e18105ef24bacebb23d694dad491e8ef1c2ea9ade944e784b1f03a15a0d5ecea 





(运行支持kubernetes原生调度的Spark程序)[https://jimmysong.io/kubernetes-handbook/usecases/running-spark-with-kubernetes-native-scheduler.html]

############ AXY部署：
0. K8S环境部署 [](./kubeadm.html)
	
	重装需要清理环境：+
	```
		rm -rf /var/lib/cni/
		rm -rf /var/lib/kubelet/*
		rm -rf /etc/cni/
		ifconfig cni0 down
		ifconfig flannel.1 down
		ifconfig docker0 down
	```
	root下执行提示cr5什么的，是因为配置文件/root/.kube/xxx.config文件缺失

1. Ambari安装后，kafka执行程序路径：
	/usr/hdp/current/kafka-broker/bin
	./kafka-console-consumer.sh --bootstrap-server anxinyun-m1:6667,anxinyun-n1:6667,anxinyun-n2:6667 --topic anxinyun_data --from-beginning

	
2. Group coordinator anxinyun-n2:6667 (id: 2147482644 rack: null) is unavailable or invalid, will attempt rediscovery

	问题分析： stackwork提示 容器内hosts无法连接外部kafka broker
	解决：修改 kube-dns
	
	#kubectl get pod kube-dns-6db897f9f-gmlcz -n kube-system -o yaml --export -f dns/dns.yaml
	
	kubectl get deploy kube-dns -o yaml -n kube-system > kube-deploy.yaml
	kubectl get deploy coredns -o yaml -n kube-system > /tmp/coredns-dump.yaml
	
	添加一下内容：
	```
	dnsPolicy: Default
      hostAliases:
      - hostnames:
        - anxinyun-m1
        ip: 10.8.30.176
      - hostnames:
        - anxinyun-n1
        ip: 10.8.30.177
      - hostnames:
        - anxinyun-n2
        ip: 10.8.30.179
      nodeName: anxinyun-m1
	  ```
	  kubectl delete -f kube-deploy.yaml
	  kubectl create -f kube-deploy.yaml
	
3. [WARN] The short-circuit local reads feature cannot be used because libhadoop cannot be loaded
	同 local hdfs lib not found, 忽略

4. java.lang.ClassNotFoundException: scala.Product  
	运行docker的scala进程时报错
	> kafka 版本错误： 2.12->> 2.11
	
	Exception in thread "main" java.lang.ClassNotFoundException: a=1
	you need to provide exactly one argument: the class of the application supervisor actor
	
	> 修改了pom中的shade-pluging
	
	No configuration setting found for key 'akka.version'
	
	> shade-pluging中增加配置，
		<transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
			<resource>reference.conf</resource>
		</transformer>
		
5.[ERROR] [07/29/2018 05:51:33.289] [default-rediscala.rediscala-client-worker-dispatcher-5] [akka://default/user/RedisClient-$a] CommandFailed(Connect( anxinyun-n1:6379,None,List(KeepAlive(true)),None,false))
	k8s pod的yaml配置文件中，redis.host= anxinyun-n2 中间多了一个空格(MMP)
	
6.  KafkaConsumer is not safe for multi-threaded access

	KafkaConsumer和操作它的线程数必须是1对1的关系
	
7. Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: "spark-et-driver/10.244.1.19"; destination host is: "anxinyun-m1":9000;
	hdfs url配错，ambari默认8020而非9000

7.1  Checkpoint RDD has a different number of partitions from original RDD. Original RDD [ID: 10658, num of partitions: 2]; Checkpoint RDD [ID: 10685, num of partitions: 0].
	
	> 修改cp目录从本地文件格式为 hdfs://.... ....
	又出现 
		org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/user/hdfs/anxinyun-check-point/e7236205-88b8-4d0e-bb3c-12d2549f8ad6/rdd-371/.part-00000-attempt-3":hdfs:hdfs:drwxr-xr-x
			> submit添加如下参数
			--conf spark.kubernetes.driverEnv.SPARK_USER=hdfs \
			--conf spark.kubernetes.driverEnv.HADOOP_USER_NAME=hdfs \
			--conf spark.executorEnv.HADOOP_USER_NAME=hdfs \
			--conf spark.executorEnv.SPARK_USER=hdfs \
			
			还可以通过：
			sudo -u hdfs hdfs dfs -chmod -R 775 /user/check-point
			或者修改hdfs中超级用户组和权限配置：
			fs.permissions.umask-mode=002  dfs.permissions.superusergroup=hdfs   dfs.permissions=false(直接关闭权限验证)
			
			又出现
				Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try
				
				hdfs conf +++
			又出现
				org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /anxinyun/structure_data/structure_1/raw/2018/7/WSD#1.csv for DFSClient_NONMAPREDUCE_50217299_1 on 10.8.30.179 because DFSClient_NONMAPREDUCE_50217299_1 is already the current lease holder.

	[参考]:
		<https://community.hortonworks.com/questions/202369/datanode-failed-to-replace-a-bad-datanode-on-the-e.html>
		<https://community.hortonworks.com/articles/16144/write-or-append-failures-in-very-small-clusters-un.html>

				

8. OOMKilled
	spark driver在k8s中运行一段时间出现 OOMKilled
	未复现
	
9. WEB UI （spark on k8s）
	kubectl port-forward spark-et-driver -n anxinyun  4040:4040
	仅能在driver机器上通过localhost:4040访问
	
	
第二次部署踩坑(08.01)
1. ubuntu系统无法进入
	所有文件处于只读状态
	可以通过 mount -o remount / 方法临时修改，但是重启后依旧
	解决方法：重装系统
	
2. hdb-selecter not found 
	`ambari仓库配置错了`
	'hadoop-client ... failed, ' error param conf
	`/etc/hadoop/conf文件丢失了，重装ambari时会发生的错误，解决方法：手动拷贝`
	kafka-broker启动失败
	``

3. k8s安装完成后所有节点处于NotReady状态
	kubectl get nodes -o wide
	发现kenerl和k8s版本不一致
	解决方法：系统upgrade后，执行以下命令：
kubeadm reset 
systemctl stop kubelet
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /run/flannel
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ip link delete cni0
ip link delete flannel.1
apt purge -y kubelet kubeadm kubectl kubernetes-cni 
systemctl daemon-reload
apt install kubernetes-cni=0.5.1-00 kubelet=1.8.2-00 kubeadm=1.8.2-00 kubectl=1.8.2-00
		sudo wget http://218.3.126.49:18088/dist/install-towercrane.sh -O install.sh && chmod +x install.sh && ./install.sh
		修改配置
		vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

		添加：
		Environment="KUBELET_EXTRA_ARGS=--fail-swap-on=false"
		重启服务
systemctl daemon-reload
systemctl restart kubelet
		关闭交互分区

		swapoff -a  

		删除 /etc/fstab 表中的交换分区记录
		
		kubeadm init --pod-network-cidr=10.244.0.0/16 --skip-preflight-checks --kubernetes-version=1.8.2
		
		kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
		
		-- kubeadm join --token 54333d.2ce641d3fbefdb3d 10.8.30.36:6443 --discovery-token-ca-cert-hash sha256:7b3d09b3d359cef255fbbddd147618c4c68ea6374fdd05a0005253256ec1ad18
	
10. 其他错误
	lient Version: version.Info{Major:"1", Minor:"8", GitVersion:"v1.8.2", GitCommit:"bdaeafa71f6c7c04636251031f93464384d54963", GitTreeState:"clean", BuildDate:"2017-10-24T19:48:57Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
	Error from server (NotFound): the server could not find the requested resource
	
	
	vents:
  Type     Reason                 Age                From                  Message
  ----     ------                 ----               ----                  -------
  Normal   Scheduled              11m                default-scheduler     Successfully assigned spark-et-driver to anxinyun-n2
  Normal   SuccessfulMountVolume  11m                kubelet, anxinyun-n2  MountVolume.SetUp succeeded for volume "download-jars-volume"
  Normal   SuccessfulMountVolume  11m                kubelet, anxinyun-n2  MountVolume.SetUp succeeded for volume "download-files-volume"
  Warning  FailedMount            11m                kubelet, anxinyun-n2  MountVolume.SetUp failed for volume "spark-init-properties" : configmaps "spark-et-94a3f73e5af13b2d83fb23ba02973cda-init-config" not found
  Normal   SuccessfulMountVolume  11m                kubelet, anxinyun-n2  MountVolume.SetUp succeeded for volume "anxinyun-token-vh5hj"
  Normal   SuccessfulMountVolume  11m                kubelet, anxinyun-n2  MountVolume.SetUp succeeded for volume "spark-init-properties"
  Normal   Pulling                10m (x3 over 11m)  kubelet, anxinyun-n2  pulling image "registry.zhiwucloud.com/anxinyun/spark:latest"
  Warning  Failed                 10m (x3 over 11m)  kubelet, anxinyun-n2  Failed to pull image "registry.zhiwucloud.com/anxinyun/spark:latest": rpc error: code = Unknown desc = Error response from daemon: manifest for registry.zhiwucloud.com/anxinyun/spark:latest not found
  Warning  FailedSync             10m (x8 over 11m)  kubelet, anxinyun-n2  Error syncing pod
  Normal   BackOff                1m (x43 over 11m)  kubelet, anxinyun-n2  Back-off pulling image "registry.zhiwucloud.com/anxinyun/spark:latest"
  
  
## 测试环境合并
1. kubernetes Network Plugin cni failed
	kubernetes flannel Failed to find any valid interface to use
	节点上的flannel pod一直处于 Init:CrashLoopBackOff
	
	解决方法： 判断节点网络是否有问题(能否访问外网、解析域名)
			重装节点kubenertes(上3)

			
## 代码重构1.0 
IDEA 2017.1.6 et
1. scala project Error:java: Compilation failed: internal java compiler error
	
	设置 > java compiler > 设置jdk版本
	
2. Diamond type are not supported at this language level
	Ctrl+Alt+Shift+S (项目设置 ) > Language Level > 8 - Lambdas,type annotations etc.	
	
3.  HDFS文件的错误：No FileSystem for scheme: hdfshttps://www.codelast.com/%E5%8E%9F%E5%88%9B-%E8%A7%A3%E5%86%B3%E8%AF%BB%E5%86%99hdfs%E6%96%87%E4%BB%B6%E7%9A%84%E9%94%99%E8%AF%AF%EF%BC%9Ano-filesystem-for-scheme-hdfs/

4. Exception in thread "main" java.lang.NoClassDefFoundError: scala/Function1
	添加引用
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.11.8</version>
        </dependency>
		

5. java项目resource中文件复制到target/class目录下后内容发生了改变(例如https的cacerts.store文件)
	原因： maven resouce的filter插件
	修改为：
	```xml
        <resources>
            <resource>
                <filtering>true</filtering>
                <directory>../src/main/resources</directory>
            </resource>
            <resource>
                <filtering>true</filtering>
                <directory>${project.basedir}/src/main/resources</directory>
                <includes>
                    <include>*.properties</include>
                </includes>
            </resource>
            <resource>
				<!-- 不使用过滤的资源文件 -->
                <filtering>false</filtering>
                <directory>${project.basedir}/src/main/resources</directory>
                <excludes>
                    <exclude>*.properties</exclude>
                </excludes>
            </resource>
        </resources>
	```

6. et_upload 中创建多个kafkastream时接收不到数据
	??
	
7. postman请求https无响应
	postman设置，关闭 SSL certificate verification
	
以后：
1. Caused by: java.io.IOException: Cannot run program "I:\@\hadoop-2.7.3\bin\winutils.exe": CreateProcess error=5, 拒绝访问。
	在windows机器中安装hadoop，参见[3]

2. Offsets out of range with no configured reset policy for partitions (spark-stream kafka)
	https://www.jianshu.com/p/40aee290f484
	
3. windows上安装hadoop （问题1解决思路）
	https://blog.csdn.net/rav009/article/details/70214788
	> https://mirrors.cnnic.cn/apache/hadoop/common/ 下载hadoop镜像并解压
	> https://github.com/steveloughran/winutils 下载对应winutils 覆盖到bin
	> 设置 HADOOP_HOME 以及 Path
	> 修改etc/hadoop-env.cmd下
	  set JAVA_HOME=C:\PROGRA~1\Java\jdk1.8.0_121
	> core-site:
	 <configuration>
			<property>
					<name>fs.defaultFS</name>
					<value>hdfs://localhost:9000</value>
			</property>
	 </configuration>
	> hdfs-site.xml
		<configuration>
				<property>
						<name>dfs.replication</name>
						<value>1</value>
				</property>
				<property>
						<name>dfs.namenode.name.dir</name>
						<value>file:/hadoop/data/dfs/namenode</value>  相同盘符根目录下响应位置
				</property>
				<property>
						<name>dfs.datanode.data.dir</name>
						<value>file:/hadoop/data/dfs/datanode</value>
				</property>
		</configuration>
	> hadoop namenode -format
	  如果不行就修改数据文件夹的权限(everyone)
	> cd sbin
	> start-dfs.cmd
	> OK

4. could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
	怀疑存储空间不足，待验证
	
5. Too many dynamic script compilations within one minute (ElasticSearch)
	重启进程
	
6. mqtt receiver进程重复接收数据；
	savoir的receiver进程中仅订阅了savoir_data，确会收到anxinyun_data的数据
	怀疑：client.id之前订阅过anxinyun_data主题
	解决方法： 修改client.id
	
7. [ERROR] Caused by: java.lang.AssertionError: assertion failed: org.joda.convert.ToString
	scala maven项目编译能通过，执行mvn install时提示错误：
	除了引用joda-time库外还需要引用 joda-convert库
	
8. 安心云ET运行一段时间报错：[Executor] Executor self-exiting due to : Driver spark-et-baed51c5f49f33339750c4602718b303-driver-svc.anxinyun.svc:7078 disassociated! Shutting down
	应该是OOM后被kill,   继续排查spark内存泄漏的问题
	参见[https://www.jianshu.com/p/80dc6209acc0]
	
9. Offsets out of range with no configured reset policy for partitions


10. receiver进程 Lost connection. reconnecting...
Client is connected (32100)
	at org.eclipse.paho.client.mqttv3.internal.ExceptionHelper.createMqttException(ExceptionHelper.java:31)
	at org.eclipse.paho.client.mqttv3.MqttAsyncClient.connect(MqttAsyncClient.java:731)
	
	在mqtt断连回调事件中，将connect方法修改为reconnect
	*(继续跟踪是否解决)
	
11. spark monitor:
记录
http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.SparkListener

12. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2727.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2727.0 (TID 1122, 10.244.1.109, executor 1): java.lang.NoClassDefFoundError: Could not initialize class et.agg.AggConfig$
 在商用环境用K8S部署ET时出错，同样版本在测试环境正常
 因为在AggConfig初始化代码中出现异常，JSON反序列化构造对象后调用redis初始部分属性超时（测试环境没有超时）

13. java.lang.IllegalStateException: Previously tracked partitions [anxinyun_data2-0] been revoked by Kafka because of consumer rebalance. This is mostly due to another stream with same group id joined, please check if there're different streaming application misconfigure to use same group id. Fundamentally different stream should use different group id
	在商用环境同时起2个K8S-ET，出错
	
14. ES启动失败：bootstrap checks failed  max virtual memory areas vm.max_map_count [65530] is too low
	sudo sysctl -w vm.max_map_count=262144 [ref](https://github.com/docker-library/elasticsearch/issues/111)

15. ES脑裂
	https://my.oschina.net/LucasZhu/blog/1543971
	在商用环境重启ES进程(jvm.Xmx设置),导致重启后没有自动组成集群
	ET入库时显示数据插入成功，查询时发现数据丢失
	
16. java内存管理
	BIG ISSUE
	没有设置xmx，默认(java8) 1/6 physical memory ~ 1/4 physical memory
	You can Check the default Java heap size by:
	java -XX:+PrintFlagsFinal -version | grep -iE 'HeapSize|PermSize|ThreadStackSize'
	
	默认GC规则
	Default garbage collectors:
	Java 7 - Parallel GC
	Java 8 - Parallel GC
	Java 9 - G1 GC
	Java 10 - G1 GC
	[gcpic](https://i.stack.imgur.com/XHfx0.jpg)

	【A Templator Handler】：AXY3.0部分java进程加上默认限制：（recalc,et-hdfs,abn,analyse,fc-alarm,aggregation）
	-Xmx2G -Xms2G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps
	
17. KAFKA IN SPARK-STREAMING常见问题：
	1). 提交OFFSET Rebalance的告警
	
	2).xxxx been revoked by Kafka because of consumer rebalance. This is mostly due to another stream with same group id joined,please check if there're different streaming application misconfigure to use same group id. Fundamentally different stream should use different group id
	
	3). Attempt to heart beat failed since the group is rebalancing, try to re-join group
	
	4). Offsets out of range with no configured reset policy for partitions
		spark-stream的executor中auto.offset.reset置为none,所以当groupid中记录的offset超出记录的上下边界，报此错误
		https://www.jianshu.com/p/40aee290f484
	
18. java.lang.OutOfMemoryError: unable to create new native thread
	spark程序执行过程中出现HDFS相关错误
	修改linux open files限制，设置 ulimit -n xxxx提升限制，ulimit -a查看
	
19. 安心云ES存储代码复制到知物云出现 NoAvaliable Nodes 错误
	ES库版本问题(zhiwu-5.5  anxinyun-6.5)
		<dependency>
            <groupId>org.elasticsearch.client</groupId>
            <artifactId>transport</artifactId>
            <version>6.2.4</version>
        </dependency>
	
20. flatMap && flatten
	// flatMap  TraversableLike.scala

	  def flatMap[B, That](f: A => GenTraversableOnce[B])(implicit bf: CanBuildFrom[Repr, B, That]): That = {
		def builder = bf(repr) // extracted to keep method size under 35 bytes, so that it can be JIT-inlined
		val b = builder
		for (x <- this) b ++= f(x).seq
		b.result
	  }
	  
	// flatten  GenericTraversableTemplate.scala

	  def flatten[B](implicit asTraversable: A => /*<:<!!!*/ GenTraversableOnce[B]): CC[B] = {
		val b = genericBuilder[B]
		for (xs <- sequential)
		  b ++= asTraversable(xs).seq
		b.result()
	  }

	两者功能上几乎没有区别
	通过 ++= 枚举元素进行添加，Map的相同键的元素会覆盖

21. ElasticSearch: [script] Too many dynamic script compilations within, max: [75/5m]
	修改ES配置 5.x max_compilations_per_minute   6.x script.max_compilations_rate
	动态修改:
	PUT /_cluster/settings
	{
		"transient" : {
			"script.max_compilations_rate" : "750/5m"
		}
	}
	
	5.5.1
	curl -XPUT iota-m2:9200/_cluster/settings -d '{"persistent" : {"script.max_compilations_per_minute" : "50"}}'
	
## APACHE SPARK -> FLINK
1. flink中使用joda.DateTime
	flink程序启动时提示 `class org.joda.time.DateTime does not contain a getter for field iMillis`
	  `Class class org.joda.time.DateTime cannot be used as a POJO type because not all fields are valid POJO fields`
	 因为DateTime中iMillis字段没有setter和getter，所以flink不能将其视为POJO类型（更多flink支持数据类型参见 https://ci.apache.org/projects/flink/flink-docs-stable/dev/types_serialization.html）
	 
	 如果使用了检查点（checkpoint）机制，需要设置jodatime的kyro序列化机制（跟spark-streaming是一样的处理）

2. flink启动过程出现NPE （NullPointException）
	因为代码里我将一个case class设置为null了，后将其改成Option[case class]的类型问题解决
	建议：如果数据结构中可能存在空（null）的情况，使用类型Option而不是直接用null赋值
	
3. java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
	在IDEA中程序可以执行，而打包的jar包无法运行
	你必须指定一种SLF4J的实现jar包在你的classpath里面，和接口jar包一样
	https://stackoverflow.com/questions/12926899/java-lang-noclassdeffounderror-org-slf4j-loggerfactory

	<!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12 -->
	<dependency>
		<groupId>org.slf4j</groupId>
		<artifactId>slf4j-log4j12</artifactId>
		<version>1.7.25</version>
		<scope>test</scope>
	</dependency>
	
	或者查看是否在maven shade插件中过滤了log相关jar

4. scala项目必须在Maven配置文件中指定scala.version属性
	
    <properties>
        <scala.version>2.11</scala.version>
    </properties>

5. flink程序(k8s)启动后无法读取kafka
	flink java.lang.RuntimeException: Unable to retrieve any partitions with KafkaTopicsDescriptor
	或者
	Committing offsets to Kafka takes longer than the checkpoint interval. Skipping commit of previous offsets because newer complete checkpoint offsets are available. This does not compromise Flink's checkpoint integrity.
	
	检查K8S DNS问题

6. flink中报ES超时错误 [NOT SOLVED]
	告警进程重启后又积压数据，报ES超时错误
	java.lang.RuntimeException: An error occurred in ElasticsearchSink.
	...
	Caused by: java.io.IOException: request retries exceeded max retry timeout [30000]
6.5. flink alarm kafka数据接收中断
	jobmanager报错： Checkpoint 1066 of job 4e65b91bc4ff8a0dbdc2ffd422e1d471 expired before completing
	
7. hdfs 文件存储错误(et-hdfs进程)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /savoir/structure_data/structure_1116/raw/2019/1/液压式静力水准仪-13.csv for DFSClient_NONMAPREDUCE_1314822252_27 on 10.8.25.214 because this file lease is currently owned by DFSClient_NONMAPREDUCE_1678645347_32 on 10.8.25.232

8. kubenertes维护
	 journalctl -eu kubelet
	 上不了网怎么办 route add default gw 10.8.30.1
	 service firewalld stop
	 service ufw stop
	 ufw disable
	 systemctl disable firewalld
	 
9. hdfs故障
	hdfs put错误：java.io.EOFException: Premature EOF: no length prefix available
	org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033460583-192.168.0.109-1538992976843:blk_1073805060_77492, type=LAST_IN_PIPELINE, downstreams=0:[]
	java.io.IOException: Failed to move meta file for ReplicaBeingWritten, blk_1073805060_77492, RBW
	  getNumBytes()     = 714
	  getBytesOnDisk()  = 714
	  getVisibleLength()= 714
	  getVolume()       = /home/fs/ds/hadoop/data/current
	  getBlockFile()    = /home/fs/ds/hadoop/data/current/BP-1033460583-192.168.0.109-1538992976843/current/rbw/blk_1073805060
	  bytesAcked=714
	  bytesOnDisk=714 from /home/fs/ds/hadoop/data/current/BP-1033460583-192.168.0.109-1538992976843/current/rbw/blk_1073805060_77492.meta to /home/fs/ds/hadoop/data/current/BP-1033460583-192.168.0.109-1538992976843/current/finalized/subdir38/subdir0/blk_1073805060_77492.meta
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.moveBlockFiles(FsDatasetImpl.java:460)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LDir.addBlock(LDir.java:78)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LDir.addBlock(LDir.java:92)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LDir.addBlock(LDir.java:92)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LDir.addBlock(LDir.java:71)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.addBlock(BlockPoolSlice.java:248)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlock(FsVolumeImpl.java:199)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeReplica(FsDatasetImpl.java:956)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:937)
		at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1236)
		at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1196)
		at java.lang.Thread.run(Thread.java:748)
	Caused by: EIO: Input/output error
		at org.apache.hadoop.io.nativeio.NativeIO.renameTo0(Native Method)
		at org.apache.hadoop.io.nativeio.NativeIO.renameTo(NativeIO.java:828)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.moveBlockFiles(FsDatasetImpl.java:458)
		... 11 more
		
10. jenkins nodejs打包报错：Cannot find module 'internal/utils/type'
	npm cache clean --force
	
11. dns错误
	代理报错：
	error: [FS-ERRHD] Error: getaddrinfo EAI_AGAIN console.theiota.cn:443
		at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:67:26)
	修改系统dns
	1. vi /etc/network/interfaces   /etc/init.d/networking restart
	2. vi /etc/resolvconf/resolv.conf.d/head  ++ nameserver 223.5.5.5    resolvconf -u   cat /etc/resolv.conf
	重启k8s dns

12. flink alarm error:
	未解决
	java.lang.Exception: Cannot deploy task Source: Custom Source -> Map -> Filter -> Map (1/3) (f25b30cf14999b76b9b0027000775c8f) - TaskManager (7f9dc13bd4fee89dac6f12a2960de37b @ flink-taskmanager-57f69bfd69-p5phv (dataPort=46709)) not responding after a rpcTimeout of 10000 ms
	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$5(Execution.java:624)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@flink-taskmanager-57f69bfd69-p5phv:41271/user/taskmanager_0#1593533421]] after [10000 ms]. Sender[null] sent message of type "org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	
	差不多的问题：
		2019-06-14 07:42:01,480 WARN  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Could not send heartbeat to target with id 3f19b797fd46f27a86f571080bca7f7a.
	java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/taskmanager_0#1723018049]] after [10000 ms]. Sender[null] sent message of type "org.apache.flink.runtime.rpc.messages.CallAsync".
		at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
		at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
		at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:647)
		at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632)
		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
		at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
		at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:772)
		at akka.dispatch.OnComplete.internal(Future.scala:258)
		at akka.dispatch.OnComplete.internal(Future.scala:256)
		at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
		at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
		at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
		at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
		at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
		at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
		at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
		at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
		at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
		at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
		at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
		at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
		at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
		at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
		at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
		at java.lang.Thread.run(Thread.java:748)
	Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/taskmanager_0#1723018049]] after [10000 ms]. Sender[null] sent message of type "org.apache.flink.runtime.rpc.messages.CallAsync".
		at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
		... 9 more

13. flink error:
	未解决
	Association with remote system [akka.tcp://flink-metrics@flink-taskmanager-] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink-metrics@flink-taskmanager-]] Caused by: [flink-taskmanager- Name does not resolve]
		
		
	2019-05-05 11:45:09,250 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink-metrics@flink-taskmanager-57f69bfd69-xr466:34809] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink-metrics@flink-taskmanager-57f69bfd69-xr466:34809]] Caused by: [flink-taskmanager-57f69bfd69-xr466: Name does not resolve]
	2019-05-05 11:45:09,250 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink-metrics@flink-taskmanager-57f69bfd69-5ksrq:34919] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink-metrics@flink-taskmanager-57f69bfd69-5ksrq:34919]] Caused by: [flink-taskmanager-57f69bfd69-5ksrq: Name does not resolve]
	
	排查：
	检查jobmanager容器内部hosts，仅发现 
	```shell
	# Kubernetes-managed hosts file.
	127.0.0.1       localhost
	...
	10.244.3.172    flink-jobmanager-7d469bccdf-mfbh5
	```
14. Flink 启动后无法读取kafka
	环境(49华为云) 
	现象：
	Committing offsets to Kafka takes longer than the checkpoint interval. Skipping commit of previous offsets because newer complete checkpoint offsets are available. This does not compromise Flink's checkpoint integrity.
	
	 Error registering AppInfo mbean
	javax.management.InstanceAlreadyExistsException: kafka.consumer:type=app-info,id=consumer-2
	原因：
	
	
15. DOCKERFILE中EntryPoint中执行bash脚本报错：
	"exec format error"  -> 文件头加上 #!/bin/bash
	"file not found" -> 可能是windows下创建的文件格式不正确，需要在linux下创建该文件

16. 告警flink进程疑似内存溢出中断
	OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.
	OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=
	CodeCache: size=245760Kb used=229411Kb max_used=235061Kb free=16348Kb
	 bounds [0x00007f61b7de6000, 0x00007f61c6de6000, 0x00007f61c6de6000]
	 total_blobs=63125 nmethods=62493 adapters=542
	 compilation: enabled
	 
	 
15. 告警flink进程redis报错：
	akka.ConfigurationException: Dispatcher [rediscala.rediscala-client-worker-dispatcher] not configured
	对比可运行版本和当前版本jar解压文件，发现reference.conf文件差异(新包中不含redisscala配置，是被quartz的配置替换了(reference.conf文件重名了!))
	解决方法：shade构建时使用插件：
	
	<transformer
			implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
		<resource>reference.conf</resource>
	</transformer>
	
16. ambari里启动hdfs的namenode时出现 | grep 'Safe mode is OFF'' returned 1.
	解决方法： 关闭HDFS的safemode； 
	$ su hdfs
	$ hdfs dfsadmin -safemode leave
	
17. kubernetes创建pod一直处于pending状态，describe显示No nodes are available that match all of the predicates: Insufficient memory (3), PodToleratesNodeTaints (1).
	> PodToleratesNodeTaints
	/etc/systemd/system/kubelet.service.d/10-kubeadm.conf
	+ Environment="DEF_ARGS=--max-pods=40"
	+ ExecStart=/usr/bin/kubelet 。。。$DEF_ARGS
	
18. Redis-akka ,ET 启动测试用例时报错 :java.lang.ClassNotFoundException: akka.event.slf4j.Slf4jLog
	手动增加引用：
	
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-slf4j_2.11</artifactId>
            <version>2.4.12</version>
        </dependency>
		
19. java.lang.NoClassDefFoundError: org/elasticsearch/common/transport/InetSocketTransportAddress
	启动重计算进程时出现上述错误：
	recalc进程又独立引用了ElasticSearch-Client6.3，去掉
	comm_util已经引用
	
20. 安装k8s 14.3 
	```bash
	apt-get install -y kubelet=1.14.3-00 kubeadm=1.14.3-00 kubectl=1.14.3-00 kubernetes-cni=0.7.5-00
	docker pull repository.anxinyun.cn/k8s/kube-apiserver:v1.14.3
	docker pull repository.anxinyun.cn/k8s/kube-controller-manager:v1.14.3
	docker pull repository.anxinyun.cn/k8s/kube-scheduler:v1.14.3
	docker pull repository.anxinyun.cn/k8s/kube-proxy:v1.14.3
	docker pull repository.anxinyun.cn/k8s/pause:3.1
	docker pull repository.anxinyun.cn/k8s/etcd:3.3.10
	docker pull repository.anxinyun.cn/k8s/coredns:1.3.1
	
	docker tag repository.anxinyun.cn/k8s/kube-apiserver:v1.14.3 k8s.gcr.io/kube-apiserver:v1.14.3
	docker tag repository.anxinyun.cn/k8s/kube-controller-manager:v1.14.3 k8s.gcr.io/kube-controller-manager:v1.14.3
	docker tag repository.anxinyun.cn/k8s/kube-scheduler:v1.14.3 k8s.gcr.io/kube-scheduler:v1.14.3
	docker tag repository.anxinyun.cn/k8s/kube-proxy:v1.14.3 k8s.gcr.io/kube-proxy:v1.14.3
	docker tag repository.anxinyun.cn/k8s/pause:3.1 k8s.gcr.io/pause:3.1
	docker tag repository.anxinyun.cn/k8s/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10
	docker tag repository.anxinyun.cn/k8s/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
	
	MASTER:
	kubeadm init --kubernetes-version=1.14.3 --apiserver-advertise-address=192.168.1.212 --pod-network-cidr=10.244.0.0/16
	
	NODES:
	kubeadm join 10.8.30.36:6443 --token g76iaz.vh3mxrtn9873w4hs --discovery-token-ca-cert-hash sha256:81655beaf8744a1ee840e7fa7d6a8083b5c204dae7c7494d00d6b27860e91a08
	```
	
	
21. 启动Flink Alarm时报错
	[未解决]
	Failed Elasticsearch bulk request: Request cannot be executed; I/O reactor status: STOPPED
	？？ 原因未知
	重启后好了

22. HDFS写文件报错
	org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot append to file/savoir/structure_data/structure_2043/raw/2019/9/轴力计ZCL-2-3.csv. Name node is in safe mode.
	Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off
	
	NameNode资源清理(dalianggejiejue)
	
23. Recalc进程异常
	日志记录 ElasticSearch中
	Exception in thread "Thread-3" Exception in thread "Thread-4" java.lang.ExceptionInInitializerError
		at org.apache.logging.log4j.core.impl.Log4jLogEvent.createContextData(Log4jLogEvent.java:472)
		
24. Flink进程报错
	[未解决]
	org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: No task slot allocated for job ID 86f84f68151c4664b0bfbc9d9886eaa2 and allocation ID AllocationID{6ad84c35694804b38fd3181398ef7d9b}.
		at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:458)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:247)
		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:162)
		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
		at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
		at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
		at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
		at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
		at akka.actor.ActorCell.invoke(ActorCell.scala:495)
		at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
		at akka.dispatch.Mailbox.run(Mailbox.scala:224)
		at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
		at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
		at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
		at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
		at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
		
25. HDFS写文件报错java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:808)
	[尝试中]
	https://www.playpi.org/2018122701.html
	
	当任务提交到集群上面以后，多个 datanode 在 getFileSystem 过程中，由于 Configuration 一样，会得到同一个 FileSystem。如果有一个 datanode 在使用完关闭连接，其它的 datanode 在访问时就会出现上述异常，导致数据缺失（如果数据恰好只存在一个 datanode 上面，可能没问题）。
	
	
	*************************************************
	*************************************************
	*************************************************
	*********************W O R K*********************
	*************************************************
	*************************************************
	
	1. ET有原始数据，没有主题数据：
	 > 查看ET日志,看是否有异常(公式计算错误)
	 > 查看设备绑定(是否绑定输出到监测原型)
	 > 是否分组
	 > 是否过滤(滑窗 blabla..)
	 > 设备绑定是旧的(设备输出字段改过，工具ProductManager需要删除_cache重启)
	 
	 
26. ★★★★ Flink 中使用 Lazing ★★★★
	TaskManager中执行任务，好像并不能调用 启动Arguments中参数  <告警Flink中参数启用Extras字段实际无效的BUG>
	
27. ET  KAFKA报错
	[WARN ] 2019-11-20 11:29:48,924 method:org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse(NetworkClient.java:968)
	[Consumer clientId=consumer-1, groupId=et.mains3] Error while fetching metadata with correlation id 38 : {savoir_alarmMsg=LEADER_NOT_AVAILABLE, savoir_data=LEADER_NOT_AVAILABLE, savoir_agg=LEADER_NOT_AVAILABLE}
	[WARN ] 2019-11-20 11:29:48,924 method:org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse(NetworkClient.java:968)
	[Consumer clientId=consumer-1, groupId=et.mains3] Error while fetching metadata with correlation id 38 : {savoir_alarmMsg=LEADER_NOT_AVAILABLE, savoir_data=LEADER_NOT_AVAILABLE, savoir_agg=LEADER_NOT_AVAILABLE}

28. deliver kafka提交超时
	[WARN ] 2019-11-26 11:15:04,314 Auto-commit of offsets {anxinyun_deliver-0=OffsetAndMetadata{offset=20642, metadata=''}} failed for group deliver1: Offset commit failed with a retriable exception. You should retry committing offsets. The underlying error was: The request timed out.

	2019-11-26 14:23:44,500 Auto-commit of offsets {anxinyun_deliver-0=OffsetAndMetadata{offset=20642, metadata=''}} failed for group deliver1: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.

29. MQTT big payload
	问题描述：
	mqtt接收消息超过500k时；接收进程连接断开，之后一直尝试重连
	问题排查过程：
	使用不同的mqtt配置参数和尝试不同的mqtt代理：
	<clean.session:false QoS:2 代理Mosquitto>  x
	<clean.session:false QoS:1 代理Mosquitto>  x
	<clean.session:false QoS:0 代理Mosquitto>  x
	<clean.session:false QoS:2 代理EMQ>  x ps.一个卵样
	<clean.session:false QoS:1 代理EMQ>  x
	<clean.session:false QoS:0 代理EMQ>  x
	<修改发送端的QoS，同样无效>
	<修改EMQX的mqtt.max_packet_size=100MB， 同样无效>
		 env:
        - name: EMQX_MQTT__MAX_PACKET_SIZE
          value: 100MB
	<尝试更换java MQTT-client库，Eclipse Paho ->FuseSource MqttClient >
		https://github.com/eclipse/paho.mqtt.java
		https://github.com/fusesource/mqtt-client
		https://www.jianshu.com/p/95b4e349cde4
		
	√√√ 更换后mqtt接受不再报错，但是kafka报错了：
	java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.RecordTooLargeException: The request included a message larger than the max message size the server will accept.
	kafka服务无法接收超过最大限制的数据包，于是谷歌后修改
		https://my.oschina.net/shyloveliyi/blog/1620012
		配置kafka server：
			message.max.bytes=12695150
		配置后错误没有了！！
		
	基于以上测试结果，于是在代码里用工厂模式实现了两种库的调用，开始自测，结果两种库的实现都能实现1M以上包的传输了（蒙比了）；
	怀疑是kafka的那个异常被Paho内部吃掉了，导致Socket异常重启，于是kafka设置改回去再测试，果然Paho又报错开始重连了
	解决办法:
	kafkaProducer.send方法用 try-catch包住
	引申：
	之前进程也时常出现重连的问题，应该也是kafka-send超时之类的问题导致的.
	总结：
	这是一个异常未捕捉引发的血案，所以一切IO操作一定要做好异常处理，不然被第三方包吃掉异常，故障就真的不好定位了。
	
30. k8s节点无法加入：
	Unable to update cni config: No networks found in /etc/cni/net.d
	https://www.cnblogs.com/zhongyuanzhao000/p/11401031.html
	方法一：
	​ 编辑 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf文件（有的是/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf文件)，
	​ 删除最后一行里的$KUBELET_NETWORK_ARGS 即可。
	​ （该方法治标不治本，没能解决我的问题）

	方法二：
	​ 考虑到node节点的kubelet报错Unable to update cni config: No networks found in /etc/cni/net.d，并且master节点的/etc/cni/net.d目录里拥有10-flannel.conflist文件，
	​ 我们可以把该文件从master节点复制到node节点的对应目录中，然后重启kubelet服务即可。
	​ (该方法亡羊补牢，对我的问题也没用)

	方法三：
	​ 注意到master节点的/etc/cni/net.d/10-flannel.conflist文件是经过 安装flannel插件 才生成的，而node节点则是在master节点安装完flannel插件后才加入集群的，
	​ 所以，我们不妨 先加入node节点，再执行 flannel插件的安装
	
	ps.重装k8s 14:
	++ rm -rf /etc/kubernetes/
	++ apt-get install -y kubelet=1.14.3-00 kubeadm=1.14.3-00 kubectl=1.14.3-00 kubernetes-cni=0.7.5-00
	++ kubeadm join --token qy5bc1.x7ujv4ej14k6gl19 10.8.30.157:6443 --discovery-token-ca-cert-hash sha256:a78c42f3349699dff04b2f079968b126c030259974aae5d40813e2af0057c2dc
	
	
31. Kafka 接收不到数据(et-upload)
	[WARN ] 2019-12-13 10:21:27,763 method:org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$4.onComplete(ConsumerCoordinator.java:649)
Auto-commit of offsets {anxinyun_theme-0=OffsetAndMetadata{offset=97109483, metadata=''}} failed for group et.upload: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.

32. NoSuchMethodError
	Exception in thread "main" java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.metric()
	
	问题“：依赖包冲突了！！
	解决：mvn dependency:tree -Dverbose> dependency.log  查看依赖树，查找对应冲突包的位置
	
33. Jenkins启动
	sudo nsenter --target `sudo docker inspect -f {{.State.Pid}} b0a28a922e07` --mount --uts --ipc --net --pid
	chown 1000:1000 /var/run/docker.sock
	
35. 数据网盘hdfs上文件无法下载
	在hdfs browser里面访问返回：
	No active nodes contain this block

	查看ambari hdfs服务状态，anxinyun-n2节点过了，先把它拉起来
	恢复了

	如果还是没有恢复，建议参考
	https://stackoverflow.com/questions/19205057/how-to-fix-corrupt-hdfs-files
	如果丢失块太多或进入‘安全模式’（只读），手动关闭
	hadoop dfsadmin -safemode leave

36. Flink HA on kubernetes
[官方文档 Standalone/Yarn](https://ci.apache.org/projects/flink/flink-docs-stable/ops/jobmanager_high_availability.html#bootstrap-zookeeper)
org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'hdfs'. The scheme is not directly supported by Flink and no Hadoop file system to support this scheme could be loaded.
拉取的镜像不支持HADOOP
DockerHub上没有对应1.9版本的Flink Hadoop版本。怎么办？

安装nfs服务：
(https://blog.csdn.net/dyzhen/article/details/90693651)
sudo apt install nfs-kernel-server

Service temporarily unavailable due to an ongoing leader election. Please refresh.


37. redis 木马 被黑 病毒
ET报错：
class et.recv.RecvDataHanler$e9e19861-c48b-47c5-96f9-bddff1ce0ef7 empty data received
redis.actors.ReplyErrorException: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
	at redis.actors.RedisReplyDecoder$$anonfun$decodeRedisReply$1.apply(RedisReplyDecoder.scala:68)
	at redis.actors.RedisReplyDecoder$$anonfun$decodeRedisReply$1.apply(RedisReplyDecoder.scala:67)
查看redis状态：
> redis-cli
> config get dir
 "/var/spool/cron"
 连接上去发现数据目录被修改,百度发现是redis被攻击 https://blog.csdn.net/qq_34326603/article/details/103294932
因为不确定是服务进程被黑还是原redis端口6379暴露，解决方法是：
1. 用docker容器创建redis服务，映射之前的redis数据文件；2. 并将端口修改为6378； 3. 修改平台应用配置并重启进程。

后续：
定位是怎么被攻击的。 redis需要加入k8s，通过内部服务的方式提供，不对外暴露端口。

38. C# 程序启动报错
尝试从一个网络位置加载程序集，在早期版本的 .NET Framework 中，这会导致对该程序集进行沙盒处理。此发行版的 .NET Framework 默认情况下不启用 CAS 策略，因此，此加载可能会很危险。
在app.config中增加“
<runtime><loadFromRemoteSources enabled="true"/></runtime>

39. flink 报错
java.lang.Exception: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator

Caused by: java.lang.NullPointerException
	at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
	at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
	
	https://zhuanlan.zhihu.com/p/90807542
	目前我所知的不支持NULL的类型包括Scala Option、Java/Scala Tuple和Scala Case Class
	
40. ES warning
 returned 2 warnings: [299 Elasticsearch-6.8.5-78990e9 "Trying to create more than 500 scroll contexts will not be allowed in the next major version by default. You can change the  [search.max_open_scroll_context] setting to use a greater default value or lower the number of scrolls that you need to run in parallel."],[299 Elasticsearch-6.8.5-78990e9 "'y' year should be replaced with 'u'. Use 'y' for year-of-era. Prefix your date format with '8' to use the new specifier."]
 
 解决方法：
 curl -X PUT http://10.12.42.153:9200/_cluster/settings -H 'Content-Type: application/json' -d'{
    "persistent" : {
        "search.max_open_scroll_context": 5000
    },
    "transient": {
        "search.max_open_scroll_context": 5000
    }
}
'

41. TODO 数据库连接失败 Flink不会自动恢复


42. 3.0直接改factor后遗症记录：
	修改sensor表；
	修改group表；
	item id变化导致：
		t_sensor_factor_threshold
		
43. Flink 部署任务时失败
java.lang.NoSuchMethodError: org.apache.flink.api.java.ClosureCleaner.clean

	Flink k8s 环境问题？
	
44. ES查询警告
[299 Elasticsearch-6.8.2-b506955 "'y' year should be replaced with 'u'. Use 'y' for year-of-era. Prefix your date format with '8' to use the new specifier."]


45. influx unable to find time zone Asia/Shanghai
	要配置$GOROOT 见influx.md
	
46. kafka console producer 发送消息大小限制：
	替代方法：
	cat /tmp/1.txt | ./kafka-console-producer.sh --broker-list node38:6667 --topic anxinyun_data3
	
	
47.
sudo docker save -o /home/user/images/ubuntu_14.04.tar ubuntu:14.04

docker load --input mediaPusherImage.tar 
docker run --name mediaPusher -d -p 9090:9090 -p 1935:1935 anxinyun/mediapusher:m0 /bin/bash /app/mediaPusherBin/startBackground.sh

48. 查看linux下文件夹大小
du -h --max-depth=1 ./

49. zhiwuyun环境k8s容器创建失败
1月 05 14:13:16 anxin-m2 kubelet[25157]: E0105 14:13:16.298912   25157 pod_workers.go:182] Error syncing pod 597a7d8e-4d24-11eb-ad00-c81f66cfe365 ("es-log-clear-1609610400-45sbg_savoir(597a7d8e-4d24-11eb-ad00-c81f66cfe365)"), skipping: failed to "CreatePodSandbox" for "es-log-clear-1609610400-45sbg_savoir(597a7d8e-4d24-11eb-ad00-c81f66cfe365)" with CreatePodSandboxError: "CreatePodSandbox for pod \"es-log-clear-1609610400-45sbg_savoir(597a7d8e-4d24-11eb-ad00-c81f66cfe365)\" failed: rpc error: code = Unknown desc = failed pulling image \"gcr.io/google_containers/pause-amd64:3.0\": Error response from daemon: Get https://gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"

问题：k8s部署容器时需要pause-amd64:3.0这个镜像，翻墙才能下载
解决：下载阿里镜像后提交到私库
docker pull registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0
修改/etc/kubernetes/kubelet配置文件，在KUBELET_ARGS后加
--pod_infra_container_image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0
// 我们的环境 
vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
docker tag hub.c.163.com/conformance/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0
// 安装DNS
docker pull hub.c.163.com/zhijiansd/k8s-dns-kube-dns-amd64:1.14.5
docker pull hub.c.163.com/zhijiansd/k8s-dns-sidecar-amd64:1.14.5
docker pull hub.c.163.com/zhijiansd/k8s-dns-dnsmasq-nanny-amd64:1.14.5

docker tag hub.c.163.com/zhijiansd/k8s-dns-kube-dns-amd64:1.14.5 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
docker tag hub.c.163.com/zhijiansd/k8s-dns-sidecar-amd64:1.14.5 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
docker tag hub.c.163.com/zhijiansd/k8s-dns-dnsmasq-nanny-amd64:1.14.5 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5

50. POI执行OOM
在生成nbjj季报表时报错 GC overhead limit exceeded
使用了库poi-tl生成word模板报表
-XX:+UseParallelGC -Xms xxxm
https://stackoverflow.com/questions/49066977/writing-large-of-data-to-excel-gc-overhead-limit-exceeded

实战中使用  -Xmx16384m -Xms16384m 解决

51. flink执行报错
 java.lang.IllegalArgumentException: The scheme (hdfs://, file://, etc) is null. Please specify the file system scheme explicitly in the URI.
 
 解决：flink配置conf/flink-conf.yaml 中state.checkpoints.dir中URI必须包含前缀scheme(hdfs://, file://)
 
 
52. Could not locate executable null\bin\winutils.exe in the Hadoop binaries
 
	+ Download winutils.exe
	+ Create folder, say C:\winutils\bin
	+ Copy winutils.exe inside C:\winutils\bin
	+ Set environment variable HADOOP_HOME to C:\winutils


53. flink getSuperClassOrInterfaceName null (FLINK 闭包检查 报错)

	类中不能有object对象
	类构造参数中尽量使用简单类型
	
54. mqtt数据丢失
	代理 apollo
	java:paho

	解决：client.id不能包含"."
	

55. hdfs写入失败(et-hdfs-gateway进程)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE .... because this file lease is currently owned by ....

Slow ReadProcessor read fields took 49152ms (threshold=30000ms); ack: seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[10.8.30.117:50010,DS-cc98f7d8-c4f6-477b-9bff-bfced3de30f8,DISK]]

Slow waitForAckedSeqno took 68831ms (threshold=30000ms).

56. Redis Cluster 方法报错
Redis官方推介的库Jedis使用过程中出现（尤其使用keys和scan方法时）
no way to dispatch this command to redis cluster because keys have different slots。

修改了redis的引擎，改用luttence后解决。 


57. flink kafka 偏移量提交问题
```
2021-03-29 11:25:48,847 ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-24, groupId=et.mainx] Offset commit failed on partition anxinyun_capture_data-1 at offset 0: The coordinator is not aware of this member.
2021-03-29 11:25:48,847 WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-24, groupId=et.mainx] Asynchronous auto-commit of offsets {anxinyun_capture_data-1=OffsetAndMetadata{offset=0, metadata=''}} failed: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.

简单翻译一下，“位移提交失败，原因是消费者组开启了rebalance且已然分配对应分区给其他消费者。这表明poll调用间隔超过了max.poll.interval.ms的值，这通常表示poll循环中的消息处理花费了太长的时间。解决方案有两个：1. 增加session.timeout.ms值；2. 减少max.poll.records值”
	
	1. group.max.session.timeout.ms in the server.properties > session.timeout.ms in the consumer.properties.
	2. group.min.session.timeout.ms in the server.properties < session.timeout.ms in the consumer.properties.
	3. request.timeout.ms > session.timeout.ms and fetch.max.wait.ms
	4. (session.timeout.ms)/3 > heartbeat.interval.ms
	5. session.timeout.ms > Worst case processing time of Consumer Records per consumer poll(ms).
	
修改和增加max.poll.interval.ms(默认5分钟)到1800s无效，尝试提高session.timeout.ms(默认30s)无效

尝试在flink中使用checkpoint机制提交offset，无效 (https://blog.csdn.net/qq_37332702/article/details/107617253)

因为同时报错：
‘Offset commit failed on partition xxxxxxxxxx at offset 25: The coordinator is not aware of this member.’
尝试更换一个groupid好了 (https://blog.csdn.net/yang_zzu/article/details/107786332)


```

58. [Producer clientId=producer-1] Got error produce response in correlation id 6262947 on topic-partition anxinyun_data-3, splitting and retrying (3 attempts left). Error: MESSAGE_TOO_LARGE
MESSAGE_TOO_LARGE kafka写入失败

59. MQTT丢失连接后重连后，订阅失败：报错
Connection lost (32109) - java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.paho.client.mqttv3.internal.CommsSender.handleRunException(CommsSender.java:194)
	at org.eclipse.paho.client.mqttv3.internal.CommsSender.run(CommsSender.java:171)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at org.eclipse.paho.client.mqttv3.internal.wire.MqttOutputStream.flush(MqttOutputStream.java:49)
	at org.eclipse.paho.client.mqttv3.internal.CommsSender.run(CommsSender.java:149)
	... 1 more
    报错后又断连
    
    重启进程还是报错：
    reason:2
    msg: Invalid client ID
    loc: Invalid client ID
    cause:null
    excep:Invalid client ID(2)
    Invalid Client ID(2)
    at ....createMqttException(ExceptionHelper.java:31)
    at ....notifyReceiveAck(ClientState.java:1040)
    
    修改ClientID后重启成功
    
    【未解决】
  
  
60. Flink 启动 redis create异常
java.lang.ClassCastException: interface akka.serialization.Serializer is not assignable from class akka.remote.serialization.MiscMessageSerializer

Redis中使用了akka，改用redis库，修正


61. MQTT 使用过程的问题 n
MQTT重连后，订阅消息时报错 （另外收到一条信息）
SEVERE: receiver.production20429120721: Timed out as no activity, keepAlive=60,000,000,000 lastOutboundActivity=8,545,217,349,987,626 lastInboundActivity=8,545,157,349,155,183 time=8,545,277,349,973,460 lastPing=8,545,217,349,991,742

https://blog.csdn.net/u012134942/article/details/103965155
显然是因为emqx服务发送数据快，程序处理数据慢。审查代码才发现业务数据已经多线程处理，但emqx客户端的上下线消息并没有多线程处理，处理速度慢，导致tcp连接接收buffer被占满。


62. ES REST Client java报错
i use
BulkResponse bulkResponse = restHighLevelClient.bulk(bulkRequest, RequestOptions.DEFAULT);
Accidental occurrence
java.lang.IllegalStateException: Request cannot be executed; I/O reactor status: STOPPED at org.apache.http.util.Asserts.check(Asserts.java:46) at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase.ensureRunning(CloseableHttpAsyncClientBase.java:90) at org.apache.http.impl.nio.client.InternalHttpAsyncClient.execute(InternalHttpAsyncClient.java:123) at org.elasticsearch.client.RestClient.performRequestAsync(RestClient.java:531) at org.elasticsearch.client.RestClient.performRequestAsyncNoCatch(RestClient.java:516) at org.elasticsearch.client.RestClient.performRequest(RestClient.java:228) at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1593) at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1563) at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1525) at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:416)

https://github.com/elastic/elasticsearch/issues/39946
https://github.com/elastic/elasticsearch/issues/42061
https://github.com/elastic/elasticsearch/issues/42133
https://github.com/elastic/elasticsearch/issues/45115
https://github.com/elastic/elasticsearch/issues/49124

63. 容器中暴露udp端口
docker run -p 19600:19600/udp repository.anxinyun.cn/anxinyun/iota-device-proxy:5.21-05-12

64. ElasticsearchException[Elasticsearch exception [type=illegal_argument_exception, reason=Limit of total fields [1000] in index [anxinyun_themes] has been exceeded]]

    PUT anxinyun_themes/_settings
    {
      "index.mapping.total_fields.limit": 2000
    }

    
65. 以太DTU状态显示问题
以太设备状态是DAC存储在Promethus中，以太的Promethus可以通过以下地址访问
http://*****:19090/

接口 link_status
```js
let res = await (0, _util.queryMetric)(opts, `iota_dac_device_status{thingId="${thingId}"}`);
```


66. C# .NET Unable to find or load Npgsql with Entity Framework
The ADO.NET provider with invariant name 'Npgsql' is either not registered in the machine or application config file, or could not be loaded. See the inner exception for details.



67. ambari运维
> ambari中 某些服务显示 心跳丢失 (Heartbeat lost)
 一般需要到对应节点上去重启ambari-agent服务：
 
 ambari-agent stop
 ambari-agent start
 ambari-agent status
 
 还是不行：shutdown -r now
 
 还是不行：重启ambari-server机器
    ambari-server start
    
    
68. ES查询
Must be less than or equal to: [1024]. This limit can be set by changing the [search.max_open_scroll_context] setting

curl -x "" -X PUT 10.8.30.155:9200/_cluster/settings -H 'Content-Type: application/json' -d'{
    "persistent" : {
        "search.max_open_scroll_context": 1024
    },
    "transient": {
        "search.max_open_scroll_context": 1024
    }
}'

69. Flink启动报错
2021-09-17 15:58:24,782 WARN org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-6452224384874177409.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.

2021-09-17 15:58:24,784 INFO org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn - Opening socket connection to server anxinyun-n3/10.8.40.113:2181

2021-09-17 15:58:24,787 INFO org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn - Socket connection established to anxinyun-n3/10.8.40.113:2181, initiating session

2021-09-17 15:58:24,795 ERROR org.apache.flink.shaded.curator.org.apache.curator.ConnectionState - Authentication failed
flink启动[ERROR - main-EventThread] (ConnectionState.java:307) Authentication failed
解决方案
删除zookeeper中的flink目录下所有东西
或者：修改zk目录
high-availability.zookeeper.path.root


70. k8s API 认证问题
error: monitor failed  Error: pods is forbidden: User "system:serviceaccount:anxinyun:default" cannot list resource "pods" in API group "" in the namespace "anxinyun"

需要创建ServiceAccount的访问权限

```sh
kubectl get sa -n anxinyun

# clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: anxinyun
  name: operator
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["services","pods"]
  verbs: ["get", "watch", "list","create","update","patch"]
  
  
kubectl create clusterrolebinding operator-pod \
  --clusterrole=operator  \
  --serviceaccount=anxinyun:default
  
  
  
fastest@test-master:~$ kubectl get sa -n anxinyun -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: "2020-08-17T10:35:35Z"
    name: default
    namespace: anxinyun
    resourceVersion: "5982"
    selfLink: /api/v1/namespaces/anxinyun/serviceaccounts/default
    uid: a1100eea-19c2-4477-afca-61344353f2e5
  secrets:
  - name: default-token-zp6cz
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
fastest@test-master:~$ kubectl describe secret default-token-zp6cz -n anxinyun
Name:         default-token-zp6cz
Namespace:    anxinyun
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: default
              kubernetes.io/service-account.uid: a1100eea-19c2-4477-afca-61344353f2e5

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  8 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImFiVlF0Y1NyZjNNTkRVMFVieTNNTzhyVlc5T094Y3J2RmFfYTF6R0pveDQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJhbnhpbnl1biIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLXpwNmN6Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhMTEwMGVlYS0xOWMyLTQ0NzctYWZjYS02MTM0NDM1M2YyZTUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6YW54aW55dW46ZGVmYXVsdCJ9.elI35PPYtQp-fletleFR7so88Vozk7g8B7oRa1zy2LxSL1m26s8X6SJAipR5uqweNyi8JML3Yo3lPhs6mmzNLxkTRVk1atyXcCSr6J_iPD2dUUaGTL-ZPRYZ1x8Eb2PfugEQM5tf5YXERXqPpEsxTLM83KkI8ogFJQhLG7s-lWZFbvcgmKpCo3lmzuYf-hO0-JngjLhRxptCUqaFx6s8QwQz0dxNn_EvtMXbZm2cTkewJdsFAzczuKtt2sLiJCl5CSRghWAqkP9pBiC2diwDKzz9A0DevG0b3n7J-9_4fPtbXa5zQI60Rg3XVZRof0XNjw5Nze0ee8bn-6XI8yxIug
fastest@test-master:~$ 
```

71. ambari 心跳失联
ambari-agent.service: Supervising process 9678 which is not our child. We'll most likely not notice when it exits

?? 
systemctl start ambari-agent.service
ambari-agent restart

?? anxinyun- flink程序报错，看不到细节，可以从UI查看stdout

72. Nodejs官方库 ElasticSearch查询超时
```js
// opts.server.sniffOnStart = true;
    // opts.server.sniffInterval = 60000;
```


73. 服务器上docker tag的国内源镜像丢失
> 大亮：服务器上磁盘空间超过80% 会自动清理
有时间看下在哪里设置 或 清理点空间

74. Invoke频繁超时
> 三个api容器分别进行下发，发现其中一个容器容易出现超时。

三个容器中使用了不同的consumer-group,更换groupid后问题解决。


75. 定时执行es备份任务（机房迁移21-11）
root@anxin-m1:/home/anxin/tools/es2u# crontab -l
0 * * * * /usr/hdp/share/hst/bin/hst-scheduled-capture.sh sync
0 2 * * 0 /usr/hdp/share/hst/bin/hst-scheduled-capture.sh
40 21 * * * /home/anxin/tools/es2u/2709.sh
5 0 * * * /home/anxin/tools/es2u/2800.sh
5 6 * * * /home/anxin/tools/es2u/2806.sh
1 7 * * * /home/anxin/tools/es2u/2807.sh


76. CNB
    1. 统一数据处理引擎架构，将flink框架应用于ET和alarm进程。
    2. 通用工具库统一编写管理，例如数据库操作、Redis读写、Kafka读写、ES操作库等，并上传maven、npm包私服管理器。
    3. 全面应用云原生部署方案，基于Kubernetes的容器化部署管理。
    4. 满足公有云和私有云融和部署，实现云平台服务下沉本地化。统一公有云和私有云技术方案。
    5. 视频golang优化改造，统一后台支撑技术栈开发语言.
    
77. Kibana容器部署
docker run --name kibana7 -p 5601:5601 -e "ELASTICSEARCH_HOSTS=http://test-n3:9200" docker.elastic.co/kibana/kibana:7.16.2



78. ES Docker AccessDeny
目录权限
sudo chown -R 1000:root docker_data


79. DAC中pq: sorry, too many clients already
【未解决】
2022/03/28 15:04:06.550982 [E] ds.GetProtocol error: pq: sorry, too many clients already


80. 问题【79】导致dac卡死
【未解决】
```sh
# k8s查看pod状态：正常
root@anxin-m1:/home/anxin/iota/k8s/iota-web# ki | grep dac
iota-dac-0                                              2/2       Running            2          47d       10.244.1.81    iota-n3

# 查看容器状态：正常
root@iota-n3:~# docker ps | grep dac


...
f52f068a425e        repository.anxinyun.cn/iota/dac              "./dac"                  5 days ago          Up 5 days                               k8s_iota-dac_iota-dac-6_iota_263e5a76-8990-11ec-8c30-c81f66cfe365_27
...
# 查看容器对应宿主机进程ID，发现进程ID丢失
root@iota-n3:~# docker inspect -f '{{.State.Pid}} {{.Id}}' $(docker ps -a -q) | grep f52f068a425e
10823 f52f068a425e0a41ab4dcf72169b2e67121900aad6c4b1317108520068320102
root@iota-n3:~# ps -ef | grep 10823
root     32580 15338  0 11:10 pts/10   00:00:00 grep --color=auto 10823

docker inspect -f '{{.State.Pid}} {{.Id}} {{.Name}}' $(docker ps -a -q)  | grep k8s_iota-dac_iota-dac
```

81. Docker : How to avoid Operation not permitted in Docker Container?
web:
  image: an_image-image:1.0
  container_name: my-container
  privileged: true
  entrypoint: ["/usr/sbin/init"]
  ports:
    - "8280:8280"
    
    
82.nodejs内存溢出
    node --max-old-space-size=4096 server.js
    
83. flink-iceberg-hadoop 启动失败
Exception in thread "main" java.lang.VerifyError: Bad return type
Exception Details:
  Location:
    org/apache/hadoop/hdfs/DFSClient.getQuotaUsage(Ljava/lang/String;)Lorg/apache/hadoop/fs/QuotaUsage; @160: areturn
  Reason:
    Type 'org/apache/hadoop/fs/ContentSummary' (current frame, stack[0]) is not assignable to 'org/apache/hadoop/fs/QuotaUsage' (from method signature)

修改Hadoop版本从 2.8.2 -> 2.6.0

怪点：之前用2.8.2是能够正常启动并查询到结果的。是因为目标37上的hadoop环境变化了吗


84. 本地调试web localhost自动跳转https
    chrome://net-internals/#hsts  删除localhost
    
85. iota网站数据不实时推送
    WS的域名证书忘了更新
    
86. 写入Iceberg报错
some thing went wrong!!! org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto cannot be cast to com.google.protobuf.Message

root@iota-m2:/# find . -xdev -type f -size +100M -print
./tmp/iota20211206.bk
./tmp/iota220523.sql
./usr/bin/kubeadm
./usr/bin/kubelet
./usr/hdp/share/hst/smartsense-activity-explorer-1.4.0.2.5.1.0-159.tar.gz
./usr/hdp/2.6.1.0-129/storm-slider-client/contrib/storm-starter/storm-starter-topologies-1.1.0.2.6.1.0-129.jar
./usr/hdp/2.6.1.0-129/hadoop/mapreduce.tar.gz
./usr/lib/firefox/libxul.so
./usr/lib/thunderbird/libxul.so
./var/log/ambari-metrics-monitor/ambari-metrics-monitor.out
./var/log/hadoop-yarn/yarn/yarn-yarn-timelineserver-iota-m2.log
./var/log/hadoop-yarn/yarn/yarn-yarn-timelineserver-iota-m2.log.1
./var/log/ambari-agent/ambari-agent.out
./var/log/hadoop-mapreduce/mapred/mapred-mapred-historyserver-iota-m2.log
./var/lib/docker/overlay2/a833abde2fc657fe6b693bb9729cbb0af5d1979c76dce78ba8ad45c046952117/diff/tmp/blobStore-f0433c46-fd1f-4a21-b633-ac1a374345fd/job_e1e0f3104565236e0c03e5b2565664b1/blob_p-09ef3911203d748b3107f3c6e563d03551abcf72-958a7e237b5dd74ad9ece80da176788e
./var/lib/docker/overlay2/a833abde2fc657fe6b693bb9729cbb0af5d1979c76dce78ba8ad45c046952117/diff/tmp/flink-web-e5dd74bd-3e09-4ed0-9af9-750c09319a27/flink-web-upload/16a1e878-ef26-48bf-b41e-ad97868d91b7_et-flink0808.jar
./var/lib/docker/overlay2/a833abde2fc657fe6b693bb9729cbb0af5d1979c76dce78ba8ad45c046952117/diff/tmp/flink-web-e5dd74bd-3e09-4ed0-9af9-750c09319a27/flink-web-upload/9e6d03e2-7293-4566-93be-f98ac0121793_et-flink0808.jar
./var/lib/docker/overlay2/5c4e24f893b7efc649f5767a668ebfd7135d09c5ef88c492c04022e4dea38c39/diff/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
./var/lib/docker/overlay2/2c3968cc69e400b8ec420e9153501f4bac6efb7a5a3499fbf6669ba7163b4080/diff/home/fs/app/et-upload.jar
./var/lib/docker/containers/e0d29ef5857b6d62e950aee0dd225de3e6b8a0228734c7fcb8b3085ff919d89a/e0d29ef5857b6d62e950aee0dd225de3e6b8a0228734c7fcb8b3085ff919d89a-json.log
./var/lib/docker/containers/4a92adcb2676a4c14f061a372861f9a1728d39e9569c743a2faa5ca4de9c8fad/4a92adcb2676a4c14f061a372861f9a1728d39e9569c743a2faa5ca4de9c8fad-json.log
./var/lib/docker/containers/964e0388a26c1b0e28879f277cc7f1b1af202f549158e36b129584850411261f/964e0388a26c1b0e28879f277cc7f1b1af202f549158e36b129584850411261f-json.log
./var/lib/docker/containers/7afc1364bcdce03eca7763c1f669584085992bc8a463dcf974b2749b9d5094fa/7afc1364bcdce03eca7763c1f669584085992bc8a463dcf974b2749b9d5094fa-json.log
./var/lib/docker/containers/426728dd4d9214ac07d4a9b13ad783cf64b54bedf0110260624d7a6a530a8f56/426728dd4d9214ac07d4a9b13ad783cf64b54bedf0110260624d7a6a530a8f56-json.log
./var/lib/docker/containers/ec60f8fbda1f37f1284c42a6d1a91b6b05db4671d5f44aae6b00ab2c19ce13ce/ec60f8fbda1f37f1284c42a6d1a91b6b05db4671d5f44aae6b00ab2c19ce13ce-json.log
./var/lib/docker/containers/fee12bafe89e4d1cad72c73a571619a270bdce7891898fb194bff103d07fa959/fee12bafe89e4d1cad72c73a571619a270bdce7891898fb194bff103d07fa959-json.log


./home/iota/es/elasticsearch-5.5.1

./home/databasebak/savoir20220808.gz
./home/databasebak/savoir20220803.gz
./home/databasebak/savoir20220725.gz
./home/databasebak/savoir20220721.gz
./home/databasebak/savoir20220801.gz
./kafka-logs/



87.Clickhouse整理
查看日志
tail -n 1000 /var/log/clickhouse-server/clickhouse-server.err.log

[1002] ClickHouse exception, message: Code: 521. DB::ErrnoException: Cannot rename /var/lib/clickhouse/metadata/pepca10.sql.tmp to /var/lib/clickhouse/metadata/pepca10.sql because the second path already exists, errno: 17, strerror: File exists. (ATOMIC_RENAME_FAIL) (version 22.8.4.7 (official build)) , host: 10.8.30.71, port: 30123;

rm /var/lib/clickhouse/metadata/pepca10.sql
rm /var/lib/clickhouse/metadata/pepca10.sql.tmp

# 查看当前
select * from pg_replication_slots;
# 删除槽
select * from  pg_drop_replication_slot('flink_xq1_workflow_action_journal');


2022/09/29 18:02:34.065291 [W] [source] unmarshal data error: {"initial_level":3,"alarm_count":10,"alarm_type_code":"3004","current_level":1,"end_time":"2019-03-20T01:54:28.095Z","structure_id":101,"source_type_id":1,"alarm_code":"3008","start_time":"2018-11-19T08:17:55.510+0800","alarm_content":"数据解析错误","source_id":"3bb016cf-cec2-4e42-810f-33464086f386","state":3,"source_name":"微功耗扬尘","alarm_type_id":4} parsing time "\"2018-11-19T08:17:55.510+0800\"" as "\"2006-01-02T15:04:05Z07:00\"": cannot parse "+0800\"" as "Z07:00"


88. ElasticSearch Distinct
cardinality

GET anxinyun_vbraws/_search
{
  "query": {
    "bool": {
      "must": [
        {"range": {
          "collect_time": {
              "gte": "2022-01-01T00:00:00.000+0800",
              "lte": "2022-12-31T00:00:00.000+0800"
          }
        }}
      ]
    }
  },
  "size": 0
  , "aggs": {
    "Devices": {
      "cardinality": {
        "field": "iota_device"
      }
    }
  }
}

89. Flink 优化建议
https://cwiki.apache.org/confluence/display/FLINK/FLIP-171%3A+Async+Sink


90. Kafka broker启动失败
[2022-10-11 14:34:28,829] FATAL Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
java.lang.NumberFormatException: For input string: "lk-RawData-4"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:230)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:31)
	at kafka.log.Log$$anonfun$loadSegments$4.apply(Log.scala:195)
	at kafka.log.Log$$anonfun$loadSegments$4.apply(Log.scala:179)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at kafka.log.Log.loadSegments(Log.scala:179)
	at kafka.log.Log.<init>(Log.scala:108)
	at kafka.log.LogManager$$anonfun$loadLogs$2$$anonfun$3$$anonfun$apply$10$$anonfun$apply$1.apply$mcV$sp(LogManager.scala:151)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:58)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2022-10-11 14:34:28,833] INFO shutting down (kafka.server.KafkaServer)
[2022-10-11 14:34:28,840] INFO Recovering unflushed segment 0 in log savoir_raw-2. (kafka.log.Log)
[2022-10-11 14:34:28,844] INFO Completed load of log savoir_raw-2 with 1 log segments and log end offset 0 in 17 ms (kafka.log.Log)
[2022-10-11 14:34:28,852] WARN Found a corrupted index file due to requirement failed: Corrupt index found, index file (/kafka-logs/__consumer_offsets-12/00000000000028472468.index) has non-zero size but the last offset is 28472468 which is no larger than the base offset 28472468.}. deleting /kafka-logs/__consumer_offsets-12/00000000000028472468.timeindex, /kafka-logs/__consumer_offsets-12/00000000000028472468.index and rebuilding index... (kafka.log.Log)
[2022-10-11 14:34:28,853] INFO shut down completed (kafka.server.KafkaServer)
[2022-10-11 14:34:28,854] FATAL Fatal error during KafkaServerStartable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)
java.lang.NumberFormatException: For input string: "lk-RawData-4"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:589)
	at java.lang.Long.parseLong(Long.java:631)
	at scala.collection.immutable.StringLike$class.toLong(StringLike.scala:230)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:31)
	at kafka.log.Log$$anonfun$loadSegments$4.apply(Log.scala:195)
	at kafka.log.Log$$anonfun$loadSegments$4.apply(Log.scala:179)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at kafka.log.Log.loadSegments(Log.scala:179)
	at kafka.log.Log.<init>(Log.scala:108)
	at kafka.log.LogManager$$anonfun$loadLogs$2$$anonfun$3$$anonfun$apply$10$$anonfun$apply$1.apply$mcV$sp(LogManager.scala:151)
	at kafka.utils.CoreUtils$$anon$1.run(CoreUtils.scala:58)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[2022-10-11 14:34:28,855] INFO shutting down (kafka.server.KafkaServer)

解决方法：一直没法恢复，重置zk和kafka数据，重启进程
rm -rf /kafka-logs
rm -rf /hadoop/zookeeper/version-2


90. Clickhouse断电重启失败
Application: DB::Exception: Suspiciously many (32) broken parts to remove.****

ref: https://segmentfault.com/a/1190000042179953

单表配置方式
在创建MergeTree表的时候特别配置一下max_suspicious_broken_parts参数

CREATE TABLE foo
(
    `A` Int64
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS max_suspicious_broken_parts = 1000;

或使用ALTER TABLE ... MODIFY SETTING命令修改

新建文件max_suspicious_broken_parts.xml写入如下内容

<?xml version="1.0"?>
<yandex>
     <merge_tree>
         <max_suspicious_broken_parts>1000</max_suspicious_broken_parts>
     </merge_tree>
</yandex>

方法二：运行以下命令：

sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data
注意这里flage目录可以是你安装时指定的具体clickhouse根目录。然后重启CK服务，CK会从另外一个备份中恢复数据。
这里是CK自带的故障恢复机制，前提是使用复制表（Replicated开头），本质是告诉CK，强制重建数据。建议使用此方法。

如果数据完全丢失的情况，进行restore时，CK本身没有带宽限制，表很多或数据量很大的话，需要做好网络压力以及时间评估。

作者：EdgeE
链接：https://www.jianshu.com/p/06b79e8db75c
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

【未解决】

91.failed to re-find parent key in index "act_idx_hi_detail_proc_def_key" for split pages 8978/9035
REINDEX INDEX act_idx_hi_detail_proc_def_key;

92. clickhouse PG同步时区问题
-- 2022-04-26 06:55:13.431000 in clickhouse
                -- ^
                -- ^
                -- ^
-- 2022-04-26 11:55:13.431000 +00:00 in pg


INSERT INTO public.workflow_process_history (id, procinst_id, form_data, create_at, version_id, apply_user, apply_department, submit_form_data)
VALUES (1112359, 'bb1206b3-c557-11ec-b4cd-0a94055aa690', '{"Dm3TerRxaR":"","JH7F48i27P":"2022-04-26","sY6XrYGNex":"2022-04-26 18:00:00","X4wGC4GkHa":"2022-04-26 22:00:00","NDB3s3m757":"4","zAth85kMQc":"加班事项：项企绩效表单修改\n预计时间：3小时\n加班时间：2022-04-26 18:00 - 2022-04-26 21:00","apBBfJhR3h":{"Dr6n4SmeS3":4,"aTwj4kkpFk":"wTaTbAezs","tp4hX5jMR8":4},"mwPHPZAxPf":[{"name":"2022-04-26加班.png","uid":"rc-upload-1651065482283-2","status":"done","url":"/assets/files/common/1651065545498_2022-04-26加班.png"}]}',
        '2022-04-26 11:55:13.431000 +08:00', 3598, 731, 123, null);


-- 2022-04-25 22:55:13.431000
-- 2022-04-26 03:55:13.431000 +08:00
-- 莫斯科时区

-- 不过暂时商用环境没问题  
-- 那可能是容器时区的问题


93.unexpected chunk size in pg_toast
Clickhouse使用MaterializedPostgreSQL Postgresql数据库物化视图时出错。很多表没能进行同步，调整办法：
https://stackoverflow.com/questions/68193710/postgres-i-am-getting-error-unexpected-chunk-number-0-expected-1-for-toast-v
有部分索引出现问题。

2022.10.21 21:45:43.435593 [ 28338 ] {} <Error> void DB::DatabaseMaterializedPostgreSQL::startSynchronization(): std::exception. Code: 1001, type: pqxx::sql_error, e.what() = Failure during '[END COPY]': ERROR:  unexpected chunk size 1996 (expected 236) in final chunk 1 for toast value 5789294 in pg_toast_2482133

查询toast
SELECT oid::regclass,
       reltoastrelid::regclass,
       pg_relation_size(reltoastrelid) AS toast_size
FROM pg_class
WHERE relkind = 'r'
  AND reltoastrelid <> 0
ORDER BY 3 DESC;
  
  oid	reltoastrelid	toast_size
user_token	pg_toast.pg_toast_2482062	3337756672
workflow_process_history	pg_toast.pg_toast_2482133	202981376

REINDEX TABLE workflow_process_history;
REINDEX table pg_toast.pg_toast_2482133;
VACUUM analyze workflow_process_history;

又出现missing chunk number 0 for toast value 5789294 in pg_toast_2482133
https://meta.discourse.org/t/how-i-fixed-a-table-from-postgres/218012

pep-ca=# select count(*) from workflow_process_history;
 count 
-------
 71635
(1 row)

循环多次执行以下语句，查看大致的区间
select * from workflow_process_history order by id limit 10000 offset 70000;

#!/bin/sh
j=70000
while [ $j -lt 71635 ]
do
  psql -U postgres -d pep-ca -c "SELECT * FROM workflow_process_history LIMIT 1 offset $j" >/dev/null || echo $j
  j=$(($j+1))
done

pep-ca=# select * from workflow_process_history order by id limit 1 offset 70103;
ERROR:  missing chunk number 0 for toast value 5789294 in pg_toast_2482133
pep-ca=# select id from workflow_process_history order by id limit 1 offset 70103;
  id   
-------
 71500
(1 row)

 delete from workflow_process_history where id=71500;
在册执行：
REINDEX TABLE workflow_process_history;
REINDEX table pg_toast.pg_toast_2482133;
VACUUM analyze workflow_process_history;


94. Flink0ET处理时卡住
https://github.com/elastic/elasticsearch/issues/47599


95. 2022.11
2022-11-17 07:41:33,773 WARN org.apache.flink.runtime.taskmanager.Task - Task 'KeyedProcess -> Flat Map -> Filter -> (Filter -> Map -> Filter -> Sink: Unnamed, Map -> Filter -> Sink: Unnamed) (2/5)' did not react to cancelling signal for 30 seconds, but is stuck in method:

org.elasticsearch.action.bulk.BulkProcessor.internalAdd(BulkProcessor.java:286)

FLINK运行一段时间ES卡住

https://blog.csdn.net/lisenyeahyeah/article/details/114573798

解决方法：
1. 升级flink库>7.3
    但是7的connect没有flink1.13的版本 ： https://search.maven.org/artifact/org.apache.flink/flink-connector-elasticsearch7/1.16.0/jar
2. 尝试关闭backoff：esSinkBuilder.setBulkFlushBackoff(false)


96. K8S拉取私有库镜像
kubectl create secret generic zhiwucloud \
    --from-file=.dockerconfigjson=~/.docker/config.json \
    --type=kubernetes.io/dockerconfigjson
    
    
    kubectl get secret zhiwucloud --output=yaml
    kubectl get secret zhiwucloud --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode

生成yaml后修改namespace，重新创建：
```yml
apiVersion: v1
data:
  .dockerconfigjson: ewoJImF1dGhzI*******************************X0KCX0KfQ==
kind: Secret
metadata:
  name: zhiwucloudsecret
  namespace: iota
type: kubernetes.io/dockerconfigjson
```

创建基于私有镜像的pod
```yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: iota-orchestrator
  labels:
    iota.service: orchestrator
  namespace: iota
spec:
  replicas: 1
  template:
    metadata:
      labels:
        iota.service: orchestrator
    spec:
      containers:
      - image: registry.cn-hangzhou.aliyuncs.com/zhiwucloud/iot-orchestrator:6.22-12-19
        imagePullPolicy: IfNotPresent
        name: iota-orchestrator
        ports:
        - containerPort: 19300
        envFrom:
        - configMapRef:
            name: iota-orchestrator
      imagePullSecrets:
      - name: zhiwucloudsecret

```

97. Akka连接问题
nc -zv 10.244.7.240 2553

[WARN] [01/12/2023 13:53:22.446] [Client-akka.remote.default-remote-dispatcher-6] [akka.tcp://Client@10.244.3.224:2553/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FActorSystem%4010.244.7.240%3A2553-0] Association with remote system [akka.tcp://ActorSystem@10.244.7.240:2553] has failed, address is now gated for [5000] ms. Reason: [Disassociated]

最终发现是因为master程序不是最新，部分类序列值不对等


98. Clickhouse使用MaterializedPostgreSQL
Clickhouse MaterializedPostgreSQL同步一段时间后中断了
【1】
2023.01.18 10:18:51.916825 [ 13006 ] {} <Error> void DB::MaterializedPostgreSQLConsumer::syncTables(): Code: 49. DB::Exception: Invalid number of rows in Chunk column DateTime64 position 2: expected 7, got 6. (LOGICAL_ERROR), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xce3f77a in /usr/bin/clickhouse
1. DB::Chunk::checkNumRowsIsConsistent() @ 0x1357f7ad in /usr/bin/clickhouse
2. DB::MaterializedPostgreSQLConsumer::syncTables() @ 0x11fe5c62 in /usr/bin/clickhouse
3. DB::MaterializedPostgreSQLConsumer::readFromReplicationSlot() @ 0x11fe7db9 in /usr/bin/clickhouse
4. DB::MaterializedPostgreSQLConsumer::consume(std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >&) @ 0x11fe8719 in /usr/bin/clickhouse
5. DB::PostgreSQLReplicationHandler::consumerFunc() @ 0x11fc9de4 in /usr/bin/clickhouse
6. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0x116cf4e1 in /usr/bin/clickhouse
7. DB::BackgroundSchedulePool::threadFunction() @ 0x116d274a in /usr/bin/clickhouse
8. ? @ 0x116d356c in /usr/bin/clickhouse
9. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xcefa46c in /usr/bin/clickhouse
10. ? @ 0xceffbde in /usr/bin/clickhouse
11. start_thread @ 0x7ea7 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so
12. clone @ 0xfcaef in /usr/lib/x86_64-linux-gnu/libc-2.31.so
 (version 22.10.2.11 (official build))
2023.01.18 10:18:55.276423 [ 12929 ] {} <Warning> PostgreSQLReplicaConsumer(pep-ca): Got TOAST value, which is not supported, default value will be used instead.


【2】
2023.01.18 10:13:22.653295 [ 581344 ] {fea6faf6-d832-4f1b-8f26-0d5c8f645614} <Error> DynamicQueryHandler: Code: 202. DB::Exception: Too many simultaneous queries. Maximum: 100. (TOO_MANY_SIMULTANEOUS_QUERIES), Stack trace (when copying this message, always include the lines below):

0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0xce3f77a in /usr/bin/clickhouse
1. DB::ProcessList::insert(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IAST const*, std::__1::shared_ptr<DB::Context>) @ 0x126f5b13 in /usr/bin/clickhouse
2. ? @ 0x12977eec in /usr/bin/clickhouse
3. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::__1::shared_ptr<DB::Context>, std::__1::function<void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>, std::__1::optional<DB::FormatSettings> const&) @ 0x1297e90e in /usr/bin/clickhouse
4. DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&) @ 0x134d99d7 in /usr/bin/clickhouse
5. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) @ 0x134dddf0 in /usr/bin/clickhouse
6. DB::HTTPServerConnection::run() @ 0x1355b48a in /usr/bin/clickhouse
7. Poco::Net::TCPServerConnection::start() @ 0x1636274f in /usr/bin/clickhouse
8. Poco::Net::TCPServerDispatcher::run() @ 0x16364adb in /usr/bin/clickhouse
9. Poco::PooledThread::run() @ 0x16520372 in /usr/bin/clickhouse
10. Poco::ThreadImpl::runnableEntry(void*) @ 0x1651dafd in /usr/bin/clickhouse
11. start_thread @ 0x7ea7 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so
12. clone @ 0xfcaef in /usr/lib/x86_64-linux-gnu/libc-2.31.so
 (version 22.10.2.11 (official build))

99. Clickhouse停止同步
2023.02.15 12:37:38.166872 [ 20110 ] {} <Error> DatabaseMaterializedPostgreSQL (iota): Unable to load replicated tables list
2023.02.15 12:38:13.457067 [ 20017 ] {} <Fatal> Application: Child process was terminated by signal 9 (KILL). If it is not done by 'forcestop' command or manually, the possible cause is OOM Killer (see 'dmesg' and look at the '/var/log/kern.log' for the details).


100. 跨平台编译
unset GOPROXY
go env -w GOPROXY=https://goproxy.cn
go env -w GO111MODULE=on
CGO_ENABLED=1 GOOS=linux GOARCH=arm64 go build -ldflags "-extldflags -static " -tags netgo -a -v -o towercrane main.go


FROM golang:1.11 AS builder

# Magic line, notice in use that the lib name is different!
RUN apt-get update && apt-get install -y gcc-aarch64-linux-gnu
# Add your app and do what you need to for dependencies
ADD . /go/src/github.com/org/repo
WORKDIR /go/src/github.com/go/repo
RUN CGO_ENABLED=1 CC=aarch64-linux-gnu-gcc GOOS=linux GOARCH=arm64 go build -o app .

# Final stage - pick any old arm64 image you want
FROM multiarch/ubuntu-core:arm64-bionic

WORKDIR /root/

COPY --from=builder /go/src/github.com/org/repo/app .
CMD ["./app"]


尝试：
> amd平台无法运行arm版本的容器
容器：
alpine registry.cn-hangzhou.aliyuncs.com/fs-devops/golang:2.22-06-22
alpine amd64:golang:1.18
alpine arm64v8/golang:1.17.11-alpine

apt-get update && apt-get install -y gcc-aarch64-linux-gnu

Cross-compile aarch64


_cgo_export.c:3:10: fatal error: stdlib.h: No such file or directory
    3 | #include <stdlib.h>
      |          ^~~~~~~~~~


最终是在arm板子上构建成功：

Debian / Ubuntu: sudo apt-get install golang gcc libgl1-mesa-dev xorg-dev


报错以下：通过非ROOT用户执行
2023/02/22 15:42:27 Fyne error:  Unable to connect to session D-Bus
2023/02/22 15:42:27   Cause: dbus: couldn't determine address of session bus
2023/02/22 15:42:27   At: /root/go/pkg/mod/fyne.io/fyne/v2@v2.3.1/app/app_xdg.go:39
No protocol specified
2023/02/22 15:42:27 PlatformError: X11: Failed to open display :0.0



101. 
2023-04-07 00:01:39,496 WARN  org.apache.flink.runtime.taskmanager.Task                    [] - Sink: stationAggDataBefore (2/2)#9 (069e1c23e659fe8762bc43a7c632f01c) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427)
	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkAsyncErrorsAndRequests(ElasticsearchSinkBase.java:432)
	at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.invoke(ElasticsearchSinkBase.java:329)
	at org.apache.flink.streaming.api.operators.StreamSink.processElement(StreamSink.java:54)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:205)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:66)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:423)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:204)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:684)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.executeInvoke(StreamTask.java:639)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:650)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:623)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:779)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:566)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: java.lang.RuntimeException: An error occurred in ElasticsearchSink.
		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.checkErrorAndRethrow(ElasticsearchSinkBase.java:427)
		at org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkBase.close(ElasticsearchSinkBase.java:366)
		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
		at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.dispose(AbstractUdfStreamOperator.java:117)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.disposeAllOperators(StreamTask.java:864)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.runAndSuppressThrowable(StreamTask.java:843)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUpInvoke(StreamTask.java:756)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.runWithCleanUpOnFail(StreamTask.java:662)
		... 4 more
	Caused by: ElasticsearchException[Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of processing of [15874765360][indices:data/write/bulk[s][p]]: request: BulkShardRequest [[anxinyun_rt_aggregation][4]] containing [7] requests, target allocation id: J7n4QPZbSde9vB6jUmkTpg, primary term: 22 on EsThreadPoolExecutor[name = es-n2/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@60c85bf1[Running, pool size = 32, active threads = 32, queued tasks = 205, completed tasks = 582788021]]]]
		at org.elasticsearch.ElasticsearchException.innerFromXContent(ElasticsearchException.java:509)
		at org.elasticsearch.ElasticsearchException.fromXContent(ElasticsearchException.java:420)
		at org.elasticsearch.action.bulk.BulkItemResponse.fromXContent(BulkItemResponse.java:139)
		at org.elasticsearch.action.bulk.BulkResponse.fromXContent(BulkResponse.java:198)
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1793)
		at org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAsyncAndParseEntity$11(RestHighLevelClient.java:1621)
		at org.elasticsearch.client.RestHighLevelClient$1.onSuccess(RestHighLevelClient.java:1721)
		at org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onSuccess(RestClient.java:842)
		at org.elasticsearch.client.RestClient$1.completed(RestClient.java:543)
		at org.elasticsearch.client.RestClient$1.completed(RestClient.java:531)
		at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:119)
		at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:177)
		at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:436)
		at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:326)
		at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265)
		at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81)
		at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39)
		at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:114)
		at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162)
		at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337)
		at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315)
		at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276)
		at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104)
		at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:588)
		... 1 more
Caused by: [CIRCULAR REFERENCE: ElasticsearchException[Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of processing of [15874765360][indices:data/write/bulk[s][p]]: request: BulkShardRequest [[anxinyun_rt_aggregation][4]] containing [7] requests, target allocation id: J7n4QPZbSde9vB6jUmkTpg, primary term: 22 on EsThreadPoolExecutor[name = es-n2/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@60c85bf1[Running, pool size = 32, active threads = 32, queued tasks = 205, completed tasks = 582788021]]]]]


102. docker目录磁盘空间不足了
docker system prune -a
https://blog.fundebug.com/2018/01/10/how-to-clean-docker-disk/
或者执行
docker image prune -a -f
https://www.cnblogs.com/quanxiaoha/p/10542278.html



103. Redis BigKey

redis-cli --bigkeys
# ANXINYUN
0 lists with 0 items (00.00% of keys, avg size 0.00)
244 hashs with 357 fields (00.12% of keys, avg size 1.46)
201561 strings with 925 689 350 bytes (97.99% of keys, avg size 4592.60)
0 streams with 0 entries (00.00% of keys, avg size 0.00)
3886 sets with 50511 members (01.89% of keys, avg size 13.00)
0 zsets with 0 members (00.00% of keys, avg size 0.00)

# EMIS
0 lists with 0 items (00.00% of keys, avg size 0.00)
0 hashs with 0 fields (00.00% of keys, avg size 0.00)
17 strings with 11 408 113 bytes (100.00% of keys, avg size 671065.47)
0 streams with 0 entries (00.00% of keys, avg size 0.00)
0 sets with 0 members (00.00% of keys, avg size 0.00)
0 zsets with 0 members (00.00% of keys, avg size 0.00)

1.安心云iota-proxy单独redis部署  api-redis.ops
2.pep单独redis部署  emis-redis.smart-xxx
3.其他应用单独使用一个    opt-redis.ops




        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/web
                    operator: Exists
                    
                    
104. 磁盘满了，docker起不来。no space left
# 大文件
sudo find . -xdev -type f -size +100M
# 大目录
du -ahx . | sort -rh | head -5

docker system prune --all --force

105. edge-backend无法ssh
因为edge上的frp客户端无法连接frps。因为无法访问外网,固定到可以访问的节点上
nodeSelector:
        kubernetes.io/hostname: iota-n1
        
        重启后
        route add default gw 10.8.10.1
        
        sudo chown clickhouse:clickhouse /home/clickhouse-server/clickhouse-server.log
sudo chmod 644 /home/clickhouse-server/clickhouse-server.log

106. 手动启动hdfs
由于ambari未恢复，需要手动启动hdfs。
一共有以下几个hdfs节点：
hdfs-namenode
hdfs-snamenode
hdfs-d1~d3

在hdfs-namenode节点上执行
sudo -u hdfs hdfs --daemon start namenode
在hdfs-snamenode节点上执行
sudo -u hdfs hdfs --daemon start secondarynamenode
在其他data节点上：
root@hdfs-d3:/home/anxinyun# mkdir /var/run/hadoop
root@hdfs-d3:/home/anxinyun# chown hdfs /var/run/hadoop/
sudo -u hdfs hdfs --daemon start datanode


107. Evicted
通过kubernetes启动Flink，其中 Taskmanager 一直重启，容器报错Evicted
查看 kubectl describe pod ...
```
 Warning  Evicted                 27m        kubelet, anxinyun-n6     The node was low on resource: memory. Container taskmanager-13 was using 13902500Ki, which exceeds its request of 0. Container iota-filebeat was using 7128Ki, which exceeds its request of 0.
```
或者
```
Message:        Pod The node had condition: [MemoryPressure]
```
或者：
```
Message:        The node was low on resource: memory. Container taskmanager-13 was using 13344796Ki, which exceeds its request of 0. Container iota-filebeat was using 7820Ki, which exceeds its request of 0. 
```

您可以使用 Kubernetes Dashboard 或者命令行工具 kubectl top nodes 来查看节点资源使用情况。
 Pod 的 YAML 文件中设置资源请求和限制
```
 resources:
  requests:
    memory: "2Gi"   # 根据您的需求调整内存请求
  limits:
    memory: "4Gi"   # 根据您的需求调整内存限制
```
删除所有未evicted状态的容器

 kubectl delete pods --field-selector=status.phase=Failed  -n ops --force --grace-period=0
 
 
108. mediapush自动启动
[root@10-020-xbw-base push]# cat /etc/systemd/system/media.service
[Unit]
Description=Media pusher
After=network.target

[Service]
WorkingDirectory=/installdir/sp/0319mediapusherbin/push
Type=simple
ExecStart=/installdir/sp/0319mediapusherbin/push/0320mediaPusher03
Restart=always
User=root

[Install]
WantedBy=multi-user.target

[root@10-020-xbw-base push]# systemctl enable media.service
/usr/bin/pkttyagent: error while loading shared libraries: libpcre.so.3: cannot open shared object file: No such file or directory
[root@10-020-xbw-base push]# systemctl start media.service
/usr/bin/pkttyagent: error while loading shared libraries: libpcre.so.3: cannot open shared object file: No such file or directory
[root@10-020-xbw-base push]# systemctl status media.service
● media.service - Media pusher
   Loaded: loaded (/etc/systemd/system/media.service; enabled; vendor preset: disabled)
   Active: active (running) since Wed 2023-10-11 22:56:59 CST; 2min 34s ago
 Main PID: 2541 (0320mediaPusher)
   CGroup: /system.slice/media.service
           └─2541 /installdir/sp/0319mediapusherbin/push/0320mediaPusher03

Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: async : false
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: BeatsStream 1697036303865 successful.
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: [2023-10-11 22:59:24] Incomming call [GET][/STREAM/1697036304044/BEATS]  list headers are below:
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: Accept-Encoding : gzip, deflate
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: Connection : close
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: Content-Type : application/json
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: Host : 10.10.10.20:9090
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: User-Agent : node-superagent/1.8.5
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: async : false
Oct 11 22:59:24 10-020-xbw-base 0320mediaPusher03[2541]: BeatsStream 1697036304044 successful.


109. 如何查看本机.NetFramework的版本
```bash
reg query "HKLM\SOFTWARE\Microsoft\NET Framework Setup\NDP\v4\Full" /v Release
```
版本值参考：
https://learn.microsoft.com/zh-cn/dotnet/framework/migration-guide/how-to-determine-which-versions-are-installed

.NET Framework 版本	Release 的值
.NET Framework 4.5	所有 Windows 操作系统：378389
.NET Framework 4.5.1	在 Windows 8.1 和 Windows Server 2012 R2 上：378675
在所有其他 Windows 操作系统上：378758
.NET Framework 4.5.2	所有 Windows 操作系统：379893
.NET Framework 4.6	在 Windows 10 上：393295
在所有其他 Windows 操作系统上：393297
.NET Framework 4.6.1	在 Windows 10 11 月更新系统上：394254
在所有其他 Windows 操作系统（包括 Windows 10）上：394271
.NET Framework 4.6.2	在 Windows 10 周年更新和 Windows Server 2016 上：394802
在所有其他 Windows 操作系统（包括其他 Windows 10 操作系统）上：394806
.NET Framework 4.7	在 Windows 10 创意者更新上：460798
在所有其他 Windows 操作系统（包括其他 Windows 10 操作系统）上：460805
.NET Framework 4.7.1	在 Windows 10 Fall Creators Update 和 Windows Server 版本 1709 上：461308
在所有其他 Windows 操作系统（包括其他 Windows 10 操作系统）上：461310
.NET Framework 4.7.2	在 Windows 10 2018 年 4 月更新和 Windows Server 版本 1803 上：461808
在除 Windows 10 2018 年 4 月更新和 Windows Server 版本 1803 之外的所有 Windows 操作系统上：461814
.NET Framework 4.8	在 Windows 10 2019 年 5 月更新和 Windows 10 2019 年 11 月更新上：528040
在 Windows 10 2020 年 5 月更新、2020 年 10 月更新、2021 年 5 月更新、2021 年 11 月更新和 2022 年更新上：528372
在 Windows 11 和 Windows Server 2022 上：528449
在所有其他 Windows 操作系统（包括其他 Windows 10 操作系统）上：528049
.NET Framework 4.8.1	在 Windows 11 2022 年更新上：533320
所有其他 Windows 操作系统：533325


110. 共享chatgpt
root@node38:/home/anxin# tail ~/.bashrc 
fi


export http_proxy=http://10.8.30.183:7890
# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
#if [ -f /etc/bash_completion ] && ! shopt -oq posix; then
#    . /etc/bash_completion
#fi
root@node38:/home/anxin# source ~/.bashrc 

docker run --name chatgpt-web --rm -it -p 3002:3002 --env OPENAI_ACCESS_TOKEN=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6Ik1UaEVOVUpHTkVNMVFURTRNMEZCTWpkQ05UZzVNRFUxUlRVd1FVSkRNRU13UmtGRVFrRXpSZyJ9.eyJodHRwczovL2FwaS5vcGVuYWkuY29tL3Byb2ZpbGUiOnsiZW1haWwiOiJ5d3cwODI4QDEyNi5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZX0sImh0dHBzOi8vYXBpLm9wZW5haS5jb20vYXV0aCI6eyJwb2lkIjoib3JnLUtNZVRyd1pFb0dnMEVVVUZZWGlWenZDMSIsInVzZXJfaWQiOiJ1c2VyLWJaZEN1WnlwU2FuSkV6UjNTNFducElLWiJ9LCJpc3MiOiJodHRwczovL2F1dGgwLm9wZW5haS5jb20vIiwic3ViIjoiYXV0aDB8NjNlMWI2N2MxZTMzNTM1NGMyNTlmMmMzIiwiYXVkIjpbImh0dHBzOi8vYXBpLm9wZW5haS5jb20vdjEiLCJodHRwczovL29wZW5haS5vcGVuYWkuYXV0aDBhcHAuY29tL3VzZXJpbmZvIl0sImlhdCI6MTcwMDA5NTk5MCwiZXhwIjoxNzAwOTU5OTkwLCJhenAiOiJUZEpJY2JlMTZXb1RIdE45NW55eXdoNUU0eU9vNkl0RyIsInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUgbW9kZWwucmVhZCBtb2RlbC5yZXF1ZXN0IG9yZ2FuaXphdGlvbi5yZWFkIG9yZ2FuaXphdGlvbi53cml0ZSBvZmZsaW5lX2FjY2VzcyJ9.mAQcMh5CLcTBMZpJGUHDJBLvTviEsJWEH6fdHRGd4uc5OyMnhWN-k50FpIrTOz6V4knlaQrpjIJFHvomVdGhHERSanSulwNqw5sWv8BPt5zrA_mLuqGFW0HBERimYud-vBJNrjBm05-DB_1dpMSF33Fb-G3OSFVzMaYDPPBBCR5naErSSHdqL112_Ocw8aiip2baZc-7g3bm1Nx3igOmay1xHb4cdY0zUhgcediMwJ0BfVC7GSCWvKB6uoM9AeYKDOynLExWRjwcIVD1wNZC8Ql7aiuiISIQSW07PIDHyJ-85nRtOUTNaHvUD2NW3COragEGPbTo7pjFLtYVGxdEDA chenzhaoyu94/chatgpt-web

111. svn log日志导出到excel
Use the following Subversion command to create an xml file out of the repository's log:

svn log -v --xml > repository_log.xml

Import the xml file into an Excel spreadsheet (not sure if it will work with LibreOffice/OpenOffice), e.g. from cmd:

start excel repository_log.xml

You can then save it as a spreadsheet.


112. ambari 安装hive
第一次安装失败后，说
Failed to download file from http://node1.ambari.com:8080/resources/mysql-connector-java.jar due to HTTP error: HTTP Error 404: Not Found

将对应资源拷贝到 /var/lib/ambari-server/resources目录即可



113. Flink突然中断，所有任务消失（未解决）
2023-11-06 16:12:18,379 INFO et.analyze.threshold$ [] - over-threshold [74642-4标7号测点 ] level:3 code:30070003 content:Z方向位移采集值:20.17mm,超3级阈值[20.0~50.0] time:2023-11-06T16:00:00.000+08:00

2023-11-06 16:12:19,092 WARN et.recv.RecvDataHanler$ [] - class et.recv.RecvDataHanler$alarm code not valid: 3002

2023-11-06 16:12:19,092 WARN et.recv.RecvDataHanler$ [] - class et.recv.RecvDataHanler$alarm code not valid: 3002

2023-11-06 16:12:19,357 INFO org.apache.flink.runtime.blob.PermanentBlobCache [] - Shutting down BLOB cache

2023-11-06 16:12:19,358 INFO org.apache.flink.runtime.state.TaskExecutorLocalStateStoresManager [] - Shutting down TaskExecutorLocalStateStoresManager.

2023-11-06 16:12:19,358 INFO org.apache.flink.runtime.blob.TransientBlobCache [] - Shutting down BLOB cache

2023-11-06 16:12:19,376 INFO org.apache.flink.runtime.filecache.FileCache [] - removed file cache directory /tmp/flink-dist-cache-dc1878f0-b507-4d01-bd13-dc97058ac8e1

2023-11-06 16:12:19,379 INFO org.apache.flink.runtime.io.disk.FileChannelManagerImpl [] - FileChannelManager removed spill file directory /tmp/flink-netty-shuffle-1a6ac9e9-2c78-4a29-86d1-38600c85b902

1. 安心云新集群K8S新集群 NotReady
新集群jump-02 Not Ready导致知物云服务不可用。
登录节点上，查看日志 journalctl -eu kubelet . 发现连不上k8s的服务接口，报错：apiserver.cluster.local not found.
进入/etc/hosts 修改如下：【原先注释相反】
#10.103.97.2 apiserver.cluster.local
10.8.40.125 apiserver.cluster.local

2. IOTa集群重启后服务启动顺序
因为IOTA集群服务依赖安心云Kafka集群，所以会导致部分进程启动后无法正常工作。故需确保进程的启动顺序如下：
KAFKA已就绪 --> 重启代理服务iota-proxy
            --> 重启编排器 iota-orchestrator     --> 重启所有DAC实例  iota-dac-n
            --> 重启规则引擎 iota-rule-engine
            --> 重启API服务 iota-api
            --> 重启网关转发代理 (savoir命名空间下) anxinyun-http-udp
            
114. where is kibana
查找logs.zhiwucloud.com的部署
ping后发现服务器地址
到服务器上找nginx
root@anxin-m1:/etc/nginx# grep -r 'logs.zhiwucloud' ./

找到发现是转发到内部服务器 anxin-m2:15601

到anxin-m2，查找kibana相关指令
anxin@anxin-m2:~/SavoirCloud/kibana/config$ history | grep kibana
发现是docker-compose启动的：
“
docker-compose start kibana
”

> anxin-m2上有两套kibana，还有个kibana.zhiwucloud.com,是通过service kibana start启动的

115. 发邮件
#邮箱

set from=no-reply@free-sun.com.cn

#默认smtp发送，stmp发送需要在邮箱内配置，允许stmp发送

set smtp=smtp.exmail.qq.com

#邮箱

set smtp-auth-user=no-reply@free-sun.com.cn

#这里填的是邮箱授权码（我的授权码就不在这现眼了）

set smtp-auth-password=Noreply123_

set smtp-auth=login

mail -s "应变数据" -a /home/anxinlocal/static/pan/structure_1/theme/2023/11/结构应变.csv 1596738708@qq.com


116. IOTA-API升级方式-容器修改提交方式
》》目前构建的版本存在问题，回退到老版本，并在此基础上进行修改容器内部内容
到指定服务器上(iota-m2)：
docker ps | grep api  找到对应容器id xxx
docker exec -it xxx /bin/ash
docker commit xxx registry.ngaiot.com/xxx/xxx:XXX
这时候修改yaml进行升级。
需要保存镜像到仓库，执行：
docker save -o xxx.tar registry.ngaiot.com/iot/iota-api:manual.v240103
scp xxx.tar anxin@anxin-m1:/home/anxin/
docker load -i xxx.tar
docker push registry.ngaiot.com/iot/iota-api:manual.v240103

117. 删除当前目录下所有指定后缀文件
find . -maxdepth 1 -type f -name "*.csv" -delete

118. 替换文本
:%s/192.168.0.133:18080/192.168.0.133:32188/g
:%s/wvp:18080/wvp:32188/g
:%s/wvp:18080/wvp:32188/g
:%s/wvp:32188/10.8.40.124:32188/g
:%s/10.8.40.124:18080/10.8.40.124:32188/g
:%s/192.168.1.133/192.168.0.133/g
 kubectl cp -niot /home/cloud/zlmconfig.ini zlm-8dd665c6b-jz7mr:/conf/config.ini
222.186.227.196
投：172.30.16.121
机房：172.30.25.129
公网：58.252.10.4

119. 视频推流无法播放：
推送显示RTMP write err
解决办法：磁盘空间不足
SRS（Simple-RTMP-Server）是一个开源的 RTMP 服务器，它允许用户将音视频流推送到服务器并进行分发。在 SRS 中，objs/nginx/html/live 目录通常用于存储临时文件或者缓存文件，用于临时存储推流的视频文件或者用于 HLS（HTTP Live Streaming）的分片文件等。

这些临时文件的具体内容取决于你的 SRS 配置以及你所使用的功能。通常情况下，这些临时文件可能包括：

HLS 分片文件：如果你启用了 HLS 功能，并且正在将音视频流转换成 HLS 格式进行分发，那么这些临时文件可能是 HLS 分片文件，用于存储已转换的视频分片。
FLV 文件：如果你将音视频流存储到文件中（例如录制直播流），那么这些临时文件可能是 FLV 格式的视频文件，用于临时存储推流的音视频数据。
其他临时文件：除了以上两种情况外，这些临时文件还可能包括其他类型的临时文件，用于缓存或者临时存储与音视频流相关的数据。
在一般情况下，这些临时文件是可以安全删除的。但在删除之前，你可能需要停止 SRS 服务器的运行，以确保没有文件被正在运行的流使用。删除这些文件不会影响已经推流或者分发的音视频流，但可能会导致一些正在进行的推流操作中断。因此，在删除这些文件之前，请确保你了解 SRS 的工作原理，并且可以在必要时恢复推流操作。


120. java直接运行
unzip DSECryptLibDemo.zip
javac -cp jna-4.0.0.jar -d bin src/DSECrypt/DSECrypt.java
java -cp bin/:jna-4.0.0.jar DSECrypt.DSECrypt

121. windows中文件夹大小
Get-ChildItem -Path "E:" -Recurse | Measure-Object -Property Length -Sum

Get-ChildItem -Path "E:" -File -Recurse | Sort-Object -Property Length -Descending

122. k8s内存分析
## 查询所有进程的内存消耗
kubectl top pod --all-namespaces --containers
返回结果如下：
tongji                         webapi-deployment-8bb6b5fb5-m9s8x                   yaml-webapi                              1m           77Mi
tongji                         webconsole-deployment-bc56dbc6c-gzllv               yaml-webconsole                          0m           46Mi
按内存大小进行排序，列出最大的前10
root@jumper-02:/home/cloud# kubectl top pod --all-namespaces --containers | awk 'NR==1{print $0; next} {print $0 | "sort -k5 -rh"}' | head -n 20
```sh
#!/bin/bash

# 获取所有命名空间中的所有 Pod
pods=$(kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{.metadata.namespace}{"\t"}{.metadata.name}{"\t"}{.status.containerStatuses[*].restartCount}{"\n"}{end}')

# 遍历每个 Pod 并获取详细信息
echo -e "Namespace\tPod Name\tRestart Count\tLast Restart Reason"
while read -r line; do
    namespace=$(echo $line | awk '{print $1}')
    pod_name=$(echo $line | awk '{print $2}')
    restart_count=$(echo $line | awk '{print $3}')
    
    # 获取 Last State 信息
    last_state=$(kubectl describe pod -n $namespace $pod_name | grep -A 5 "Last State" | grep -E "Reason|Exit Code|Started|Finished")
    if [ -z "$last_state" ]; then
        last_state="N/A"
    fi
    
    # 输出结果
    echo -e "$namespace\t$pod_name\t$restart_count\t$last_state"
done <<< "$pods"
```

123. DAC CPU消耗过大
查看TOP -C后 PID 8858的消耗最大

cat /proc/8858/environ

124. 容器内的pg无法启动
在容器外面的宿主机上安装pg12
sudo apt install postgresql-12 postgresql-contrib

如果 pg_resetwal 报告无法为 pg_control 确定有效数据，您可以通过指定 -f（强制）选项强制它继续。在这种情况下，将用合理的值替换丢失的数据。大多数字段预计会匹配，但对于下一个 OID、下一个事务 ID 和纪元、下一个多事务 ID 和偏移量以及 WAL 起始位置字段，可能需要手动协助。这些字段可以使用下面讨论的选项进行设置。如果您无法确定所有这些字段的正确值，仍然可以使用 -f，但恢复的数据库必须比平常更加谨慎对待：立即进行转储和重新加载是必要的。在转储之前，不要在数据库中执行任何数据修改操作，因为任何此类操作都可能使损坏变得更严重。

您可以通过运行以下命令来确定是否存在这种情况：

Postgres >= 10
pg_resetwal DATADIR

Postgres < 10
pg_resetxlog DATADIR

sudo -u postgres pg_resetwal /../pgdata


#125. 通过命令替换 /conf/config.ini文件中http://10.8.40.124:18080/为http://10.8.40.124:32188/

sed 's|http://10.8.40.124:18080/|http://10.8.40.124:32188/|g' /conf/config.ini
sed 's|http://wvp:32188/|http://10.8.40.124:32188/|g' /conf/config.ini
sed 's|fs-zlm-0|fs-zlm-2|g' /conf/config.ini


126. 服务器无法访问公网
route del -net 0.0.0.0 netmask 0.0.0.0 dev eno3
route add default gw 10.8.10.1


127. kibana
anxin-m2上有两个kibana服务：
(1) logs.zhiwucloud.com
root@anxin-m2:/home/anxin/SavoirCloud/kibana# docker-compose start kibana
Starting kibana ... done
账号： JustSearch    密码：   S****_logs_Search

(2) 另一个服务:(kibana.zhiwucloud.com)
service kibana start
service kibana status


kubectl get pods --all-namespaces | grep Terminating | awk '{print $1 " " $2}' | xargs -n2 bash -c 'kubectl delete pod -n $0 $1 --grace-period=0 --force'
kubectl get pods --all-namespaces | grep ContainerStatusUnknown | awk '{print $1 " " $2}' | xargs -n2 bash -c 'kubectl delete pod -n $0 $1 --grace-period=0 --force'


##### Wbo@hi1I
##### 1mDN3#DSz$
##### 349 095 503 sm4d26
##### AT_fwQV5TNeDOiaPvJRUwsC2fcEzfzUiPi1
