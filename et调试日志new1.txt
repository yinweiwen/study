FAULTS RECORDS
=================
1. ES 报错 NoNodeAvailableException
	ES集群的IP、端口或集群名称是否正确(es-savoir,not es-savior)
	
2. 本机调试代码报错：org.apache.hadoop.security.AccessControlException: Permission denied: user=yww08, access=WRITE, inode="/anxinyun/structure_data/structure_2/raw/2018/7/液位变送器1.csv":testerone:supergroup:-rw-r--r--
	设置环境变量
	System.setProperty("HADOOP_USER_NAME",hdfsUser)
	
3. 已拥有为“NETStandard.Library”定义的依赖项。
	工具栏---工具----扩展和更新 重装nuget升级
	
4.  Spark: URI is not hierarchical
	引用资源文件错误，修改 `val source=Source.fromFile(getClass.getResource("/jiangsu.json").toURI)`  >> `val source=Source.fromInputStream(getClass.getResourceAsStream("/jiangsu.json"))`
	
5. [Savoir]  修改在Rdd.foreachPartition中存储后，出现Hdfs存储错误：
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /savoir/structure_data/structure_4/theme/2018/7/温湿度.csv for DFSClient_NONMAPREDUCE_-602640997_1250 on 10.8.30.117 because DFSClient_NONMAPREDUCE_-602640997_1250 is already the current lease holder.
	排查可能存在多线程读写文件：
	
6：No configuration setting found for key 'akka'
	<https://stackoverflow.com/questions/31011243/no-configuration-setting-found-for-key-akka-version>
	使用插件maven-shade-plugin,配置见具体项目；
	其中： 
	```xml
		<filters>
			<filter>
				<!--错误:Invalid signature file digest for Manifest main attributes-->
				<artifact>*:*</artifact>
				<excludes>
					<exclude>META-INF/*.SF</exclude>
					<exclude>META-INF/*.DSA</exclude>
					<exclude>META-INF/*.RSA</exclude>
				</excludes>
			</filter>
		</filters>
	```
	
7. java.util.concurrent.TimeoutException: Futures timed out after [100000 milliseconds]
	配置profile未修改为production发布，即还是使用new SparkConf().setMaster("local[*]").setAppName(appname)初始化SparkConf
	
8. Offset commit failed.
	org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	简单翻译一下，“位移提交失败，原因是消费者组开启了rebalance且已然分配对应分区给其他消费者。这表明poll调用间隔超过了max.poll.interval.ms的值，这通常表示poll循环中的消息处理花费了太长的时间。解决方案有两个：1. 增加session.timeout.ms值；2. 减少max.poll.records值”
	
	1. group.max.session.timeout.ms in the server.properties > session.timeout.ms in the consumer.properties.
	2. group.min.session.timeout.ms in the server.properties < session.timeout.ms in the consumer.properties.
	3. request.timeout.ms > session.timeout.ms and fetch.max.wait.ms
	4. (session.timeout.ms)/3 > heartbeat.interval.ms
	5. session.timeout.ms > Worst case processing time of Consumer Records per consumer poll(ms).
	
9. IntelliJ IDEA中提交svn时卡在 performing vcs refresh
	File > Invalidate Caches / Restart...   选择 Invalidate and Restart

10. kubeadm join token过期
	$ kubeadm token create
	abcdef.1234567890abcdef

	# get root ca cert fingerprint
	$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
	e18105ef24bacebb23d694dad491e8ef1c2ea9ade944e784b1f03a15a0d5ecea 





(运行支持kubernetes原生调度的Spark程序)[https://jimmysong.io/kubernetes-handbook/usecases/running-spark-with-kubernetes-native-scheduler.html]

############ AXY部署：
0. K8S环境部署 [](./kubeadm.html)
	
	重装需要清理环境：+
	```
		rm -rf /var/lib/cni/
		rm -rf /var/lib/kubelet/*
		rm -rf /etc/cni/
		ifconfig cni0 down
		ifconfig flannel.1 down
		ifconfig docker0 down
	```
	root下执行提示cr5什么的，是因为配置文件/root/.kube/xxx.config文件缺失

1. Ambari安装后，kafka执行程序路径：
	/usr/hdp/current/kafka-broker/bin
	./kafka-console-consumer.sh --bootstrap-server anxinyun-m1:6667,anxinyun-n1:6667,anxinyun-n2:6667 --topic anxinyun_data --from-beginning

	
2. Group coordinator anxinyun-n2:6667 (id: 2147482644 rack: null) is unavailable or invalid, will attempt rediscovery

	问题分析： stackwork提示 容器内hosts无法连接外部kafka broker
	解决：修改 kube-dns
	
	#kubectl get pod kube-dns-6db897f9f-gmlcz -n kube-system -o yaml --export -f dns/dns.yaml
	
	kubectl get deploy kube-dns -o yaml -n kube-system > kube-deploy.yaml
	kubectl get deploy coredns -o yaml -n kube-system > /tmp/coredns-dump.yaml
	
	添加一下内容：
	```
	dnsPolicy: Default
      hostAliases:
      - hostnames:
        - anxinyun-m1
        ip: 10.8.30.176
      - hostnames:
        - anxinyun-n1
        ip: 10.8.30.177
      - hostnames:
        - anxinyun-n2
        ip: 10.8.30.179
      nodeName: anxinyun-m1
	  ```
	  kubectl delete -f kube-deploy.yaml
	  kubectl create -f kube-deploy.yaml
	
3. [WARN] The short-circuit local reads feature cannot be used because libhadoop cannot be loaded
	同 local hdfs lib not found, 忽略

4. java.lang.ClassNotFoundException: scala.Product  
	运行docker的scala进程时报错
	> kafka 版本错误： 2.12->> 2.11
	
	Exception in thread "main" java.lang.ClassNotFoundException: a=1
	you need to provide exactly one argument: the class of the application supervisor actor
	
	> 修改了pom中的shade-pluging
	
	No configuration setting found for key 'akka.version'
	
	> shade-pluging中增加配置，
		<transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
			<resource>reference.conf</resource>
		</transformer>
		
5.[ERROR] [07/29/2018 05:51:33.289] [default-rediscala.rediscala-client-worker-dispatcher-5] [akka://default/user/RedisClient-$a] CommandFailed(Connect( anxinyun-n1:6379,None,List(KeepAlive(true)),None,false))
	k8s pod的yaml配置文件中，redis.host= anxinyun-n2 中间多了一个空格(MMP)
	
6.  KafkaConsumer is not safe for multi-threaded access

	KafkaConsumer和操作它的线程数必须是1对1的关系
	
7. Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: "spark-et-driver/10.244.1.19"; destination host is: "anxinyun-m1":9000;
	hdfs url配错，ambari默认8020而非9000

7.1  Checkpoint RDD has a different number of partitions from original RDD. Original RDD [ID: 10658, num of partitions: 2]; Checkpoint RDD [ID: 10685, num of partitions: 0].
	
	> 修改cp目录从本地文件格式为 hdfs://.... ....
	又出现 
		org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/user/hdfs/anxinyun-check-point/e7236205-88b8-4d0e-bb3c-12d2549f8ad6/rdd-371/.part-00000-attempt-3":hdfs:hdfs:drwxr-xr-x
			> submit添加如下参数
			--conf spark.kubernetes.driverEnv.SPARK_USER=hdfs \
			--conf spark.kubernetes.driverEnv.HADOOP_USER_NAME=hdfs \
			--conf spark.executorEnv.HADOOP_USER_NAME=hdfs \
			--conf spark.executorEnv.SPARK_USER=hdfs \
			
			还可以通过：
			sudo -u hdfs hdfs dfs -chmod -R 775 /user/check-point
			或者修改hdfs中超级用户组和权限配置：
			fs.permissions.umask-mode=002  dfs.permissions.superusergroup=hdfs   dfs.permissions=false(直接关闭权限验证)
			
			又出现
				Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try
				
				hdfs conf +++
			又出现
				org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /anxinyun/structure_data/structure_1/raw/2018/7/WSD#1.csv for DFSClient_NONMAPREDUCE_50217299_1 on 10.8.30.179 because DFSClient_NONMAPREDUCE_50217299_1 is already the current lease holder.

	[参考]:
		<https://community.hortonworks.com/questions/202369/datanode-failed-to-replace-a-bad-datanode-on-the-e.html>
		<https://community.hortonworks.com/articles/16144/write-or-append-failures-in-very-small-clusters-un.html>

				

8. OOMKilled
	spark driver在k8s中运行一段时间出现 OOMKilled
	未复现
	
9. WEB UI （spark on k8s）
	kubectl port-forward spark-et-driver -n anxinyun  4040:4040
	仅能在driver机器上通过localhost:4040访问
	
	
第二次部署踩坑(08.01)
1. ubuntu系统无法进入
	所有文件处于只读状态
	可以通过 mount -o remount / 方法临时修改，但是重启后依旧
	解决方法：重装系统
	
2. hdb-selecter not found 
	`ambari仓库配置错了`
	'hadoop-client ... failed, ' error param conf
	`/etc/hadoop/conf文件丢失了，重装ambari时会发生的错误，解决方法：手动拷贝`
	kafka-broker启动失败
	``

3. k8s安装完成后所有节点处于NotReady状态
	kubectl get nodes -o wide
	发现kenerl和k8s版本不一致
	解决方法：系统upgrade后，执行以下命令：
kubeadm reset 
systemctl stop kubelet
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /run/flannel
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ip link delete cni0
ip link delete flannel.1
apt purge -y kubelet kubeadm kubectl kubernetes-cni 
systemctl daemon-reload
apt install kubernetes-cni=0.5.1-00 kubelet=1.8.2-00 kubeadm=1.8.2-00 kubectl=1.8.2-00
		
		修改配置
		vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

		添加：
		Environment="KUBELET_EXTRA_ARGS=--fail-swap-on=false"
		重启服务
systemctl daemon-reload
systemctl restart kubelet
		关闭交互分区

		swapoff -a  

		删除 /etc/fstab 表中的交换分区记录
		
		kubeadm init --pod-network-cidr=10.244.0.0/16 --skip-preflight-checks --kubernetes-version=1.8.2
		
		kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
		
		-- kubeadm join --token 54333d.2ce641d3fbefdb3d 10.8.30.36:6443 --discovery-token-ca-cert-hash sha256:7b3d09b3d359cef255fbbddd147618c4c68ea6374fdd05a0005253256ec1ad18
	
10. 其他错误
	lient Version: version.Info{Major:"1", Minor:"8", GitVersion:"v1.8.2", GitCommit:"bdaeafa71f6c7c04636251031f93464384d54963", GitTreeState:"clean", BuildDate:"2017-10-24T19:48:57Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
	Error from server (NotFound): the server could not find the requested resource
	
	
	vents:
  Type     Reason                 Age                From                  Message
  ----     ------                 ----               ----                  -------
  Normal   Scheduled              11m                default-scheduler     Successfully assigned spark-et-driver to anxinyun-n2
  Normal   SuccessfulMountVolume  11m                kubelet, anxinyun-n2  MountVolume.SetUp succeeded for volume "download-jars-volume"
  Normal   SuccessfulMountVolume  11m                kubelet, anxinyun-n2  MountVolume.SetUp succeeded for volume "download-files-volume"
  Warning  FailedMount            11m                kubelet, anxinyun-n2  MountVolume.SetUp failed for volume "spark-init-properties" : configmaps "spark-et-94a3f73e5af13b2d83fb23ba02973cda-init-config" not found
  Normal   SuccessfulMountVolume  11m                kubelet, anxinyun-n2  MountVolume.SetUp succeeded for volume "anxinyun-token-vh5hj"
  Normal   SuccessfulMountVolume  11m                kubelet, anxinyun-n2  MountVolume.SetUp succeeded for volume "spark-init-properties"
  Normal   Pulling                10m (x3 over 11m)  kubelet, anxinyun-n2  pulling image "registry.zhiwucloud.com/anxinyun/spark:latest"
  Warning  Failed                 10m (x3 over 11m)  kubelet, anxinyun-n2  Failed to pull image "registry.zhiwucloud.com/anxinyun/spark:latest": rpc error: code = Unknown desc = Error response from daemon: manifest for registry.zhiwucloud.com/anxinyun/spark:latest not found
  Warning  FailedSync             10m (x8 over 11m)  kubelet, anxinyun-n2  Error syncing pod
  Normal   BackOff                1m (x43 over 11m)  kubelet, anxinyun-n2  Back-off pulling image "registry.zhiwucloud.com/anxinyun/spark:latest"
  
  
## 测试环境合并
1. kubernetes Network Plugin cni failed
	kubernetes flannel Failed to find any valid interface to use
	节点上的flannel pod一直处于 Init:CrashLoopBackOff
	
	解决方法： 判断节点网络是否有问题(能否访问外网、解析域名)
			重装节点kubenertes(上3)

			
## 代码重构1.0 
IDEA 2017.1.6 et
1. scala project Error:java: Compilation failed: internal java compiler error
	
	设置 > java compiler > 设置jdk版本
	
2. Diamond type are not supported at this language level
	Ctrl+Alt+Shift+S (项目设置 ) > Language Level > 8 - Lambdas,type annotations etc.	
	
3.  HDFS文件的错误：No FileSystem for scheme: hdfshttps://www.codelast.com/%E5%8E%9F%E5%88%9B-%E8%A7%A3%E5%86%B3%E8%AF%BB%E5%86%99hdfs%E6%96%87%E4%BB%B6%E7%9A%84%E9%94%99%E8%AF%AF%EF%BC%9Ano-filesystem-for-scheme-hdfs/

4. Exception in thread "main" java.lang.NoClassDefFoundError: scala/Function1
	添加引用
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>2.11.8</version>
        </dependency>
		

5. java项目resource中文件复制到target/class目录下后内容发生了改变(例如https的cacerts.store文件)
	原因： maven resouce的filter插件
	修改为：
	```xml
        <resources>
            <resource>
                <filtering>true</filtering>
                <directory>../src/main/resources</directory>
            </resource>
            <resource>
                <filtering>true</filtering>
                <directory>${project.basedir}/src/main/resources</directory>
                <includes>
                    <include>*.properties</include>
                </includes>
            </resource>
            <resource>
				<!-- 不使用过滤的资源文件 -->
                <filtering>false</filtering>
                <directory>${project.basedir}/src/main/resources</directory>
                <excludes>
                    <exclude>*.properties</exclude>
                </excludes>
            </resource>
        </resources>
	```

6. et_upload 中创建多个kafkastream时接收不到数据
	??
	
7. postman请求https无响应
	postman设置，关闭 SSL certificate verification
	
以后：
1. Caused by: java.io.IOException: Cannot run program "I:\@\hadoop-2.7.3\bin\winutils.exe": CreateProcess error=5, 拒绝访问。
	在windows机器中安装hadoop，参见[3]

2. Offsets out of range with no configured reset policy for partitions (spark-stream kafka)
	https://www.jianshu.com/p/40aee290f484
	
3. windows上安装hadoop （问题1解决思路）
	https://blog.csdn.net/rav009/article/details/70214788
	> https://mirrors.cnnic.cn/apache/hadoop/common/ 下载hadoop镜像并解压
	> https://github.com/steveloughran/winutils 下载对应winutils 覆盖到bin
	> 设置 HADOOP_HOME 以及 Path
	> 修改etc/hadoop-env.cmd下
	  set JAVA_HOME=C:\PROGRA~1\Java\jdk1.8.0_121
	> core-site:
	 <configuration>
			<property>
					<name>fs.defaultFS</name>
					<value>hdfs://localhost:9000</value>
			</property>
	 </configuration>
	> hdfs-site.xml
		<configuration>
				<property>
						<name>dfs.replication</name>
						<value>1</value>
				</property>
				<property>
						<name>dfs.namenode.name.dir</name>
						<value>file:/hadoop/data/dfs/namenode</value>  相同盘符根目录下响应位置
				</property>
				<property>
						<name>dfs.datanode.data.dir</name>
						<value>file:/hadoop/data/dfs/datanode</value>
				</property>
		</configuration>
	> hadoop namenode -format
	  如果不行就修改数据文件夹的权限(everyone)
	> cd sbin
	> start-dfs.cmd
	> OK

4. could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and no node(s) are excluded in this operation.
	怀疑存储空间不足，待验证
	
5. Too many dynamic script compilations within one minute (ElasticSearch)
	重启进程
	
6. mqtt receiver进程重复接收数据；
	savoir的receiver进程中仅订阅了savoir_data，确会收到anxinyun_data的数据
	怀疑：client.id之前订阅过anxinyun_data主题
	解决方法： 修改client.id
	
7. [ERROR] Caused by: java.lang.AssertionError: assertion failed: org.joda.convert.ToString
	scala maven项目编译能通过，执行mvn install时提示错误：
	除了引用joda-time库外还需要引用 joda-convert库
	
8. 安心云ET运行一段时间报错：[Executor] Executor self-exiting due to : Driver spark-et-baed51c5f49f33339750c4602718b303-driver-svc.anxinyun.svc:7078 disassociated! Shutting down
	应该是OOM后被kill,   继续排查spark内存泄漏的问题
	参见[https://www.jianshu.com/p/80dc6209acc0]
	
9. Offsets out of range with no configured reset policy for partitions


10. receiver进程 Lost connection. reconnecting...
Client is connected (32100)
	at org.eclipse.paho.client.mqttv3.internal.ExceptionHelper.createMqttException(ExceptionHelper.java:31)
	at org.eclipse.paho.client.mqttv3.MqttAsyncClient.connect(MqttAsyncClient.java:731)
	
	在mqtt断连回调事件中，将connect方法修改为reconnect
	*(继续跟踪是否解决)
	
11. spark monitor:
记录
http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.SparkListener

12. org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2727.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2727.0 (TID 1122, 10.244.1.109, executor 1): java.lang.NoClassDefFoundError: Could not initialize class et.agg.AggConfig$
 在商用环境用K8S部署ET时出错，同样版本在测试环境正常
 因为在AggConfig初始化代码中出现异常，JSON反序列化构造对象后调用redis初始部分属性超时（测试环境没有超时）

13. java.lang.IllegalStateException: Previously tracked partitions [anxinyun_data2-0] been revoked by Kafka because of consumer rebalance. This is mostly due to another stream with same group id joined, please check if there're different streaming application misconfigure to use same group id. Fundamentally different stream should use different group id
	在商用环境同时起2个K8S-ET，出错
	
14. ES启动失败：bootstrap checks failed  max virtual memory areas vm.max_map_count [65530] is too low
	sudo sysctl -w vm.max_map_count=262144 [ref](https://github.com/docker-library/elasticsearch/issues/111)

15. ES脑裂
	https://my.oschina.net/LucasZhu/blog/1543971
	在商用环境重启ES进程(jvm.Xmx设置),导致重启后没有自动组成集群
	ET入库时显示数据插入成功，查询时发现数据丢失
	
16. java内存管理
	BIG ISSUE
	没有设置xmx，默认(java8) 1/6 physical memory ~ 1/4 physical memory
	You can Check the default Java heap size by:
	java -XX:+PrintFlagsFinal -version | grep -iE 'HeapSize|PermSize|ThreadStackSize'
	
	默认GC规则
	Default garbage collectors:
	Java 7 - Parallel GC
	Java 8 - Parallel GC
	Java 9 - G1 GC
	Java 10 - G1 GC
	[gcpic](https://i.stack.imgur.com/XHfx0.jpg)

	【A Templator Handler】：AXY3.0部分java进程加上默认限制：（recalc,et-hdfs,abn,analyse,fc-alarm,aggregation）
	-Xmx2G -Xms2G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps
	
17. KAFKA IN SPARK-STREAMING常见问题：
	1). 提交OFFSET Rebalance的告警
	
	2).xxxx been revoked by Kafka because of consumer rebalance. This is mostly due to another stream with same group id joined,please check if there're different streaming application misconfigure to use same group id. Fundamentally different stream should use different group id
	
	3). Attempt to heart beat failed since the group is rebalancing, try to re-join group
	
	4). Offsets out of range with no configured reset policy for partitions
		spark-stream的executor中auto.offset.reset置为none,所以当groupid中记录的offset超出记录的上下边界，报此错误
		https://www.jianshu.com/p/40aee290f484
	
18. java.lang.OutOfMemoryError: unable to create new native thread
	spark程序执行过程中出现HDFS相关错误
	修改linux open files限制，设置 ulimit -n xxxx提升限制，ulimit -a查看
	
19. 安心云ES存储代码复制到知物云出现 NoAvaliable Nodes 错误
	ES库版本问题(zhiwu-5.5  anxinyun-6.5)
		<dependency>
            <groupId>org.elasticsearch.client</groupId>
            <artifactId>transport</artifactId>
            <version>6.2.4</version>
        </dependency>
	
20. flatMap && flatten
	// flatMap  TraversableLike.scala

	  def flatMap[B, That](f: A => GenTraversableOnce[B])(implicit bf: CanBuildFrom[Repr, B, That]): That = {
		def builder = bf(repr) // extracted to keep method size under 35 bytes, so that it can be JIT-inlined
		val b = builder
		for (x <- this) b ++= f(x).seq
		b.result
	  }
	  
	// flatten  GenericTraversableTemplate.scala

	  def flatten[B](implicit asTraversable: A => /*<:<!!!*/ GenTraversableOnce[B]): CC[B] = {
		val b = genericBuilder[B]
		for (xs <- sequential)
		  b ++= asTraversable(xs).seq
		b.result()
	  }

	两者功能上几乎没有区别
	通过 ++= 枚举元素进行添加，Map的相同键的元素会覆盖

21. ElasticSearch: [script] Too many dynamic script compilations within, max: [75/5m]
	修改ES配置 5.x max_compilations_per_minute   6.x script.max_compilations_rate
	动态修改:
	PUT /_cluster/settings
	{
		"transient" : {
			"script.max_compilations_rate" : "750/5m"
		}
	}
	
	5.5.1
	curl -XPUT iota-m2:9200/_cluster/settings -d '{"persistent" : {"script.max_compilations_per_minute" : "50"}}'
	
## APACHE SPARK -> FLINK
1. flink中使用joda.DateTime
	flink程序启动时提示 `class org.joda.time.DateTime does not contain a getter for field iMillis`
	  `Class class org.joda.time.DateTime cannot be used as a POJO type because not all fields are valid POJO fields`
	 因为DateTime中iMillis字段没有setter和getter，所以flink不能将其视为POJO类型（更多flink支持数据类型参见 https://ci.apache.org/projects/flink/flink-docs-stable/dev/types_serialization.html）
	 
	 如果使用了检查点（checkpoint）机制，需要设置jodatime的kyro序列化机制（跟spark-streaming是一样的处理）

2. flink启动过程出现NPE （NullPointException）
	因为代码里我将一个case class设置为null了，后将其改成Option[case class]的类型问题解决
	建议：如果数据结构中可能存在空（null）的情况，使用类型Option而不是直接用null赋值
	
3. java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory
	在IDEA中程序可以执行，而打包的jar包无法运行
	你必须指定一种SLF4J的实现jar包在你的classpath里面，和接口jar包一样
	https://stackoverflow.com/questions/12926899/java-lang-noclassdeffounderror-org-slf4j-loggerfactory

	<!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-log4j12 -->
	<dependency>
		<groupId>org.slf4j</groupId>
		<artifactId>slf4j-log4j12</artifactId>
		<version>1.7.25</version>
		<scope>test</scope>
	</dependency>
	
	或者查看是否在maven shade插件中过滤了log相关jar

4. scala项目必须在Maven配置文件中指定scala.version属性
	
    <properties>
        <scala.version>2.11</scala.version>
    </properties>

5. flink程序(k8s)启动后无法读取kafka
	flink java.lang.RuntimeException: Unable to retrieve any partitions with KafkaTopicsDescriptor
	或者
	Committing offsets to Kafka takes longer than the checkpoint interval. Skipping commit of previous offsets because newer complete checkpoint offsets are available. This does not compromise Flink's checkpoint integrity.
	
	检查K8S DNS问题

6. flink中报ES超时错误 [NOT SOLVED]
	告警进程重启后又积压数据，报ES超时错误
	java.lang.RuntimeException: An error occurred in ElasticsearchSink.
	...
	Caused by: java.io.IOException: request retries exceeded max retry timeout [30000]
6.5. flink alarm kafka数据接收中断
	jobmanager报错： Checkpoint 1066 of job 4e65b91bc4ff8a0dbdc2ffd422e1d471 expired before completing
	
7. hdfs 文件存储错误(et-hdfs进程)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE /savoir/structure_data/structure_1116/raw/2019/1/液压式静力水准仪-13.csv for DFSClient_NONMAPREDUCE_1314822252_27 on 10.8.25.214 because this file lease is currently owned by DFSClient_NONMAPREDUCE_1678645347_32 on 10.8.25.232

8. kubenertes维护
	 journalctl -eu kubelet
	 上不了网怎么办 route add default gw 10.8.30.1
	 service firewalld stop
	 service ufw stop
	 ufw disable
	 systemctl disable firewalld
	 
9. hdfs故障
	hdfs put错误：java.io.EOFException: Premature EOF: no length prefix available
	org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1033460583-192.168.0.109-1538992976843:blk_1073805060_77492, type=LAST_IN_PIPELINE, downstreams=0:[]
	java.io.IOException: Failed to move meta file for ReplicaBeingWritten, blk_1073805060_77492, RBW
	  getNumBytes()     = 714
	  getBytesOnDisk()  = 714
	  getVisibleLength()= 714
	  getVolume()       = /home/fs/ds/hadoop/data/current
	  getBlockFile()    = /home/fs/ds/hadoop/data/current/BP-1033460583-192.168.0.109-1538992976843/current/rbw/blk_1073805060
	  bytesAcked=714
	  bytesOnDisk=714 from /home/fs/ds/hadoop/data/current/BP-1033460583-192.168.0.109-1538992976843/current/rbw/blk_1073805060_77492.meta to /home/fs/ds/hadoop/data/current/BP-1033460583-192.168.0.109-1538992976843/current/finalized/subdir38/subdir0/blk_1073805060_77492.meta
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.moveBlockFiles(FsDatasetImpl.java:460)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LDir.addBlock(LDir.java:78)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LDir.addBlock(LDir.java:92)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LDir.addBlock(LDir.java:92)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.LDir.addBlock(LDir.java:71)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.addBlock(BlockPoolSlice.java:248)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlock(FsVolumeImpl.java:199)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeReplica(FsDatasetImpl.java:956)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.finalizeBlock(FsDatasetImpl.java:937)
		at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.finalizeBlock(BlockReceiver.java:1236)
		at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:1196)
		at java.lang.Thread.run(Thread.java:748)
	Caused by: EIO: Input/output error
		at org.apache.hadoop.io.nativeio.NativeIO.renameTo0(Native Method)
		at org.apache.hadoop.io.nativeio.NativeIO.renameTo(NativeIO.java:828)
		at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.moveBlockFiles(FsDatasetImpl.java:458)
		... 11 more
		
10. jenkins nodejs打包报错：Cannot find module 'internal/utils/type'
	npm cache clean --force
	
11. dns错误
	代理报错：
	error: [FS-ERRHD] Error: getaddrinfo EAI_AGAIN console.theiota.cn:443
		at GetAddrInfoReqWrap.onlookup [as oncomplete] (dns.js:67:26)
	修改系统dns
	1. vi /etc/network/interfaces   /etc/init.d/networking restart
	2. vi /etc/resolvconf/resolv.conf.d/head  ++ nameserver 223.5.5.5    resolvconf -u   cat /etc/resolv.conf
	重启k8s dns

12. flink alarm error:
	未解决
	java.lang.Exception: Cannot deploy task Source: Custom Source -> Map -> Filter -> Map (1/3) (f25b30cf14999b76b9b0027000775c8f) - TaskManager (7f9dc13bd4fee89dac6f12a2960de37b @ flink-taskmanager-57f69bfd69-p5phv (dataPort=46709)) not responding after a rpcTimeout of 10000 ms
	at org.apache.flink.runtime.executiongraph.Execution.lambda$deploy$5(Execution.java:624)
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@flink-taskmanager-57f69bfd69-p5phv:41271/user/taskmanager_0#1593533421]] after [10000 ms]. Sender[null] sent message of type "org.apache.flink.runtime.rpc.messages.RemoteRpcInvocation".
	at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
	at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
	
	差不多的问题：
		2019-06-14 07:42:01,480 WARN  org.apache.flink.runtime.taskexecutor.TaskExecutor            - Could not send heartbeat to target with id 3f19b797fd46f27a86f571080bca7f7a.
	java.util.concurrent.CompletionException: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/taskmanager_0#1723018049]] after [10000 ms]. Sender[null] sent message of type "org.apache.flink.runtime.rpc.messages.CallAsync".
		at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
		at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
		at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:647)
		at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:632)
		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)
		at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1977)
		at org.apache.flink.runtime.concurrent.FutureUtils$1.onComplete(FutureUtils.java:772)
		at akka.dispatch.OnComplete.internal(Future.scala:258)
		at akka.dispatch.OnComplete.internal(Future.scala:256)
		at akka.dispatch.japi$CallbackBridge.apply(Future.scala:186)
		at akka.dispatch.japi$CallbackBridge.apply(Future.scala:183)
		at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36)
		at org.apache.flink.runtime.concurrent.Executors$DirectExecutionContext.execute(Executors.java:83)
		at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:44)
		at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:252)
		at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:603)
		at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:126)
		at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
		at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109)
		at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
		at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:329)
		at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:280)
		at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:284)
		at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:236)
		at java.lang.Thread.run(Thread.java:748)
	Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/taskmanager_0#1723018049]] after [10000 ms]. Sender[null] sent message of type "org.apache.flink.runtime.rpc.messages.CallAsync".
		at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:604)
		... 9 more

13. flink error:
	未解决
	Association with remote system [akka.tcp://flink-metrics@flink-taskmanager-] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink-metrics@flink-taskmanager-]] Caused by: [flink-taskmanager- Name does not resolve]
		
		
	2019-05-05 11:45:09,250 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink-metrics@flink-taskmanager-57f69bfd69-xr466:34809] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink-metrics@flink-taskmanager-57f69bfd69-xr466:34809]] Caused by: [flink-taskmanager-57f69bfd69-xr466: Name does not resolve]
	2019-05-05 11:45:09,250 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink-metrics@flink-taskmanager-57f69bfd69-5ksrq:34919] has failed, address is now gated for [50] ms. Reason: [Association failed with [akka.tcp://flink-metrics@flink-taskmanager-57f69bfd69-5ksrq:34919]] Caused by: [flink-taskmanager-57f69bfd69-5ksrq: Name does not resolve]
	
	排查：
	检查jobmanager容器内部hosts，仅发现 
	```shell
	# Kubernetes-managed hosts file.
	127.0.0.1       localhost
	...
	10.244.3.172    flink-jobmanager-7d469bccdf-mfbh5
	```
14. Flink 启动后无法读取kafka
	环境(49华为云) 
	现象：
	Committing offsets to Kafka takes longer than the checkpoint interval. Skipping commit of previous offsets because newer complete checkpoint offsets are available. This does not compromise Flink's checkpoint integrity.
	
	 Error registering AppInfo mbean
	javax.management.InstanceAlreadyExistsException: kafka.consumer:type=app-info,id=consumer-2
	原因：
	
	
15. DOCKERFILE中EntryPoint中执行bash脚本报错：
	"exec format error"  -> 文件头加上 #!/bin/bash
	"file not found" -> 可能是windows下创建的文件格式不正确，需要在linux下创建该文件

16. 告警flink进程疑似内存溢出中断
	OpenJDK 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.
	OpenJDK 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=
	CodeCache: size=245760Kb used=229411Kb max_used=235061Kb free=16348Kb
	 bounds [0x00007f61b7de6000, 0x00007f61c6de6000, 0x00007f61c6de6000]
	 total_blobs=63125 nmethods=62493 adapters=542
	 compilation: enabled
	 
	 
15. 告警flink进程redis报错：
	akka.ConfigurationException: Dispatcher [rediscala.rediscala-client-worker-dispatcher] not configured
	对比可运行版本和当前版本jar解压文件，发现reference.conf文件差异(新包中不含redisscala配置，是被quartz的配置替换了(reference.conf文件重名了!))
	解决方法：shade构建时使用插件：
	
	<transformer
			implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
		<resource>reference.conf</resource>
	</transformer>
	
16. ambari里启动hdfs的namenode时出现 | grep 'Safe mode is OFF'' returned 1.
	解决方法： 关闭HDFS的safemode； 
	$ su hdfs
	$ hdfs dfsadmin -safemode leave
	
17. kubernetes创建pod一直处于pending状态，describe显示No nodes are available that match all of the predicates: Insufficient memory (3), PodToleratesNodeTaints (1).
	> PodToleratesNodeTaints
	/etc/systemd/system/kubelet.service.d/10-kubeadm.conf
	+ Environment="DEF_ARGS=--max-pods=40"
	+ ExecStart=/usr/bin/kubelet 。。。$DEF_ARGS
	
18. Redis-akka ,ET 启动测试用例时报错 :java.lang.ClassNotFoundException: akka.event.slf4j.Slf4jLog
	手动增加引用：
	
        <dependency>
            <groupId>com.typesafe.akka</groupId>
            <artifactId>akka-slf4j_2.11</artifactId>
            <version>2.4.12</version>
        </dependency>
		
19. java.lang.NoClassDefFoundError: org/elasticsearch/common/transport/InetSocketTransportAddress
	启动重计算进程时出现上述错误：
	recalc进程又独立引用了ElasticSearch-Client6.3，去掉
	comm_util已经引用
	
20. 安装k8s 14.3 
	```bash
	apt-get install -y kubelet=1.14.3-00 kubeadm=1.14.3-00 kubectl=1.14.3-00 kubernetes-cni=0.7.5-00
	docker pull repository.anxinyun.cn/k8s/kube-apiserver:v1.14.3
	docker pull repository.anxinyun.cn/k8s/kube-controller-manager:v1.14.3
	docker pull repository.anxinyun.cn/k8s/kube-scheduler:v1.14.3
	docker pull repository.anxinyun.cn/k8s/kube-proxy:v1.14.3
	docker pull repository.anxinyun.cn/k8s/pause:3.1
	docker pull repository.anxinyun.cn/k8s/etcd:3.3.10
	docker pull repository.anxinyun.cn/k8s/coredns:1.3.1
	
	docker tag repository.anxinyun.cn/k8s/kube-apiserver:v1.14.3 k8s.gcr.io/kube-apiserver:v1.14.3
	docker tag repository.anxinyun.cn/k8s/kube-controller-manager:v1.14.3 k8s.gcr.io/kube-controller-manager:v1.14.3
	docker tag repository.anxinyun.cn/k8s/kube-scheduler:v1.14.3 k8s.gcr.io/kube-scheduler:v1.14.3
	docker tag repository.anxinyun.cn/k8s/kube-proxy:v1.14.3 k8s.gcr.io/kube-proxy:v1.14.3
	docker tag repository.anxinyun.cn/k8s/pause:3.1 k8s.gcr.io/pause:3.1
	docker tag repository.anxinyun.cn/k8s/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10
	docker tag repository.anxinyun.cn/k8s/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
	
	MASTER:
	kubeadm init --kubernetes-version=1.14.3 --apiserver-advertise-address=192.168.1.212 --pod-network-cidr=10.244.0.0/16
	
	NODES:
	kubeadm join 10.8.30.36:6443 --token g76iaz.vh3mxrtn9873w4hs --discovery-token-ca-cert-hash sha256:81655beaf8744a1ee840e7fa7d6a8083b5c204dae7c7494d00d6b27860e91a08
	```
	
	
21. 启动Flink Alarm时报错
	[未解决]
	Failed Elasticsearch bulk request: Request cannot be executed; I/O reactor status: STOPPED
	？？ 原因未知
	重启后好了

22. HDFS写文件报错
	org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.SafeModeException): Cannot append to file/savoir/structure_data/structure_2043/raw/2019/9/轴力计ZCL-2-3.csv. Name node is in safe mode.
	Resources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off
	
	NameNode资源清理(dalianggejiejue)
	
23. Recalc进程异常
	日志记录 ElasticSearch中
	Exception in thread "Thread-3" Exception in thread "Thread-4" java.lang.ExceptionInInitializerError
		at org.apache.logging.log4j.core.impl.Log4jLogEvent.createContextData(Log4jLogEvent.java:472)
		
24. Flink进程报错
	[未解决]
	org.apache.flink.runtime.taskexecutor.exceptions.TaskSubmissionException: No task slot allocated for job ID 86f84f68151c4664b0bfbc9d9886eaa2 and allocation ID AllocationID{6ad84c35694804b38fd3181398ef7d9b}.
		at org.apache.flink.runtime.taskexecutor.TaskExecutor.submitTask(TaskExecutor.java:458)
		at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:247)
		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:162)
		at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.onReceive(AkkaRpcActor.java:142)
		at akka.actor.UntypedActor$$anonfun$receive$1.applyOrElse(UntypedActor.scala:165)
		at akka.actor.Actor$class.aroundReceive(Actor.scala:502)
		at akka.actor.UntypedActor.aroundReceive(UntypedActor.scala:95)
		at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526)
		at akka.actor.ActorCell.invoke(ActorCell.scala:495)
		at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257)
		at akka.dispatch.Mailbox.run(Mailbox.scala:224)
		at akka.dispatch.Mailbox.exec(Mailbox.scala:234)
		at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
		at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
		at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
		at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
		
25. HDFS写文件报错java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:808)
	[尝试中]
	https://www.playpi.org/2018122701.html
	
	当任务提交到集群上面以后，多个 datanode 在 getFileSystem 过程中，由于 Configuration 一样，会得到同一个 FileSystem。如果有一个 datanode 在使用完关闭连接，其它的 datanode 在访问时就会出现上述异常，导致数据缺失（如果数据恰好只存在一个 datanode 上面，可能没问题）。
	
	
	*************************************************
	*************************************************
	*************************************************
	*********************W O R K*********************
	*************************************************
	*************************************************
	
	1. ET有原始数据，没有主题数据：
	 > 查看ET日志,看是否有异常(公式计算错误)
	 > 查看设备绑定(是否绑定输出到监测原型)
	 > 是否分组
	 > 是否过滤(滑窗 blabla..)
	 > 设备绑定是旧的(设备输出字段改过，工具ProductManager需要删除_cache重启)
	 
	 
26. ★★★★ Flink 中使用 Lazing ★★★★
	TaskManager中执行任务，好像并不能调用 启动Arguments中参数  <告警Flink中参数启用Extras字段实际无效的BUG>
	
27. ET  KAFKA报错
	[WARN ] 2019-11-20 11:29:48,924 method:org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse(NetworkClient.java:968)
	[Consumer clientId=consumer-1, groupId=et.mains3] Error while fetching metadata with correlation id 38 : {savoir_alarmMsg=LEADER_NOT_AVAILABLE, savoir_data=LEADER_NOT_AVAILABLE, savoir_agg=LEADER_NOT_AVAILABLE}
	[WARN ] 2019-11-20 11:29:48,924 method:org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse(NetworkClient.java:968)
	[Consumer clientId=consumer-1, groupId=et.mains3] Error while fetching metadata with correlation id 38 : {savoir_alarmMsg=LEADER_NOT_AVAILABLE, savoir_data=LEADER_NOT_AVAILABLE, savoir_agg=LEADER_NOT_AVAILABLE}

28. deliver kafka提交超时
	[WARN ] 2019-11-26 11:15:04,314 Auto-commit of offsets {anxinyun_deliver-0=OffsetAndMetadata{offset=20642, metadata=''}} failed for group deliver1: Offset commit failed with a retriable exception. You should retry committing offsets. The underlying error was: The request timed out.

	2019-11-26 14:23:44,500 Auto-commit of offsets {anxinyun_deliver-0=OffsetAndMetadata{offset=20642, metadata=''}} failed for group deliver1: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.

29. MQTT big payload
	问题描述：
	mqtt接收消息超过500k时；接收进程连接断开，之后一直尝试重连
	问题排查过程：
	使用不同的mqtt配置参数和尝试不同的mqtt代理：
	<clean.session:false QoS:2 代理Mosquitto>  x
	<clean.session:false QoS:1 代理Mosquitto>  x
	<clean.session:false QoS:0 代理Mosquitto>  x
	<clean.session:false QoS:2 代理EMQ>  x ps.一个卵样
	<clean.session:false QoS:1 代理EMQ>  x
	<clean.session:false QoS:0 代理EMQ>  x
	<修改发送端的QoS，同样无效>
	<修改EMQX的mqtt.max_packet_size=100MB， 同样无效>
		 env:
        - name: EMQX_MQTT__MAX_PACKET_SIZE
          value: 100MB
	<尝试更换java MQTT-client库，Eclipse Paho ->FuseSource MqttClient >
		https://github.com/eclipse/paho.mqtt.java
		https://github.com/fusesource/mqtt-client
		https://www.jianshu.com/p/95b4e349cde4
		
	√√√ 更换后mqtt接受不再报错，但是kafka报错了：
	java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.RecordTooLargeException: The request included a message larger than the max message size the server will accept.
	kafka服务无法接收超过最大限制的数据包，于是谷歌后修改
		https://my.oschina.net/shyloveliyi/blog/1620012
		配置kafka server：
			message.max.bytes=12695150
		配置后错误没有了！！
		
	基于以上测试结果，于是在代码里用工厂模式实现了两种库的调用，开始自测，结果两种库的实现都能实现1M以上包的传输了（蒙比了）；
	怀疑是kafka的那个异常被Paho内部吃掉了，导致Socket异常重启，于是kafka设置改回去再测试，果然Paho又报错开始重连了
	解决办法:
	kafkaProducer.send方法用 try-catch包住
	引申：
	之前进程也时常出现重连的问题，应该也是kafka-send超时之类的问题导致的.
	总结：
	这是一个异常未捕捉引发的血案，所以一切IO操作一定要做好异常处理，不然被第三方包吃掉异常，故障就真的不好定位了。
	
30. k8s节点无法加入：
	Unable to update cni config: No networks found in /etc/cni/net.d
	https://www.cnblogs.com/zhongyuanzhao000/p/11401031.html
	方法一：
	​ 编辑 /etc/systemd/system/kubelet.service.d/10-kubeadm.conf文件（有的是/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf文件)，
	​ 删除最后一行里的$KUBELET_NETWORK_ARGS 即可。
	​ （该方法治标不治本，没能解决我的问题）

	方法二：
	​ 考虑到node节点的kubelet报错Unable to update cni config: No networks found in /etc/cni/net.d，并且master节点的/etc/cni/net.d目录里拥有10-flannel.conflist文件，
	​ 我们可以把该文件从master节点复制到node节点的对应目录中，然后重启kubelet服务即可。
	​ (该方法亡羊补牢，对我的问题也没用)

	方法三：
	​ 注意到master节点的/etc/cni/net.d/10-flannel.conflist文件是经过 安装flannel插件 才生成的，而node节点则是在master节点安装完flannel插件后才加入集群的，
	​ 所以，我们不妨 先加入node节点，再执行 flannel插件的安装
	
	ps.重装k8s 14:
	++ rm -rf /etc/kubernetes/
	++ apt-get install -y kubelet=1.14.3-00 kubeadm=1.14.3-00 kubectl=1.14.3-00 kubernetes-cni=0.7.5-00
	++ kubeadm join --token qy5bc1.x7ujv4ej14k6gl19 10.8.30.157:6443 --discovery-token-ca-cert-hash sha256:a78c42f3349699dff04b2f079968b126c030259974aae5d40813e2af0057c2dc
	
	
31. Kafka 接收不到数据(et-upload)
	[WARN ] 2019-12-13 10:21:27,763 method:org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$4.onComplete(ConsumerCoordinator.java:649)
Auto-commit of offsets {anxinyun_theme-0=OffsetAndMetadata{offset=97109483, metadata=''}} failed for group et.upload: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.

32. NoSuchMethodError
	Exception in thread "main" java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.metric()
	
	问题“：依赖包冲突了！！
	解决：mvn dependency:tree -Dverbose> dependency.log  查看依赖树，查找对应冲突包的位置
	
33. Jenkins启动
	sudo nsenter --target `sudo docker inspect -f {{.State.Pid}} b0a28a922e07` --mount --uts --ipc --net --pid
	chown 1000:1000 /var/run/docker.sock
	
35. 数据网盘hdfs上文件无法下载
	在hdfs browser里面访问返回：
	No active nodes contain this block

	查看ambari hdfs服务状态，anxinyun-n2节点过了，先把它拉起来
	恢复了

	如果还是没有恢复，建议参考
	https://stackoverflow.com/questions/19205057/how-to-fix-corrupt-hdfs-files
	如果丢失块太多或进入‘安全模式’（只读），手动关闭
	hadoop dfsadmin -safemode leave

36. Flink HA on kubernetes
[官方文档 Standalone/Yarn](https://ci.apache.org/projects/flink/flink-docs-stable/ops/jobmanager_high_availability.html#bootstrap-zookeeper)
org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'hdfs'. The scheme is not directly supported by Flink and no Hadoop file system to support this scheme could be loaded.
拉取的镜像不支持HADOOP
DockerHub上没有对应1.9版本的Flink Hadoop版本。怎么办？

安装nfs服务：
(https://blog.csdn.net/dyzhen/article/details/90693651)
sudo apt install nfs-kernel-server

Service temporarily unavailable due to an ongoing leader election. Please refresh.


37. redis 木马 被黑 病毒
ET报错：
class et.recv.RecvDataHanler$e9e19861-c48b-47c5-96f9-bddff1ce0ef7 empty data received
redis.actors.ReplyErrorException: MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error.
	at redis.actors.RedisReplyDecoder$$anonfun$decodeRedisReply$1.apply(RedisReplyDecoder.scala:68)
	at redis.actors.RedisReplyDecoder$$anonfun$decodeRedisReply$1.apply(RedisReplyDecoder.scala:67)
查看redis状态：
> redis-cli
> config get dir
 "/var/spool/cron"
 连接上去发现数据目录被修改,百度发现是redis被攻击 https://blog.csdn.net/qq_34326603/article/details/103294932
因为不确定是服务进程被黑还是原redis端口6379暴露，解决方法是：
1. 用docker容器创建redis服务，映射之前的redis数据文件；2. 并将端口修改为6378； 3. 修改平台应用配置并重启进程。

后续：
定位是怎么被攻击的。 redis需要加入k8s，通过内部服务的方式提供，不对外暴露端口。

38. C# 程序启动报错
尝试从一个网络位置加载程序集，在早期版本的 .NET Framework 中，这会导致对该程序集进行沙盒处理。此发行版的 .NET Framework 默认情况下不启用 CAS 策略，因此，此加载可能会很危险。
在app.config中增加“
<runtime><loadFromRemoteSources enabled="true"/></runtime>

39. flink 报错
java.lang.Exception: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator

Caused by: java.lang.NullPointerException
	at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:92)
	at org.apache.flink.api.scala.typeutils.CaseClassSerializer.copy(CaseClassSerializer.scala:32)
	
	https://zhuanlan.zhihu.com/p/90807542
	目前我所知的不支持NULL的类型包括Scala Option、Java/Scala Tuple和Scala Case Class
	
40. ES warning
 returned 2 warnings: [299 Elasticsearch-6.8.5-78990e9 "Trying to create more than 500 scroll contexts will not be allowed in the next major version by default. You can change the  [search.max_open_scroll_context] setting to use a greater default value or lower the number of scrolls that you need to run in parallel."],[299 Elasticsearch-6.8.5-78990e9 "'y' year should be replaced with 'u'. Use 'y' for year-of-era. Prefix your date format with '8' to use the new specifier."]
 
 解决方法：
 curl -X PUT http://10.12.42.153:9200/_cluster/settings -H 'Content-Type: application/json' -d'{
    "persistent" : {
        "search.max_open_scroll_context": 5000
    },
    "transient": {
        "search.max_open_scroll_context": 5000
    }
}
'

41. TODO 数据库连接失败 Flink不会自动恢复


42. 3.0直接改factor后遗症记录：
	修改sensor表；
	修改group表；
	item id变化导致：
		t_sensor_factor_threshold
		
43. Flink 部署任务时失败
java.lang.NoSuchMethodError: org.apache.flink.api.java.ClosureCleaner.clean

	Flink k8s 环境问题？
	
44. ES查询警告
[299 Elasticsearch-6.8.2-b506955 "'y' year should be replaced with 'u'. Use 'y' for year-of-era. Prefix your date format with '8' to use the new specifier."]


45. influx unable to find time zone Asia/Shanghai
	要配置$GOROOT 见influx.md
	
46. kafka console producer 发送消息大小限制：
	替代方法：
	cat /tmp/1.txt | ./kafka-console-producer.sh --broker-list node38:6667 --topic anxinyun_data3
	
	
47.
sudo docker save -o /home/user/images/ubuntu_14.04.tar ubuntu:14.04

docker load --input mediaPusherImage.tar 
docker run --name mediaPusher -d -p 9090:9090 -p 1935:1935 anxinyun/mediapusher:m0 /bin/bash /app/mediaPusherBin/startBackground.sh

48. 查看linux下文件夹大小
du -h --max-depth=1 ./

49. zhiwuyun环境k8s容器创建失败
1月 05 14:13:16 anxin-m2 kubelet[25157]: E0105 14:13:16.298912   25157 pod_workers.go:182] Error syncing pod 597a7d8e-4d24-11eb-ad00-c81f66cfe365 ("es-log-clear-1609610400-45sbg_savoir(597a7d8e-4d24-11eb-ad00-c81f66cfe365)"), skipping: failed to "CreatePodSandbox" for "es-log-clear-1609610400-45sbg_savoir(597a7d8e-4d24-11eb-ad00-c81f66cfe365)" with CreatePodSandboxError: "CreatePodSandbox for pod \"es-log-clear-1609610400-45sbg_savoir(597a7d8e-4d24-11eb-ad00-c81f66cfe365)\" failed: rpc error: code = Unknown desc = failed pulling image \"gcr.io/google_containers/pause-amd64:3.0\": Error response from daemon: Get https://gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"

问题：k8s部署容器时需要pause-amd64:3.0这个镜像，翻墙才能下载
解决：下载阿里镜像后提交到私库
docker pull registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0
修改/etc/kubernetes/kubelet配置文件，在KUBELET_ARGS后加
--pod_infra_container_image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0
// 我们的环境 
vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
docker tag hub.c.163.com/conformance/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0
// 安装DNS
docker pull hub.c.163.com/zhijiansd/k8s-dns-kube-dns-amd64:1.14.5
docker pull hub.c.163.com/zhijiansd/k8s-dns-sidecar-amd64:1.14.5
docker pull hub.c.163.com/zhijiansd/k8s-dns-dnsmasq-nanny-amd64:1.14.5

docker tag hub.c.163.com/zhijiansd/k8s-dns-kube-dns-amd64:1.14.5 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5
docker tag hub.c.163.com/zhijiansd/k8s-dns-sidecar-amd64:1.14.5 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5
docker tag hub.c.163.com/zhijiansd/k8s-dns-dnsmasq-nanny-amd64:1.14.5 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5

50. POI执行OOM
在生成nbjj季报表时报错 GC overhead limit exceeded
使用了库poi-tl生成word模板报表
-XX:+UseParallelGC -Xms xxxm
https://stackoverflow.com/questions/49066977/writing-large-of-data-to-excel-gc-overhead-limit-exceeded

实战中使用  -Xmx16384m -Xms16384m 解决

51. flink执行报错
 java.lang.IllegalArgumentException: The scheme (hdfs://, file://, etc) is null. Please specify the file system scheme explicitly in the URI.
 
 解决：flink配置conf/flink-conf.yaml 中state.checkpoints.dir中URI必须包含前缀scheme(hdfs://, file://)
 
 
52. Could not locate executable null\bin\winutils.exe in the Hadoop binaries
 
	+ Download winutils.exe
	+ Create folder, say C:\winutils\bin
	+ Copy winutils.exe inside C:\winutils\bin
	+ Set environment variable HADOOP_HOME to C:\winutils


53. flink getSuperClassOrInterfaceName null (FLINK 闭包检查 报错)

	类中不能有object对象
	类构造参数中尽量使用简单类型
	
54. mqtt数据丢失
	代理 apollo
	java:paho

	解决：client.id不能包含"."
	

55. hdfs写入失败(et-hdfs-gateway进程)
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to APPEND_FILE .... because this file lease is currently owned by ....

Slow ReadProcessor read fields took 49152ms (threshold=30000ms); ack: seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0, targets: [DatanodeInfoWithStorage[10.8.30.117:50010,DS-cc98f7d8-c4f6-477b-9bff-bfced3de30f8,DISK]]

Slow waitForAckedSeqno took 68831ms (threshold=30000ms).

56. Redis Cluster 方法报错
Redis官方推介的库Jedis使用过程中出现（尤其使用keys和scan方法时）
no way to dispatch this command to redis cluster because keys have different slots。

修改了redis的引擎，改用luttence后解决。 


57. flink kafka 偏移量提交问题
```
2021-03-29 11:25:48,847 ERROR org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-24, groupId=et.mainx] Offset commit failed on partition anxinyun_capture_data-1 at offset 0: The coordinator is not aware of this member.
2021-03-29 11:25:48,847 WARN  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - [Consumer clientId=consumer-24, groupId=et.mainx] Asynchronous auto-commit of offsets {anxinyun_capture_data-1=OffsetAndMetadata{offset=0, metadata=''}} failed: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.

简单翻译一下，“位移提交失败，原因是消费者组开启了rebalance且已然分配对应分区给其他消费者。这表明poll调用间隔超过了max.poll.interval.ms的值，这通常表示poll循环中的消息处理花费了太长的时间。解决方案有两个：1. 增加session.timeout.ms值；2. 减少max.poll.records值”
	
	1. group.max.session.timeout.ms in the server.properties > session.timeout.ms in the consumer.properties.
	2. group.min.session.timeout.ms in the server.properties < session.timeout.ms in the consumer.properties.
	3. request.timeout.ms > session.timeout.ms and fetch.max.wait.ms
	4. (session.timeout.ms)/3 > heartbeat.interval.ms
	5. session.timeout.ms > Worst case processing time of Consumer Records per consumer poll(ms).
	
修改和增加max.poll.interval.ms(默认5分钟)到1800s无效，尝试提高session.timeout.ms(默认30s)无效

尝试在flink中使用checkpoint机制提交offset，无效 (https://blog.csdn.net/qq_37332702/article/details/107617253)

因为同时报错：
‘Offset commit failed on partition xxxxxxxxxx at offset 25: The coordinator is not aware of this member.’
尝试更换一个groupid好了 (https://blog.csdn.net/yang_zzu/article/details/107786332)


```

58. [Producer clientId=producer-1] Got error produce response in correlation id 6262947 on topic-partition anxinyun_data-3, splitting and retrying (3 attempts left). Error: MESSAGE_TOO_LARGE
MESSAGE_TOO_LARGE kafka写入失败

59. MQTT丢失连接后重连后，订阅失败：报错
Connection lost (32109) - java.net.SocketException: Broken pipe (Write failed)
	at org.eclipse.paho.client.mqttv3.internal.CommsSender.handleRunException(CommsSender.java:194)
	at org.eclipse.paho.client.mqttv3.internal.CommsSender.run(CommsSender.java:171)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.SocketException: Broken pipe (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at org.eclipse.paho.client.mqttv3.internal.wire.MqttOutputStream.flush(MqttOutputStream.java:49)
	at org.eclipse.paho.client.mqttv3.internal.CommsSender.run(CommsSender.java:149)
	... 1 more
    报错后又断连
    
    重启进程还是报错：
    reason:2
    msg: Invalid client ID
    loc: Invalid client ID
    cause:null
    excep:Invalid client ID(2)
    Invalid Client ID(2)
    at ....createMqttException(ExceptionHelper.java:31)
    at ....notifyReceiveAck(ClientState.java:1040)
    
    修改ClientID后重启成功
    
    【未解决】
  
  
60. Flink 启动 redis create异常
java.lang.ClassCastException: interface akka.serialization.Serializer is not assignable from class akka.remote.serialization.MiscMessageSerializer

Redis中使用了akka，改用redis库，修正


61. MQTT 使用过程的问题 n
MQTT重连后，订阅消息时报错 （另外收到一条信息）
SEVERE: receiver.production20429120721: Timed out as no activity, keepAlive=60,000,000,000 lastOutboundActivity=8,545,217,349,987,626 lastInboundActivity=8,545,157,349,155,183 time=8,545,277,349,973,460 lastPing=8,545,217,349,991,742

https://blog.csdn.net/u012134942/article/details/103965155
显然是因为emqx服务发送数据快，程序处理数据慢。审查代码才发现业务数据已经多线程处理，但emqx客户端的上下线消息并没有多线程处理，处理速度慢，导致tcp连接接收buffer被占满。


62. ES REST Client java报错
i use
BulkResponse bulkResponse = restHighLevelClient.bulk(bulkRequest, RequestOptions.DEFAULT);
Accidental occurrence
java.lang.IllegalStateException: Request cannot be executed; I/O reactor status: STOPPED at org.apache.http.util.Asserts.check(Asserts.java:46) at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase.ensureRunning(CloseableHttpAsyncClientBase.java:90) at org.apache.http.impl.nio.client.InternalHttpAsyncClient.execute(InternalHttpAsyncClient.java:123) at org.elasticsearch.client.RestClient.performRequestAsync(RestClient.java:531) at org.elasticsearch.client.RestClient.performRequestAsyncNoCatch(RestClient.java:516) at org.elasticsearch.client.RestClient.performRequest(RestClient.java:228) at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1593) at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1563) at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1525) at org.elasticsearch.client.RestHighLevelClient.bulk(RestHighLevelClient.java:416)

https://github.com/elastic/elasticsearch/issues/39946
https://github.com/elastic/elasticsearch/issues/42061
https://github.com/elastic/elasticsearch/issues/42133
https://github.com/elastic/elasticsearch/issues/45115
https://github.com/elastic/elasticsearch/issues/49124

63. 容器中暴露udp端口
docker run -p 19600:19600/udp repository.anxinyun.cn/anxinyun/iota-device-proxy:5.21-05-12

64. ElasticsearchException[Elasticsearch exception [type=illegal_argument_exception, reason=Limit of total fields [1000] in index [anxinyun_themes] has been exceeded]]

    PUT anxinyun_themes/_settings
    {
      "index.mapping.total_fields.limit": 2000
    }

    
65. 以太DTU状态显示问题
以太设备状态是DAC存储在Promethus中，以太的Promethus可以通过以下地址访问
http://*****:19090/

接口 link_status
```js
let res = await (0, _util.queryMetric)(opts, `iota_dac_device_status{thingId="${thingId}"}`);
```


66. C# .NET Unable to find or load Npgsql with Entity Framework
The ADO.NET provider with invariant name 'Npgsql' is either not registered in the machine or application config file, or could not be loaded. See the inner exception for details.



67. ambari运维
> ambari中 某些服务显示 心跳丢失 (Heartbeat lost)
 一般需要到对应节点上去重启ambari-agent服务：
 
 ambari-agent stop
 ambari-agent start
 ambari-agent status
 
 还是不行：shutdown -r now
 
 还是不行：重启ambari-server机器
    ambari-server start
    
    
68. ES查询
Must be less than or equal to: [1024]. This limit can be set by changing the [search.max_open_scroll_context] setting

curl -x "" -X PUT 10.8.30.155:9200/_cluster/settings -H 'Content-Type: application/json' -d'{
    "persistent" : {
        "search.max_open_scroll_context": 1024
    },
    "transient": {
        "search.max_open_scroll_context": 1024
    }
}'

69. Flink启动报错
2021-09-17 15:58:24,782 WARN org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn - SASL configuration failed: javax.security.auth.login.LoginException: No JAAS configuration section named 'Client' was found in specified JAAS configuration file: '/tmp/jaas-6452224384874177409.conf'. Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.

2021-09-17 15:58:24,784 INFO org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn - Opening socket connection to server anxinyun-n3/10.8.40.113:2181

2021-09-17 15:58:24,787 INFO org.apache.flink.shaded.zookeeper.org.apache.zookeeper.ClientCnxn - Socket connection established to anxinyun-n3/10.8.40.113:2181, initiating session

2021-09-17 15:58:24,795 ERROR org.apache.flink.shaded.curator.org.apache.curator.ConnectionState - Authentication failed
flink启动[ERROR - main-EventThread] (ConnectionState.java:307) Authentication failed
解决方案
删除zookeeper中的flink目录下所有东西
或者：修改zk目录
high-availability.zookeeper.path.root


70. k8s API 认证问题
error: monitor failed  Error: pods is forbidden: User "system:serviceaccount:anxinyun:default" cannot list resource "pods" in API group "" in the namespace "anxinyun"

需要创建ServiceAccount的访问权限

```sh
kubectl get sa -n anxinyun

# clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: anxinyun
  name: operator
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["services","pods"]
  verbs: ["get", "watch", "list","create","update","patch"]
  
  
kubectl create clusterrolebinding operator-pod \
  --clusterrole=operator  \
  --serviceaccount=anxinyun:default
  
  
  
fastest@test-master:~$ kubectl get sa -n anxinyun -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: "2020-08-17T10:35:35Z"
    name: default
    namespace: anxinyun
    resourceVersion: "5982"
    selfLink: /api/v1/namespaces/anxinyun/serviceaccounts/default
    uid: a1100eea-19c2-4477-afca-61344353f2e5
  secrets:
  - name: default-token-zp6cz
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
fastest@test-master:~$ kubectl describe secret default-token-zp6cz -n anxinyun
Name:         default-token-zp6cz
Namespace:    anxinyun
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: default
              kubernetes.io/service-account.uid: a1100eea-19c2-4477-afca-61344353f2e5

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  8 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImFiVlF0Y1NyZjNNTkRVMFVieTNNTzhyVlc5T094Y3J2RmFfYTF6R0pveDQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJhbnhpbnl1biIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLXpwNmN6Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhMTEwMGVlYS0xOWMyLTQ0NzctYWZjYS02MTM0NDM1M2YyZTUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6YW54aW55dW46ZGVmYXVsdCJ9.elI35PPYtQp-fletleFR7so88Vozk7g8B7oRa1zy2LxSL1m26s8X6SJAipR5uqweNyi8JML3Yo3lPhs6mmzNLxkTRVk1atyXcCSr6J_iPD2dUUaGTL-ZPRYZ1x8Eb2PfugEQM5tf5YXERXqPpEsxTLM83KkI8ogFJQhLG7s-lWZFbvcgmKpCo3lmzuYf-hO0-JngjLhRxptCUqaFx6s8QwQz0dxNn_EvtMXbZm2cTkewJdsFAzczuKtt2sLiJCl5CSRghWAqkP9pBiC2diwDKzz9A0DevG0b3n7J-9_4fPtbXa5zQI60Rg3XVZRof0XNjw5Nze0ee8bn-6XI8yxIug
fastest@test-master:~$ 
```

71. ambari 心跳失联
ambari-agent.service: Supervising process 9678 which is not our child. We'll most likely not notice when it exits

?? 
systemctl start ambari-agent.service
ambari-agent restart

?? anxinyun- flink程序报错，看不到细节，可以从UI查看stdout

72. Nodejs官方库 ElasticSearch查询超时
```js
// opts.server.sniffOnStart = true;
    // opts.server.sniffInterval = 60000;
```


73. 服务器上docker tag的国内源镜像丢失
> 大亮：服务器上磁盘空间超过80% 会自动清理
有时间看下在哪里设置 或 清理点空间

74. Invoke频繁超时
> 三个api容器分别进行下发，发现其中一个容器容易出现超时。

三个容器中使用了不同的consumer-group,更换groupid后问题解决。


75. 定时执行es备份任务（机房迁移21-11）
root@anxin-m1:/home/anxin/tools/es2u# crontab -l
0 * * * * /usr/hdp/share/hst/bin/hst-scheduled-capture.sh sync
0 2 * * 0 /usr/hdp/share/hst/bin/hst-scheduled-capture.sh
40 21 * * * /home/anxin/tools/es2u/2709.sh
5 0 * * * /home/anxin/tools/es2u/2800.sh
5 6 * * * /home/anxin/tools/es2u/2806.sh
1 7 * * * /home/anxin/tools/es2u/2807.sh


76. CNB
    1. 统一数据处理引擎架构，将flink框架应用于ET和alarm进程。
    2. 通用工具库统一编写管理，例如数据库操作、Redis读写、Kafka读写、ES操作库等，并上传maven、npm包私服管理器。
    3. 全面应用云原生部署方案，基于Kubernetes的容器化部署管理。
    4. 满足公有云和私有云融和部署，实现云平台服务下沉本地化。统一公有云和私有云技术方案。
    5. 视频golang优化改造，统一后台支撑技术栈开发语言.
    
77. Kibana容器部署
docker run --name kibana7 -p 5601:5601 -e "ELASTICSEARCH_HOSTS=http://test-n3:9200" docker.elastic.co/kibana/kibana:7.16.2



78. ES Docker AccessDeny
目录权限
sudo chown -R 1000:root docker_data


79. DAC中pq: sorry, too many clients already
【未解决】
2022/03/28 15:04:06.550982 [E] ds.GetProtocol error: pq: sorry, too many clients already


80. 问题【79】导致dac卡死
【未解决】
```sh
# k8s查看pod状态：正常
root@anxin-m1:/home/anxin/iota/k8s/iota-web# ki | grep dac
iota-dac-0                                              2/2       Running            2          47d       10.244.1.81    iota-n3

# 查看容器状态：正常
root@iota-n3:~# docker ps | grep dac


...
f52f068a425e        repository.anxinyun.cn/iota/dac              "./dac"                  5 days ago          Up 5 days                               k8s_iota-dac_iota-dac-6_iota_263e5a76-8990-11ec-8c30-c81f66cfe365_27
...
# 查看容器对应宿主机进程ID，发现进程ID丢失
root@iota-n3:~# docker inspect -f '{{.State.Pid}} {{.Id}}' $(docker ps -a -q) | grep f52f068a425e
10823 f52f068a425e0a41ab4dcf72169b2e67121900aad6c4b1317108520068320102
root@iota-n3:~# ps -ef | grep 10823
root     32580 15338  0 11:10 pts/10   00:00:00 grep --color=auto 10823
```

81. Docker : How to avoid Operation not permitted in Docker Container?
web:
  image: an_image-image:1.0
  container_name: my-container
  privileged: true
  entrypoint: ["/usr/sbin/init"]
  ports:
    - "8280:8280"