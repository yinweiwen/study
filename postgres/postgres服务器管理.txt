## postgres安装
apt update
apt install postgresql postgresql-contrib

vi /etc/postgresql/9.5/main/pg_hba.conf
添加如下行：
host    all             all             10.8.30.0/24            md5


su postgres

在磁盘上初始化一个数据库存储区域。 我们称之为一个数据库集簇（SQL 标准使用的术语是目录集簇）
$ initdb -D /usr/local/pgsql/data
-- 
$ pg_ctl -D /usr/local/pgsql/data initdb
启动数据库服务器
$ postgres -D /usr/local/pgsql/data
pg_ctl start -l logfile

关闭：
三种信号 - 1.SIGTERM 智能关闭 2.SIGINT 快速关闭 3.SIGQUIT 立即关闭
pg_ctl stop [-W] [-t seconds] [-s] [-D datadir] [-m s[mart] | f[ast] | i[mmediate] ]

pg_ctl reload
SQL函数 pg_reload_conf()

升级（主发行版本）的方式：
1. 先转储然后重新载入
2. pg_upgrade

客户端认证
/var/lib/pgsql/9.6/data/pg_hba.conf

# "local" is for Unix domain socket connections only
local   all             all                                     peer
# IPv4 local connections:
host    all             all             127.0.0.1/32            ident
host    all             all             10.8.30.117/32          trust

常见psql操作：
psql -U user -d database
切换数据库 \c database
列举数据库 \l
列举表 \dt
查看表结构 \d tbname
查看索引 \di
查看编码格式 \encoding

文件位置：
默认的文件路径：/var/lib/pgsql/9.x/data
数据集簇PGDATA
PGDATA目录下：
	配置文件：
		pg_hba.conf			pg的访问权限
		pg_ident.conf		操作系统用户名和pg中用户名的映射关系
		postgresql.conf		
	数据文件：
		base	包含每个数据库对应的子目录的子目录
			子目录的名字是数据库在pg_database中的OID
			名称为1的是模板数据库template1.(template1在initdb完成之后是可以由用户修改的，template0则始终提供一个未被修改的干净模板)
				每个表和索引都存储在单独的文件中，以该表或索引的filenode号命名(pg_class.relfilenode 注意filenode和OID不尽相同)
				文件大小超过1G后，分裂成1G大小的段，命名如filenode.1,filenode.2等
				大对象另外存储在TOAST表中
				_fsm
				_vm
		global	包含集群范围的表的子目录，比如pg_database
		pg_clog	包含事务提交状态数据的子目录
		pg_multixact	包含多重事务（multi-transaction）状态数据的子目录（用于共享的行锁）
		pg_subtrans	包含子事务状态数据的子目录
		pg_tblspc	包含指向表空间的符号链接的子目录
			默认表空间没有在此目录下关联 
			pg_default - PGDATA/base
			pg_globla -- PGDATA/global
		pg_twophase	包含用于准备好事务状态文件的子目录
		pg_xlog
	其他：
		postmaster.opts		记录 postmaster 最后一次启动时使用的命令行参数的文件
		PG_VERSION			版本号
		postmaster.pid		服务进程号pid以及共享内存段ID

		
参数设置：
修改参数设置方法：
	修改postgresql.conf文件
	ALTER SYSTEM
	系统表pg_file_settings
	对本地会话 SET configuration_parameter TO DEFAULT;  (SHOW [Configuration_parameter/ALL])
	服务器启动时 postgres -c log_connections=yes
	include 'shared.conf'
	
数据库：
	CREATE/DROP ROLE name
	CREATE DATABASE dbname OWNER rolename TEMPLATE template0
	DTOP DATABASE dbname
	表空间 CREATE TABLESPACE xxx LOCATION '/ssd1/postgres/data'
		CREATE TABLE table TABLESPACE space
		
清理：
	VACUUM、VACUUM FULL
	1. 恢复磁盘空间
		MVCC
		包含大量死亡版本时，VACUUM FULL/CLUSTER/ALTER TABLE的表重写变体，需要额外的磁盘空间
		TRUNCATE - 立即移除该表的整个内容
	2. 更新规划器统计信息
		VACUUM ANALYZE
	3. 更新可见性映射
	4. 防止事务 ID 回卷失败

	REINDEX - 提高索引空间利用率、提供访问速度(B-Tree)
	
	日志
	配置在 postgresql.conf 中‘ERROR REPORTING AND LOGGING’小节
	
		
系统表：

## REINDEX { INDEX | TABLE | DATABASE | SYSTEM } name [ FORCE ]

##2018 comming

#清理空闲连接
SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid() AND state = 'idle' AND datname='AnxinCloud';


SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='SmartSite' and pid <> pg_backend_pid();
-- 当前活动连接
select count(1) from pg_stat_activity;
-- 显示最大连接数
show max_connections;
-- 显示保留连接数
show superuser_reserved_connections;

PG执行Add column操作阻塞，问题未知；解决方法：重启pg服务


## 数据库备份和还原
# 23test 
	su postgres
	
	pg_dump -h 127.0.0.1 -U FashionAdmin -d SavoirCloud > /tmp/sc.bak

	(dropdb -U FashionAdmin SecureCloud)
	createdb -h 127.0.0.1 -p 5432 -U FashionAdmin SecureCloud

	psql -h 127.0.0.1 -U FashionAdmin -d SecureCloud < /tmp/sc.bak

	psql -U FashionAdmin -d SecureCloud
    
    <><><>
    pg_restore -d dbname -U FashionAdmin filename


  自动备份，不用输入密码
  pg_dump "host=iota-m1 port=5433 user=FashionAdmin password=123456 dbname=SavoirCloud" | gzip > /home/databasebak/savoirbk20210907.bak
# 删除数据库
	dropdb -h 127.0.0.1 -U FashionAdmin SecureCloud
	或
	psql> drop database SecureCloud
	提示有连接无法删除：
	SELECT pg_terminate_backend(pid)
	FROM pg_stat_activity
	WHERE datname = 'mydb';

执行数据库脚本
psql -U postgres -f secure_cloud.sql 

pg_dump --dbname="Anxinyun0726" --file="anxinyun0726.sql" --create -U FashionAdmin

复制数据到csv：
psql

\copy (Select * From foo) To '/tmp/test.csv' With CSV

psql -h iota-m1 -p 5433 -d iOTA_console -U postgres "\copy ($(<devices.sql)) To '/tmp/devices.csv' With CSV";



create TEMP VIEW v1 AS
select m.id,m.name,m.image,m.model,m."desc",m."createdAt" as created_at,m."updatedAt" as updated_at, \
       c.name as company,
       dp.dps,u.username as username,
       i.name iname,it."name" itname,it."desc" itdesc,
       cm.name cmname,cm."desc" cmdesc,
       pm.name pmname,pm."desc" pmdesc,pm.resource pmresource,
       cp.caps,

       r.properties
from "DeviceMeta" m -- 设备原型
left join "CompanyCertification" c on m."vendorId"=c.id -- 公司
join "User" u on c."userId"=u.id -- 用户
left join (SELECT "deviceMetaId",jsonb_agg((SELECT x FROM (SELECT name,"showName","desc") x)) AS dps FROM "DeviceProperty"
    GROUP BY "deviceMetaId") dp on dp."deviceMetaId"=m.id -- 设备上的属性
left join "CapabilityMeta" cm on m.id=cm."deviceMetaId"  -- 能力原型
left join "ProtocolMeta" pm on cm."protocolMetaId"=pm.id -- 根据能力原型找到协议
left join "CapabilityMetaInterface" cmi on cmi."capabilityMetaId"=cm.id -- 能力原型接口表（1:n)
left join "DeviceMetaInterface" dmi on dmi.id=cmi."deviceMetaInterfaceId" -- 接口关系表
left join "InterfaceMeta" i on dmi."interfaceMetaId"=i.id -- 接口原型
left join "InterfaceType" it on i."interfaceTypeId"=it.id
left join (SELECT "capabilityMetaId",jsonb_agg((SELECT x FROM (SELECT name,"showName","desc","unit",category) x)) AS caps FROM "CapabilityProperty"
    GROUP BY "capabilityMetaId") cp on cp."capabilityMetaId"= cm.id
inner join "FilteredResource" r on r.key=m.id and r.id= (select id from "FilteredResource" r1 where r1.key=m.id order by r1."createdAt" desc limit 1 );

\copy (select * from v2) To '/tmp/devices2023.csv' With CSV;

DROP VIEW v1;



# 让自增 Seq (Serial) 跟上ID
	PERFORM setval('"public"."T_DIM_FORMULA_PARA_FormulaParaID_seq"',(SELECT MAX(formula_para_id) FROM t_formula_para), true);
	
# pg_dump 导出
pg_dump --dbname="haiwen1106" --file=/home/anxin/dump1.sql -h 10.8.30.32 --no-owner --column-inserts



# 时间处理：
select s."SENSOR_LOCATION_DESCRIPTION",b."TEMPERATURE_VALUE",b."ACQUISITION_DATETIME" from "T_DIM_SENSOR" s,"T_THEMES_ENVI_TEMP_HUMI" b where s."SENSOR_ID"=b."SENSOR_ID" and b."ID" in (
  select min("ID") from "T_THEMES_ENVI_TEMP_HUMI"  where "ACQUISITION_DATETIME">'2019-12-01' and "ACQUISITION_DATETIME"<'2019-12-11'
  GROUP BY "SENSOR_ID",date_trunc('hour',"ACQUISITION_DATETIME"),floor(date_part('minute',"ACQUISITION_DATETIME")/30)
) ORDER BY "ACQUISITION_DATETIME";
以上sql：没30分钟取一条数据

select now(); 
select current_date;

select date_part('day',now());

select date_part('day',now()::timestamp-'2018-01-10 10:12:15::timestamp');

select extract(day from now()-create_time) from t_test;

select now()-interval '2 day';

select date_trunc('month',now()); -- 本月第一天


## upsert 
insert into t_data_latest(sensor_id,safety_factor_type_id,acquisition_datetime,data)
values(1,1,'2019-12-15','{1,2,3}') ON CONFLICT(sensor_id)
    DO UPDATE set acquisition_datetime='2019-12-15 01:00:00',data='{2,null,1}';


## windows 数据文件迁移
9.6：
[https://www.landui.com/help/show-3587.html](Windows如何设置或更改PostgreSQL数据目录位置)  [https://radumas.info/blog/tutorial/2016/08/08/Migrating-PostgreSQL-Data-Directory-Windows.html](%%)
	修改注册表：
	对应键值位置在"HKEY_LOCAL_MACHINESYSTEMCurrentControlSetServicespostgresql-x64-9.5ImagePath"，将“－D”后的目录名修改为新的数据目录位置即可，如果目录路径中含有空格，需要用引号引起。
	移动文件。
	启动时报超时错误：
	pg_resetxlog.exe -f [pgData folder location]



## 高频率更新的表文件比较大
	
	现象：t_data_latest最新数据表，数据经常update，2周后数据表大小接近1G
	https://dba.stackexchange.com/questions/69464/postgres-table-growing-on-massive-updates
	
	解决：
	
## postgresql 数据库大小查看

1. SELECT oid from pg_database where datname='postgres';
到数据库数据所在目录下查找对应文件夹的大小即可

2.select pg_size_pretty(pg_database_size('NBJJ_1114'));

 pg_dump --dbname="dayawan_ceshi" --no-owner --schema='public' -t "t_sync_equ_orgs" -t "t_sync_risk_orgs" > /tmp/dayaw.sql



-- 查询数据库占用的磁盘空间
select pg_size_pretty(pg_database_size('NBJJ_1114'));

-- 查询指定数据库下各个表的数据条数
select relname,reltuples as rowCounts
from pg_class
where relkind = 'r' and relnamespace = (select oid from pg_namespace where nspname='public')
and relname like 't_%' order by rowCounts desc;

-- 查询各个表大小
select relname, 
       pg_size_pretty(pg_relation_size(relid)) 
from pg_stat_user_tables 
where schemaname = 'public' 
order by pg_relation_size(relid) desc; 

## 数据库数据部分备份方法：
pg_dump -U postgres -d ztjsmartsite --no-owner --data-only --table=public.t_worker -f /worker.sql 

 
 备份所有表结构DLL
 pg_dump -h 10.8.30.32 -U postgres --dbname="haiwen20200401" --no-owner --schema='public' --oids --schema-only --file="/tmp/schema.sql"
 pg_dump -U postgres --dbname="Anxinyun0726" --no-owner --schema='public' --oids -T 't_user_token' --file="/tmp/data.sql"
 
 指定表数据备份
 pg_dump --dbname="AnxinCloud" --no-owner --schema='public'  -T 't_user_token' --file="/tmp/anxincloud0824.sql"
 
  pg_dump -h 10.8.30.32 -U postgres --dbname="haiwen20200401" --no-owner --schema='public' --data-only --table=public.\"T_ALARM_PUSHPOLICY\" --table=public.\"T_ALARM_PUSHPOLICY_DETAIL\" --table=public.\"T_ALARM_RECEIVER\" --table=public.\"T_ALARM_RECEIVER_STRUCTURE\" --table=public.\"T_BIM\" --table=public.\"T_BIM_STATIONS\" --table=public.\"T_BRIDGE_COMPONENT\" --table=public.\"T_BRIDGE_EVALUATE\" --table=public.\"T_BRIDGE_EVALUATE_SCORE\" --table=public.\"T_BRIDGE_MEMBER\" --table=public.\"T_BRIDGE_PART\" --table=public.\"T_CONSTRUCTION_LOG_DETAIL\" --table=public.\"T_DATA_AGGREGATION\" --table=public.\"T_DATA_FILE_TYPE\"   --table=public.\"T_DATA_OVERLOAD_STATISTIC\" --table=public.\"T_DATA_RATIONAL_FILTER_CONFIG\" --table=public.\"T_DIM_AGG_CONFIG\" --table=public.\"T_DIM_AGG_TYPE\" --table=public.\"T_DIM_AGG_WAY\" --table=public.\"T_DIM_CALCULATION_METHOD\" --table=public.\"T_DIM_CONSTRUCTION_LOG_ITEM\" --table=public.\"T_DIM_CONSTRUCTION_LOG_TEMPLATE\" --table=public.\"T_DIM_CONSTRUCTION_TEMPLATE_ITEM\" --table=public.\"T_DIM_DATETIME\" --table=public.\"T_DIM_DEVICE_STATUS_LATEST\" --table=public.\"T_DIM_DIRECTION\" --table=public.\"T_DIM_DTU\" --table=public.\"T_DIM_DTU_ACCESS\" --table=public.\"T_DIM_DTU_PRODUCT\" --table=public.\"T_DIM_ETHERNET\" --table=public.\"T_DIM_FACTOR_AGG_CONFIG\" --table=public.\"T_DIM_FACTOR_PRODUCT_TYPE\" --table=public.\"T_DIM_FACTOR_UNIT_INT\" --table=public.\"T_DIM_FATIGUE_DAMAGE_CONFIG\" --table=public.\"T_DIM_FATIGUE_DAMAGE_INFO\" --table=public.\"T_DIM_FORMULAID\" --table=public.\"T_DIM_FORMULAID_SET\" --table=public.\"T_DIM_FORMULA_PARA\" --table=public.\"T_DIM_FORMULA_PARA_NAME\" --table=public.\"T_DIM_GROUP\" --table=public.\"T_DIM_GROUP_TYPE\" --table=public.\"T_DIM_GROUP_TYPE_SAFE_FACTOR\"  --table=public.\"T_DIM_IDAU_DIAGNOSIS_ATTRIBUTES\" --table=public.\"T_DIM_LOCAL_FILE\" --table=public.\"T_DIM_NETWORK\" --table=public.\"T_DIM_NVR_FACTORY\" --table=public.\"T_DIM_PHONE_VALIDATE_CODE\" --table=public.\"T_DIM_PRODUCTCATAGORY_ORIGINALDATA\" --table=public.\"T_DIM_PRODUCT_CATEGORY\" --table=public.\"T_DIM_PRODUCT_TYPE\" --table=public.\"T_DIM_PROTOCOL_TYPE\" --table=public.\"T_DIM_REGION\" --table=public.\"T_DIM_REGION_TYPE\" --table=public.\"T_DIM_REMOTE_DTU\" --table=public.\"T_DIM_RESOURCE\" --table=public.\"T_DIM_ROLE\" --table=public.\"T_DIM_ROLE_RESOURCE\" --table=public.\"T_DIM_SAFETY_FACTOR_TYPE\" --table=public.\"T_DIM_SENSOR\" --table=public.\"T_DIM_SENSORPRODUCT_SAFETYFACTORTYPE\" --table=public.\"T_DIM_SENSOR_CORRENT\" --table=public.\"T_DIM_SENSOR_EXT\" --table=public.\"T_DIM_SENSOR_FATIGUE_DAMAGE\" --table=public.\"T_DIM_SENSOR_GPS\" --table=public.\"T_DIM_SENSOR_GROUP\" --table=public.\"T_DIM_SENSOR_GROUP_CEXIE\" --table=public.\"T_DIM_SENSOR_GROUP_CHENJIANG\" --table=public.\"T_DIM_SENSOR_GROUP_JINRUNXIAN\" --table=public.\"T_DIM_SENSOR_PRODUCT\" --table=public.\"T_DIM_SENSOR_TIME_BATCH\" --table=public.\"T_DIM_SERIAL_PORT\" --table=public.\"T_DIM_STRUCTUER_LINE\" --table=public.\"T_DIM_STRUCTUER_PROGRESS\" --table=public.\"T_DIM_STRUCTURE\" --table=public.\"T_DIM_STRUCTURE_CONSTRUCTION_LOG_CONFIG\" --table=public.\"T_DIM_STRUCTURE_FACTOR\" --table=public.\"T_DIM_STRUCTURE_TYPE\" --table=public.\"T_DIM_STRUCTURE_TYPE_FACTOR\" --table=public.\"T_DIM_STRUCT_DTU\" --table=public.\"T_DIM_STRUCT_FACTOR_UNIT\" --table=public.\"T_DIM_USER\" --table=public.\"T_DIM_USER_STRUCTURE\" --table=public.\"T_DIM_VEHICLE_CATEGORY\" --table=public.\"T_DIM_VEHICLE_OVERLOAD\" --table=public.\"T_DIM_VIBRATION_CHANNEL\" --table=public.\"T_DIM_VIBRATION_MODULE\" --table=public.\"T_DIM_VIDEOPOINT_SENSOR\" --table=public.\"T_DIM_VIDEO_FACTOR\" --table=public.\"T_DIM_VIDEO_MONITORING_POINTS\" --table=public.\"T_DIM_VIDEO_NVR\" --table=public.\"T_DIM_VIDEO_PUSH_SERVER_ADDRESS\" --table=public.\"T_DIM_VIDEO_STRUCTURE\" --table=public.\"T_DIM_WARNING_DEVICETYPE\" --table=public.\"T_DIM_WARNING_TYPE\" --table=public.\"T_EXTRACT_SENSOR_MAP\" --table=public.\"T_EXTRACT_TABLE_MAP\" --table=public.\"T_EXTRACT_TIME\" --table=public.\"T_FACT_CHANGE_RATE_THRESHOLD\" --table=public.\"T_FACT_SENSOR_THRESHOLD\" --table=public.\"T_REPORT_CONFIG\" --table=public.\"T_REPORT_CONFIG_TEMPLATE\" --table=public.\"T_REPORT_TEMPLATE\" --table=public.UserToken  --file="/tmp/data.sql"
  
  /**
  拆寻数据记录 大小 数据条数
  select relname,reltuples as rowCounts
from pg_class
where relkind = 'r' and relnamespace = (select oid from pg_namespace where nspname='public')
and relname like 't_%' order by rowCounts desc;
  
  **/
  ## with pattern
  pg_dump -h 10.8.30.32 -U postgres --dbname="NBJJ_1114" --no-owner --schema='public' --oids --schema-only --file="/tmp/schema.sql"
  pg_dump -h 10.8.30.32 -U postgres --dbname="NBJJ_1114" --no-owner --schema='public' --data-only -T 't*log' -T 't_data_traffic_load_original' -T 't_themes_traffic_load_simplify' -T 't_sensor_status_statistics' -T 't_data_original' -T 't_data_latest' -T 't_themes*' -T 't_warning' -T 't_warning_archive' -T 't_warning_detail' -T 't_warning_deal_detail' -T 't_user_token' --file="/tmp/data.sql"
  
  还原：
  psql -U postgres -d haiwen -f ./Desktop/schema.sql
  psql -U postgres -d haiwen -f ./Desktop/data.sql
=======
-- 查询索引大小
select indexrelname, 
       pg_size_pretty(pg_relation_size(indexrelid)) 
from pg_stat_user_indexes 
where schemaname = 'public' 
order by pg_relation_size(indexrelid) desc; 

-- 查询表大小
select relname, 
       pg_size_pretty(pg_relation_size(relid)) 
from pg_stat_user_tables 
where schemaname = 'public' 
order by pg_relation_size(relid) desc; 


copy (select st.name,f.name,f.id,f.proto,fpix from t_structure_type st,t_structure_type_factor sf,t_factor f
LEFT JOIN
  (SELECT proto,jsonb_agg((SELECT x FROM (SELECT faci.id,faci.name,faci.field_name,unit.name as unit_name) x)) AS fpix FROM t_factor_proto_item faci,t_item_unit unit
  WHERE unit.item=faci.id AND unit.if_default GROUP BY proto) AS fpi ON fpi.proto=f.proto
where sf.factor=f.id and st.id=sf.structure_type order by st.id,f.proto) to '/tmp/allfactors.txt';


-- 查询某个表的所有外键约束

SELECT x.table_name, x.column_name,x.constraint_name

         FROM  information_schema.key_column_usage x

      INNER JOIN  (SELECT  t.relname,

                   a.conname

             FROM  pg_constraint a

             INNER JOIN pg_class ft

                     ON ft.oid = a.confrelid

             INNER JOIN pg_class t

                     ON t.oid = a.conrelid

            WHERE  a.contype = 'f'

              AND  a.confrelid =

                   (select e.oid

                      from pg_class e

                     where e.relname = 't_sensor')

            ) tp

      ON (x.table_name = tp.relname AND

          x.constraint_name = tp.conname);
		  
## 查询物联网卡：
// pg
select t.id,t.name,t."createdAt",u.username,d.properties::json->'SIMNum' from "Thing" t,"Device" d,"User" u
where d."thingId"=t.id and
      t."belongTo"=u.id and
      d."deviceMetaId" in (select id from "DeviceMeta" where name='DTU')
and char_length((d.properties::json->'SIMNum')::varchar)>13;

sqlserver
select st.STRUCTURE_NAME_CN,REMOTE_DTU_SUBSCRIBER,u.PlatformId from T_DIM_STRUCTURE st,T_DIM_STRUCT_DTU sd,T_DIM_REMOTE_DTU d,T_DIM_USER u
where st.IsDelete=0 and st.ID=sd.StructureId
  and sd.DtuId=d.ID and st.owner=u.USER_NO
and REMOTE_DTU_SUBSCRIBER is not null and len(replace(REMOTE_DTU_SUBSCRIBER,' ',''))>11;

## 更新jsonb中某一个字段
> use jsonb_set
UPDATE "Device" SET properties
    = jsonb_set(properties::jsonb, '{url}', '"http://10.8.40.250:7005/gnss/"'::jsonb, false)  WHERE (properties::json->'url')::varchar like '%http://10.8.25.254:7005/gnss/%';
    
    
## 数据库自动备份

https://www.cnblogs.com/telwanggs/p/11547370.html

https://www.jianshu.com/p/b2c254897071

## pg_dump 备份
### Options
```
dbname
指定数据库的名称，如果没有指定,会引用环境变量 PGDATABASE， 如果环境变量没有设置,那么会指定使用用户名来连接。

-a 
--data-only
只转储数据,而不是架构(数据的定义)。表中的数据，大对象的序列值被转储。此选项与specifying --section=data类似。

-b 
--blobs
包含大对象，当指定参数为 --schema, --table, or --schema-only 是该动作默认执行，所以-b选项只在选择性转储数据时有用。

-c 
--clean
输出在创建数据库创建命令之前先清理（删除）该数据库对象的命令
此选项只对纯文本格式有意义的。对于归档格式，你可以在调用pg_restore时指定该选项

-C 
--create
以一条创建该数据库本身并且与这个数据库联接等命令开头进行输出。 (如果是这种形式的脚本，那么你在运行脚本之前和哪个数据库联接就 不重要了。) ，此选项只对纯文本格式有意义的。对于归档格式，你可以在调用pg_restore时指定该选项。

-E encoding
--encoding=encoding
指定字符集编码。 默认情况下使用数据库编码。 (另一个方法是设置 PGCLIENTENCODING环境来达到同样的结果

-f file 
--file=file
把输出发往指定的文件．如果忽略这些，则使用标准输出。

-F format 
--format=format
选择输出的格式。 如下:

p 
plain
输出一个纯文本 SQL 脚本文件( 默认)

c 
custom
输出一个自定义格式用于pg_store 。最灵活的输出格式,它允许恢复过程中手动选择和重新排序的档案条目。默认压缩。

d 
directory
输出一个目录格式的存档用于pg_restore输入。 这将创建带有一个存储每张表和BLOB数据的文件的目录，加上一个所谓的内容文件表用机器可以读取的格式 描述转储对象, 这种格式pg_restore可以读取。 默认压缩

t 
tar
输出适合输入到 pg_restore 里的tar归档文件． 使用这个归档允许在恢复数据库时重新排序和/或把表结构排除在外． 同时也可能可以在恢复的时候限制对哪些数据进行恢复

-i 
--ignore-version
一个工具选项,现在被忽略了.

-n schema 
--schema=schema
指定输出的框架名。指定的框架将被输出转储。

-N schema 
--exclude-schema=schema
排除输出的框架名。指定的框架将不被输出。


-o 
--oids
为每个表都输出对象标识（OID）． 如果你的应用在某种程度上引用了OID字段的话，(比如，在外键约束中 用到)． 那么使用这个选项． 否则，不应该使用这个选项

-O 
--no-owner
不把对象的所有权设置为对应源数据库。 

这个选项只是对纯文本格式有意义．对于其它格式，在你调用 pg_restore 的时候你可以声明该选项.

-R 
--no-reconnect
此选项已经过时的，但可以向后兼容.

-s 
--schema-only
只输出架构（数据定义），不输出数据。

-S username 
--superuser=username
在某些场合，pg_dump 创建的脚本或者归档需要有 超级用户访问的权限，比如在关闭触发器或者为大纲元素甚至所有属性时． 这个选项声明在这些场合时使用的用户名

-t table 
--table=table
只输出表 table的数据

-T table 
--exclude-table=table
不输出表table的数据

-v 
--verbose
声明冗余模式。 这样将令 pg_dump 在标准错误上打印 进度信息.

-V 
--version
输出pg_dump的版本信息然后退出.

-x 
--no-privileges 
--no-acl
避免输出 ACL（赋予/撤消 命令）和表的所有者关系信息

-Z 0..9 
--compress=0..9
声明在那些支持压缩的格式中使用的压缩级别．零意味着没有 压缩 （目前只有客户化格式支持压缩） 

--no-security-labels
不转储安全标签.

--no-tablespaces
不输出选择表空间命令.

--no-unlogged-table-data
不转储没有日志记录的表的内容

--quote-all-identifiers
强制引用所有标识符 ，这可能用于转储迁移到未来的版本引入额外的关键字.

-? 
--help
显示帮助并退出.

以下选项控制数据库连接参数.

-h host 
--host=host
声明运行服务器 的机器的主机名．缺省是使用本地Unix主控套接字，而不是一个 IP 联接． 如果主机名以斜扛开头，则它被用做到 Unix 域套接字的路径

-p port 
--port=port
声明服务器 正在侦听并等待联接的TCP/IP 端口或本地 Unix 主控套接字文件句柄． 缺省的端口号是5432，或者环境变量 PGPORT 的值（如果存在）

-U username 
--username=username
以给出用户身分联接

-w 
--no-password          从来不需要密码提示 
-W 
--password
强制口令提示．如果服务器需要口令认证，那么这个动作应该自动发生


--role=rolename
指定一个角色的名字是用来创建转储

```

备份脚本
```bash
@ECHO OFF
@setlocal enableextensions
@cd /d "%~dp0"
SET PGPATH="C:\Program Files\PostgreSQL\9.5\bin\pg_dump"
SET SVPATH=F:\
SET PRJDB=NBJJ_1114
SET DBUSR=postgres
SET DBROLE=postgres
FOR /F "TOKENS=1,2,3 DELIMS=/ " %%i IN ('DATE /T') DO SET d=%%i-%%j-%%k
FOR /F "TOKENS=1,2,3 DELIMS=: " %%i IN ('TIME /T') DO SET t=%%i%%j%%k
SET DBDUMP=%PRJDB%_%d%_%t%.backup
@ECHO OFF
%PGPATH% -h localhost -p 5432 -U %DBUSR% --role %DBROLE% -w -F c -b -v -f %SVPATH%%DBDUMP% %PRJDB% 
echo Backup Taken Complete %SVPATH%%DBDUMP%
pause
```

## 查询表被哪些表应用
```sql

SELECT oid, relname FROM pg_class WHERE oid in
(
SELECT conrelid FROM pg_CONSTRAINT WHERE confrelid in (
    SELECT oid FROM pg_class WHERE relname = 'DeviceMeta'
    )
);
```

## 进度查看
PG9.6支持了pg_stat_progress_vacuum，PG13目前有如下5个进度视图：
27.4.1. ANALYZE Progress Reporting
27.4.2. CREATE INDEX Progress Reporting
27.4.3. VACUUM Progress Reporting
27.4.4. CLUSTER Progress Reporting
27.4.5. Base Backup Progress Reporting
 

官方文档有详细说明： 

https://www.postgresql.org/docs/13/progress-reporting.html

## 创建分区/表分区
https://blog.csdn.net/u010251897/article/details/80136995
```sql
CREATE OR REPLACE FUNCTION almart_partition_trigger()  
RETURNS TRIGGER AS $$  
DECLARE date_text TEXT;  
DECLARE insert_statement TEXT;  
BEGIN  
    SELECT to_char(NEW.date_key, 'YYYY_MM_DD') INTO date_text;  
    insert_statement := 'INSERT INTO almart_'  
        || date_text  
        ||' VALUES ($1.*)';  
    EXECUTE insert_statement USING NEW;  
    RETURN NULL;  
    EXCEPTION  
    WHEN UNDEFINED_TABLE  
    THEN  
        EXECUTE  
            'CREATE TABLE IF NOT EXISTS almart_'  
            || date_text  
            || '(CHECK (date_key = '''  
            || date_text  
            || ''')) INHERITS (almart)';  
        RAISE NOTICE 'CREATE NON-EXISTANT TABLE almart_%', date_text;  
        EXECUTE  
            'CREATE INDEX almart_date_key_'  
            || date_text  
            || ' ON almart_'  
            || date_text  
            || '(date_key)';  
        EXECUTE insert_statement USING NEW;  
    RETURN NULL;  
END;  
$$  
LANGUAGE plpgsql;  
```

删除过期
```sql
CREATE TABLESPACE cheap_table_space LOCATION '/data/cheap_disk';  
ALTER TABLE almart_2014_12_15 SET TABLESPACE cheap_table_space;  
```


分区实战
```sql
-- 创建存储过程函数
CREATE OR REPLACE FUNCTION t_themes_force_steelbar_partition_trigger()
RETURNS TRIGGER AS $$
DECLARE date_text TEXT;
DECLARE insert_statement TEXT;
BEGIN
    SELECT to_char(NEW.acquisition_datetime, 'YYYY_MM') INTO date_text;
    insert_statement := 'INSERT INTO t_themes_force_steelbar_'
        || date_text
        ||' VALUES ($1.*)';
    EXECUTE insert_statement USING NEW;
    RETURN NULL;
    EXCEPTION
    WHEN UNDEFINED_TABLE
    THEN
        EXECUTE
            'CREATE TABLE IF NOT EXISTS t_themes_force_steelbar_'
            || date_text
            || '(CHECK (to_char(acquisition_datetime, ''YYYY_MM'') = '''
            || date_text
            || ''')) INHERITS (t_themes_force_steelbar)';
        RAISE NOTICE 'CREATE NON-EXISTANT TABLE t_themes_force_steelbar_%', date_text;
        EXECUTE
            'CREATE INDEX idx_t_themes_force_steelbar_'
            || date_text
            || ' ON t_themes_force_steelbar_'
            || date_text
            || '(acquisition_datetime, sensor_id)';
        EXECUTE insert_statement USING NEW;
    RETURN NULL;
END;
$$
LANGUAGE plpgsql;

-- 绑定触发器
create trigger insert_t_themes_force_steelbar_partition_trigger
    BEFORE INSERT ON t_themes_force_steelbar
    FOR EACH ROW EXECUTE PROCEDURE t_themes_force_steelbar_partition_trigger();

--循环插入
do $$
    declare
        batch int:=1000000; -- 100w
        idx int8:=92822223;
        begin
        while idx-batch >0 loop
            insert into t_themes_force_steelbar select * from t_themes_force_steelbar_new where id between (idx-batch) and idx;
            RAISE NOTICE 'from % to %',(idx-batch),idx;
            idx = idx-batch;
            end loop;
    end;
$$;
```


## 查找data目录
https://www.cnblogs.com/kerrycode/p/14250746.html
show data_directory;

select setting from pg_settings where name='data_directory';

## 最大连接数
 show max_connections;

修改 
vim /var/lib/pgsql/9.4/data/postgresql.conf
vi /var/lib/postgresql/10/main/postgresql.auto.conf
max_connections = 100

service postgresql restart

## 查找所有mqtt的url
```sql
select distinct (properties::json->'URL')::varchar from "DeviceInterface" where "deviceMetaInterfaceId" in (
    select id from "DeviceMetaInterface" where "interfaceMetaId" in (
select id from "InterfaceMeta" where "interfaceTypeId"=6 order by "createdAt")
    );
```


## THINGs下统一增加周期维度
```sql
do
$$
    declare
        thingid varchar;
        dimid uuid;
        schid uuid;
        curthings CURSOR for select t.id from "Thing" t,"User" u where t."belongTo"=u.id and u.username='free-sun-home-test';
    begin
            open curthings;

            LOOP
                fetch curthings into thingid;
                EXIT WHEN NOT FOUND;

                if not exists(select * from "Dimension" d,"Scheme" s where s."dimensionId"=d.id and s.mode='R' and d."thingId"=thingid) then
                    dimid:= MD5(random()::text || clock_timestamp()::text)::uuid;
                    schid:= MD5(random()::text || clock_timestamp()::text)::uuid;
                    INSERT INTO "public"."Dimension"("id", "name", "desc", "createdAt", "updatedAt", "thingId")
                    VALUES(dimid,'周期','','2021-04-02 15:55:18.047+08','2021-04-02 15:55:18.047+08',thingid);
                    INSERT INTO "public"."Scheme"("id", "name", "mode", "interval", "unit", "repeats", "beginTime", "endTime", "notifyMode", "capabilityNotifyMode", "createdAt", "updatedAt", "dimensionId")
                     VALUES (schid, '周期', 'R', 5, 'second', NULL, '2021-04-14 10:37:32+08', NULL, 1, 1, '2021-04-14 10:38:11.879+08', '2021-04-14 10:38:11.879+08',dimid);

                    raise notice 'insert thing %',thingid;
                end if;
            end loop;
    end;
$$;
```

## 设备统计
distinct on 
```sql
Copy (select distinct on(d.name,d.model,d."createdAt",d."updatedAt") d.name,d.model,d."createdAt",d."updatedAt",dev.id as thingId,dev.name as "thingName",dev.username,m."interfaceTypeId",m.name as "interfaceName"
from "DeviceMeta" d
    join "DeviceMetaInterface" dmi  on dmi."deviceMetaId"=d.id
    join "InterfaceMeta" m on dmi."interfaceMetaId"=m.id
left join (
    select t.id,dd."deviceMetaId",t.name,u.username from "Device" dd,"Thing" t,"User" u where dd."thingId"=t.id and t."belongTo"=u.id
    ) as dev on dev."deviceMetaId"=d.id
where dmi."deviceMetaId"=d.id and dmi."interfaceMetaId"=m.id
      and d."createdAt" between '2020-01-01' and '2021-01-01'
order by d."createdAt") To '/tmp/test.csv' With CSV DELIMITER ',' HEADER;
```


查询所有外键关联表
```sql
SELECT
    tc.table_schema,
    tc.constraint_name,
    tc.table_name,
    kcu.column_name,
    ccu.table_schema AS foreign_table_schema,
    ccu.table_name AS foreign_table_name,
    ccu.column_name AS foreign_column_name
FROM
    information_schema.table_constraints AS tc
    JOIN information_schema.key_column_usage AS kcu
      ON tc.constraint_name = kcu.constraint_name
      AND tc.table_schema = kcu.table_schema
    JOIN information_schema.constraint_column_usage AS ccu
      ON ccu.constraint_name = tc.constraint_name
      AND ccu.table_schema = tc.table_schema
WHERE tc.constraint_type = 'FOREIGN KEY' AND ccu.table_name='t_structure' AND ccu.column_name='id';
```


跨库查询
```sql
CREATE EXTENSION dblink;

SELECT * 
FROM  (
   SELECT *
   FROM   dblink('host=iota-m1 port=5433 user=postgres password=ROOT dbname=SavoirCloud','select id,iota_thing_id,name from t_structure where org=87')
   AS     tb1(id int, iota_thing_id varchar,name varchar)
) AS tb1
LEFT JOIN "Thing" tb2 ON tb2.id = tb1.iota_thing_id where tb2.id is null;
```

还有一个办法：postgres-fdw
https://www.postgresql.org/docs/current/postgres-fdw.html
```sql
Here is an example of creating a foreign table with postgres_fdw. First install the extension:

CREATE EXTENSION postgres_fdw;
Then create a foreign server using CREATE SERVER. In this example we wish to connect to a PostgreSQL server on host 192.83.123.89 listening on port 5432. The database to which the connection is made is named foreign_db on the remote server:

CREATE SERVER foreign_server
        FOREIGN DATA WRAPPER postgres_fdw
        OPTIONS (host '192.83.123.89', port '5432', dbname 'foreign_db');
A user mapping, defined with CREATE USER MAPPING, is needed as well to identify the role that will be used on the remote server:

CREATE USER MAPPING FOR local_user
        SERVER foreign_server
        OPTIONS (user 'foreign_user', password 'password');
Now it is possible to create a foreign table with CREATE FOREIGN TABLE. In this example we wish to access the table named some_schema.some_table on the remote server. The local name for it will be foreign_table:

CREATE FOREIGN TABLE foreign_table (
        id integer NOT NULL,
        data text
)
        SERVER foreign_server
        OPTIONS (schema_name 'some_schema', table_name 'some_table');
```

# vacuum
http://www.45fan.com/article.php?aid=1yh1pEcdfr3BmvDD

xmin界限: 仍活跃的事务ID

解决xmin界限问题：
1、 查找长时间运行事务 pg_terminate_backend
2、查找复制槽
3、查找准备好的事务

vacuum的效果
1.1释放,再利用 更新/删除的行所占据的磁盘空间.
1.2更新POSTGRESQL查询计划中使用的统计数据
1.3防止因事务ID的重置而使非常老的数据丢失。

可以用vacuumdb --help查询。
-a/--all vacuum所有的数据库
-d dbname 只vacuum dbname这个数据库
-f/--full 执行full的vacuum
-t table 只vacuum table这个数据表


VACUUM 命令：
https://www.postgresql.org/docs/current/sql-vacuum.html



```shell
SELECT relname, age(relfrozenxid) as xid_age, pg_size_pretty(pg_table_size(oid)) as table_size FROM pg_class WHERE relkind = 'r' and pg_table_size(oid) > 1073741824
ORDER BY age(relfrozenxid) DESC LIMIT 20;

relname	xid_age	table_size
t_worker	35963498	2738 MB


VACUUM (VERBOSE, ANALYZE) t_worker;
VACUUM (ANALYZE, VERBOSE, FULL) t_worker; (最终生效)



#或者
su postgres
vacuumdb --analyze --verbose --table 't_worker' "SiteDemo"

#进度查询
https://dba.stackexchange.com/questions/44657/how-much-time-will-a-vacuum-autovacuum-operation-take
I get table's disk size using pg_total_relation_size() - this includes indexes and TOAST size, which is what VACUUM processes. This gives me the idea of how many bytes the VACUUM has to read.
I run VACUUM on the table.
I find the pid of the VACUUM process (in pg_catalog.pg_stat_activity).
In Linux shell I run while true; do cat /proc/123/io | grep read_bytes; sleep 60; done (where 123 is the pid) - this shows me bytes read by the process from the disk so far.
```

# PG 查询表类型、所有表
```sql
select nsp.nspname as object_schema,
       cls.relname as object_name,
       rol.rolname as owner,
       case cls.relkind
         when 'r' then 'TABLE'
         when 'm' then 'MATERIALIZED_VIEW'
         when 'i' then 'INDEX'
         when 'S' then 'SEQUENCE'
         when 'v' then 'VIEW'
         when 'c' then 'TYPE'
         else cls.relkind::text
       end as object_type,
       CASE relreplident
          WHEN 'd' THEN 'default'
          WHEN 'n' THEN 'nothing'
          WHEN 'f' THEN 'full'
          WHEN 'i' THEN 'index'
       END AS replica_identity
from pg_class cls
  join pg_roles rol on rol.oid = cls.relowner
  join pg_namespace nsp on nsp.oid = cls.relnamespace
where nsp.nspname not in ('information_schema', 'pg_catalog')
  and cls.relkind='r'
  and rol.rolname = current_user  --- remove this if you want to see all objects
order by nsp.nspname, cls.relname;
```


# PG_TOAST
select * from pg_class where relname='pg_toast_16489';

SELECT oid::regclass,
       reltoastrelid::regclass,
       pg_relation_size(reltoastrelid) AS toast_size
FROM pg_class
WHERE relkind = 'r'
  AND reltoastrelid <> 0
ORDER BY 3 DESC;


# 按行业和监测因素统计测点
select st.name "行业类型",f.name "监测因素",c "已配置点位数" from (
select st.structure_type,s.factor,count(*) as c from t_sensor s,t_structure st where s.structure=st.id group by s.factor,st.structure_type order by st.structure_type asc,c desc
) ss,t_structure_type st,t_factor f where ss.structure_type=st.id and ss.factor=f.id;


### 
记录一次数据表异常大小的问题：
select pg_size_pretty(pg_total_relation_size('t_worker'));
查询表大小：
```
select pg_size_pretty(pg_total_relation_size('t_worker'));
```

但是查询当前表内的索引大小只有：
index_name	index_definition	total_index_size
t_worker_id2	CREATE UNIQUE INDEX t_worker_id2 ON public.t_worker USING btree (id)	48 kB
```
SELECT
    indexname AS index_name,
    indexdef AS index_definition,
    pg_size_pretty(pg_total_relation_size(indexname::regclass)) AS total_index_size
FROM
    pg_indexes
WHERE
    tablename = 't_worker';
```

在dbAdminer WEB客户端查询的大小为：
显示索引大小大约为900多G


## 更新字符串中指定的substring

UPDATE t_data_traffic_load_original
SET pic = substring(pic from 'client(.+)')
WHERE pic LIKE 'client%'
and acq>'2023-12-14';


## 查询表的用户权限
SELECT
    grantee AS user_or_group,
    privilege_type AS permission,
    table_schema,
    table_name
FROM
    information_schema.table_privileges
WHERE
    table_schema = 'public' AND table_name = 't_themes_deformation_surface_displacement';

GRANT SELECT ON t_themes_deformation_surface_displacement TO readonly;
GRANT SELECT ON t_themes_deformation_settlement TO readonly;


## 分表

alter table t_themes_deformation_surface_displacement rename to t_themes_deformation_surface_displacement_old;
-- 创建主表（监测数据表）
create table t_themes_deformation_settlement
(
	id serial8 primary key,
	sensor_id integer,
	safety_factor_type_id integer default 11 not null,
	physical_quantity_value numeric(18,6) default NULL::numeric,
	settlement_value numeric(18,6) default NULL::numeric,
	settlement_score smallint,
	acquisition_datetime timestamp(6) default NULL::timestamp without time zone,
	orderby_column integer,
	description varchar(300) default NULL::character varying,
	temperature_value numeric(18,6) default NULL::numeric,
	reserved_field_2 varchar(30) default NULL::character varying,
	reserved_field_3 varchar(30) default NULL::character varying,
	reserved_field_4 varchar(30) default NULL::character varying,
	reserved_field_5 varchar(30) default NULL::character varying,
	press_original numeric(18,6) default NULL::numeric,
	settlement_value_original numeric(18,6) default NULL::numeric,
	agg_type integer,
	agg_way integer default 2
);

create index "ix_t_themes_deformation_settlement_datetime_sensorid"
	on t_themes_deformation_settlement (acquisition_datetime, sensor_id);

create index "ix_t_themes_deformation_settlement_datetime_sensorid_agg"
	on t_themes_deformation_settlement (sensor_id asc, acquisition_datetime desc, agg_type asc, agg_way asc);

-- 创建分表（设置Check时间范围约束）
create table t_themes_deformation_settlement_x_old ( CHECK ( acquisition_datetime < DATE '2024-03-01')) inherits (t_themes_deformation_settlement);
create table t_themes_deformation_settlement_2024 ( CHECK ( acquisition_datetime >= DATE '2024-03-01' AND acquisition_datetime < DATE '2025-01-01' )) inherits (t_themes_deformation_settlement);
create table t_themes_deformation_settlement_2025 ( CHECK ( acquisition_datetime >= DATE '2025-01-01' AND acquisition_datetime < DATE '2026-01-01' )) inherits (t_themes_deformation_settlement);

-- 创建RULE规则使得插入原表自动插入到指定分表
CREATE OR REPLACE FUNCTION t_themes_deformation_settlement_insert_trigger()
RETURNS TRIGGER AS $$
BEGIN
    IF ( NEW.acquisition_datetime < DATE '2024-03-01' ) THEN
        INSERT INTO t_themes_deformation_settlement_x_old VALUES (NEW.*);
    ELSIF ( NEW.acquisition_datetime >= DATE '2024-03-01' AND
            NEW.acquisition_datetime < DATE '2025-01-01' ) THEN
        INSERT INTO t_themes_deformation_settlement_2024 VALUES (NEW.*);
    ELSIF ( NEW.acquisition_datetime >= DATE '2025-01-01' AND
            NEW.acquisition_datetime < DATE '2026-01-01' ) THEN
        INSERT INTO t_themes_deformation_settlement_2025 VALUES (NEW.*);
    ELSE
        RAISE EXCEPTION 'Date out of range.';
    END IF;
    RETURN NULL;
END;
$$
LANGUAGE plpgsql;

-- 绑定到主表插入的触发器中
CREATE TRIGGER insert_t_themes_deformation_settlement_trigger
    BEFORE INSERT ON t_themes_deformation_settlement
    FOR EACH ROW EXECUTE PROCEDURE t_themes_deformation_settlement_insert_trigger();

-- 模拟数据
-- INSERT INTO public.x (sensor_id, safety_factor_type_id, physical_quantity_value, settlement_value, settlement_score, acquisition_datetime, orderby_column, description, temperature_value, reserved_field_2, reserved_field_3, reserved_field_4, reserved_field_5, press_original, settlement_value_original, agg_type, agg_way) VALUES
-- (653, 118, null, 0.000000, null, '2024-02-07 08:47:02.000000', null, null, null, null, null, null, null, null, null, null, 2);
-- INSERT INTO public.x (sensor_id, safety_factor_type_id, physical_quantity_value, settlement_value, settlement_score, acquisition_datetime, orderby_column, description, temperature_value, reserved_field_2, reserved_field_3, reserved_field_4, reserved_field_5, press_original, settlement_value_original, agg_type, agg_way) VALUES
-- (653, 118, null, 0.000000, null, '2024-03-07 08:47:02.000000', null, null, null, null, null, null, null, null, null, null, 2);

SET constraint_exclusion =on; -- 开启这个后，只查询指定时间区间内的表
EXPLAIN  select * from t_themes_deformation_settlement where acquisition_datetime>'2024-01-03';

-- 查看子表中是否有触发器
SELECT tgname AS trigger_name
FROM pg_trigger
WHERE tgrelid = 'public.t_themes_deformation_settlement_x_old'::regclass;

-- 21Y条数据,耗时： Minutes （未完成，换成下面说明的循环执行方式）
ALTER TABLE t_themes_deformation_settlement DISABLE TRIGGER ALL;
insert into t_themes_deformation_settlement_x_old select * from t_themes_deformation_settlement;
ALTER TABLE t_themes_deformation_settlement ENABLE TRIGGER ALL;

select count(*) from t_themes_deformation_settlement_2025;

-- 数据拷贝
-- 2146055989
CREATE OR REPLACE FUNCTION copy_data_in_batches()
RETURNS VOID AS $$
DECLARE
    batch_size INT := 1000000; -- 每个批次复制的行数
    max_id integer :=0;
BEGIN
    SELECT MAX(id) INTO max_id FROM t_themes_deformation_settlement_old;

    WHILE max_id >0 LOOP
        RAISE NOTICE 'Copying data from ID % to %', max_id, max_id - batch_size + 1;

        INSERT INTO t_themes_deformation_settlement_x_old (sensor_id, safety_factor_type_id, physical_quantity_value, settlement_value, settlement_score, acquisition_datetime, orderby_column, description, temperature_value, reserved_field_2, reserved_field_3, reserved_field_4, reserved_field_5, press_original, settlement_value_original, agg_type, agg_way)
        SELECT sensor_id, safety_factor_type_id, physical_quantity_value, settlement_value, settlement_score, acquisition_datetime, orderby_column, description, temperature_value, reserved_field_2, reserved_field_3, reserved_field_4, reserved_field_5, press_original, settlement_value_original, agg_type, agg_way
        FROM t_themes_deformation_settlement_old
        WHERE id BETWEEN max_id - batch_size + 1 AND max_id
        AND agg_type is null
        ORDER BY id DESC;

        max_id:=max_id-batch_size;
        end loop;
END;
$$ LANGUAGE plpgsql;

select copy_data_in_batches();

select max(id) from t_themes_deformation_surface_displacement;

-- 创建索引
create index ix_t_themes_deformation_surface_displacement_2025_sensor_time
	on public.t_themes_deformation_surface_displacement_2025 (acquisition_datetime, sensor_id);
create index t_themes_deformation_surface_displacement_2024_index_sensor_time_agg
	on public.t_themes_deformation_surface_displacement_2024 (sensor_id, acquisition_datetime, agg_type, agg_way);
create index ix_t_themes_deformation_surface_displacement_old_sensor_time
	on public.t_themes_deformation_surface_displacement_x_old (acquisition_datetime, sensor_id);
create index t_themes_deformation_surface_displacement_old_index_sensor_agg
	on public.t_themes_deformation_surface_displacement_x_old (sensor_id, acquisition_datetime, agg_type, agg_way);

create unique index t_themes_deformation_surface_displacement_2024_id_pk
	on public.t_themes_deformation_surface_displacement_2024 (id);